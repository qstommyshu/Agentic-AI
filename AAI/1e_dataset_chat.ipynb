{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Exercise 1:</b> Dataset Chat</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696341b-1f79-4c34-a9e6-ccc5dc411a45",
   "metadata": {},
   "source": [
    "**Welcome back! This is the first exercise in the course, so see what you can do!**\n",
    "\n",
    "This notebook serves as a hands-on exercise following the \"main-lecture\" notebook. The exercises in this series utilizes the same dataset to gradually help us enable more complex LLM interactions.\n",
    "\n",
    "In the previous notebook, we implemented a basic multi-agent system to generate synthetic multi-turn conversations. While useful for generating artificial dialogues, this implementation lacks practicality for end-user applications.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "**In this notebook, we will:**\n",
    "\n",
    "- Build a simple user-facing chatbot that interacts with a dataset.\n",
    "- Address the challenge of handling datasets too large for our modelâ€™s context window.\n",
    "- Develop a summarization pipeline to preprocess data efficiently.\n",
    "\n",
    "This dataset will remain in use throughout the course, so our first step is to enable a simple interactive chat with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee8272-833f-4d4b-abce-351bafff3093",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 1:** Setting Up A Workshop Assistant Chatbot\n",
    "\n",
    "From the previous exercise, you can specify a simple chatbot that interacts with the user using a simple loop. Based on this, the following function establishes a simple chatbot loop where a user can interact with an AI agent. If no processing function (chain) is provided, it defaults to an unimplemented generator that outputs a placeholder message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a208ed-38b0-4730-a540-0de4e1612f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def not_implemented_gen(state):\n",
    "    \"\"\"A placeholder generator that informs users the chain is not yet implemented.\"\"\"\n",
    "    message = \"Chain Not Implemented. Enter with no inputs or interrupt execution to exit.\"\n",
    "    for letter in message:\n",
    "        yield letter\n",
    "        sleep(0.005)\n",
    "\n",
    "def chat_with_chain(state={}, chain=not_implemented_gen):\n",
    "    \"\"\"\n",
    "    Interactive chat function that processes user input through a specified chain.\n",
    "    \n",
    "    Parameters:\n",
    "        state (dict): Maintains chat history and context.\n",
    "        chain (callable): Function to generate responses based on the chat history.\n",
    "    \"\"\"\n",
    "    assert isinstance(state, dict)\n",
    "    state[\"messages\"] = state.get(\"messages\", [])\n",
    "    while True:\n",
    "        try:\n",
    "            human_msg = input(\"\\n[Human]:\")\n",
    "            if not human_msg.strip(): break\n",
    "            agent_msg = \"\"\n",
    "            state[\"messages\"] += [(\"user\", human_msg)]\n",
    "            print(flush=True)\n",
    "            print(\"[Agent]: \", end=\"\", flush=True)\n",
    "            for token in getattr(chain, \"stream\", chain)(state):\n",
    "                agent_msg += token\n",
    "                print(getattr(token, \"content\", token), end=\"\", flush=True)\n",
    "            state[\"messages\"] += [(\"ai\", agent_msg)]\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"KeyboardInterrupt\")\n",
    "            break\n",
    "\n",
    "## Initialize chat with the placeholder generator\n",
    "chat_with_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd5d6c-8039-408e-a6cf-ac3081b7fdc7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "From this, we can define a conversational pipeline with an LLM, a prompt template, and a starting state. A prompt, llm, and an output parser are provided, so please combine them together into a chain for your `chat_with_chain` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d974c15-ef48-4b64-9006-331a2530f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from functools import partial\n",
    "\n",
    "## Define an NVIDIA-backed LLM\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://nim-llm:8000/v1\")\n",
    "\n",
    "## Define a structured prompt\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "     \"Assist users with their workshop-related queries using the provided context. \"\n",
    "     \"Do not reference the 'context' as 'context' explicitly.\"),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "## Construct the processing pipeline\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "## Initialize chatbot state\n",
    "state = {\n",
    "    \"messages\": [(\"ai\", \"Hello! I'm the NVIDIA DLI Chatbot! How can I help you?\")],\n",
    "    \"context\": \"\",  # Empty for now; will be updated later\n",
    "}\n",
    "\n",
    "## Wrap function to integrate AI response generation\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "\n",
    "## Start the chatbot with the AI pipeline\n",
    "chat(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db33c4-be56-49a8-984c-cb4a4192ec3d",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 2:** Pulling In Some Context\n",
    "\n",
    "For this course, we will start by using a small dataset of workshop catalog from the GTC 2025 conference. This includes a real selection of workshops which were each proposed independently and vary in detail. This should be reminiscent of an organically-accumulated datapool... partially because it is one. The data can be found in [`gtc-data-2025.csv`](./gtc-data-2025.csv), so let's go ahead and load it in as a list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f05dd8-73a9-4044-9e36-81f4242c3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "## Load dataset\n",
    "filepath = \"gtc-data-2025.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "## Convert to JSON for structured processing\n",
    "raw_entries = json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "## Display the first few records\n",
    "raw_entries[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7a275-fdda-4cb1-9ffb-4f4c0e153f06",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can quickly process them into a more natural format, and then try to concatenate them together to create a viable \"context string\" for our model. Automation to create context is quite common in real workflows to augment LLMs, so no reason not to do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e8f53-63b8-42f3-a391-a18d407134f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify(entry, description_key='description'):\n",
    "    \"\"\"Formats workshop details into a human-readable string.\"\"\"\n",
    "    return (\n",
    "        f\"{entry.get('name')}\\n\"\n",
    "        f\"Presenters: {entry.get('instructors')}\\n\"\n",
    "        f\"Description: {entry.get(description_key)}\"\n",
    "    )\n",
    "\n",
    "## Convert dataset entries to structured text\n",
    "raw_blurbs = [\n",
    "    f\"[Session {i+1}]\\n{stringify(entry)}\" \n",
    "    for i, entry in enumerate(raw_entries)\n",
    "]\n",
    "\n",
    "## Construct full context string\n",
    "raw_context = \"The following workshops are slated to be presented at NVIDIA's GTC 2025 Conference:\\n\\n\"\n",
    "raw_context += \"\\n\\n\".join(raw_blurbs)\n",
    "\n",
    "## Display context statistics\n",
    "print(f\"Full Context Length (characters): {len(raw_context)}\")\n",
    "print(\"-\"*40)\n",
    "print(raw_context[:2000])  # Preview the first portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e5172-768e-4d06-8aee-7e2c9d647783",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using your previous abstraction, pass the context into your prompt and see if it works:\n",
    "## TODO: Initialize your state based on your opinionated chat chain\n",
    "state = {}\n",
    "\n",
    "try:\n",
    "    ## TODO: Perform the conversation with your long context (it's ok if it fails)\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a3275-511f-402d-84be-cb12b5a8ae58",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details><summary><b>Hint</b></summary>\n",
    "\n",
    "Recall that we just have a `chat` function which wraps a reusable chain. So we can just invoke it using `chat(state)` for some `state`. Then, we just need to figure out what should go in our prompt. To refresh your memory:\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "     \"Assist users with their course-related queries using the provided context. \"\n",
    "     \"Do not reference the 'context' as 'context' explicitly.\"),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "<details><summary><b>Solution</b></summary>\n",
    "\n",
    "Given that our prompt expects to take in a dictionary which includes a `context` (interpretable as strings) and `messages` (interpretable as a list of messages like `[(\"user\", \"Hello World\")]`), we can initiate our prompt with no message history and the `raw_context` as our context.\n",
    "\n",
    "```python\n",
    "state = {\"messages\": [], \"context\": raw_context,}\n",
    "try:\n",
    "    chat(state)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "This probably didn't work, and for good reason. The model is launched with a max context of `2^13 = 8192` tokens, and the current context is probably a bit too long. Let's verify it with a [tokenizer](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct/blob/main/tokenizer.json) for the model in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea4244-8242-48a5-b1cf-fcf27dde1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "llama_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\", clean_up_tokenization_spaces=True)\n",
    "\n",
    "def token_len(text):\n",
    "    \"\"\"Counts token length of given text.\"\"\"\n",
    "    return len(llama_tokenizer.encode(text=text))\n",
    "\n",
    "print(f\"String Length of Context: {len(raw_context)}\")\n",
    "print(f\"Token Length of Context: {token_len(raw_context)}\")\n",
    "\n",
    "## Preview context\n",
    "print(raw_context[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14702102-4fb0-46ef-9fcc-2e720a1a1c5d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**You may be thinking \"don't most models have much longer contexts,\" and you would be right with a few key caveats:**\n",
    "\n",
    "- Using an API service, you would still be paying for the tokens anyway, so maybe having a long static context isn't the best idea?\n",
    "- Even if your context length is supported, most models still experience some amount of quality degredation as your inputs get longer. Furthermore, more inputs = more opportunities for conflicting data and text structures.\n",
    "- For an arbitrary document or even a document pool, you are likely to find yourself stretching into max context more often than not. Even if a model is good for this dataset, will it still work well for a database?\n",
    "\n",
    "In our case, we are operating on a sample of course descriptions of various detail and quality, which adds up to a large pool of inconsistent entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0eee7-d819-45fa-9776-8127f6611279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sorted_raw_blurbs = sorted(raw_blurbs, key=token_len)\n",
    "\n",
    "def plot_token_len(entries, color=\"green\", alpha=1, len_fn=token_len):\n",
    "    \"\"\"Plots token lengths of all entries.\"\"\"\n",
    "    plt.bar(x=range(len(entries)), height=[len_fn(v) for v in entries], width=1.0, color=color, alpha=alpha)\n",
    "\n",
    "plot_token_len(sorted_raw_blurbs, color=\"green\")\n",
    "plt.xlabel(\"Entry Sample Number\")\n",
    "plt.ylabel(\"Token Length\")\n",
    "plt.show() \n",
    "\n",
    "print(\"SHORTEST ENTRIES:\")\n",
    "sample_blurbs = sorted_raw_blurbs[:3] + sorted_raw_blurbs[-3:]\n",
    "\n",
    "for entry in sample_blurbs:\n",
    "    print(entry, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a997a46-37b3-4461-ae0f-e35b2ebdad6e",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 3:** Summarizing Our Long Context\n",
    "\n",
    "Maybe we can convert each of these entries into something shorter and more uniform? Maybe as a preprocessing step, we can just process all of these entries into a more consistent form. Not only will this help our model reason about the full context, but we'll also be able to leverage the entries' uniform nature to improve the consistency of our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475ac8a-536c-4ed6-b4a6-8f2a2a91cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## TODO: Create a symmary system message to instruct the LLM.\n",
    "## Reuse the chat_chain as-it-was, remembering that it expects \"messages\" and \"context\"\n",
    "summary_msg = (\n",
    "    \"Summarize the presentation description down to only a few important sentences.\"\n",
    "    \" Start with '(Summary) '\"\n",
    "    ## Feel free to customize\n",
    ")\n",
    "\n",
    "def summarize(context_str, summary_msg=summary_msg):\n",
    "    return \"(Summary) No summary\"\n",
    "\n",
    "print(summarize(stringify(raw_entries[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497fece0-31e4-4dde-8059-1b196c376093",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details><summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "return chat_chain.invoke({\n",
    "    \"messages\": [(\"user\", summary_msg)],\n",
    "    \"context\": context_str\n",
    "})\n",
    "```\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "It's natural language and not required to be well-formatted, but we can sufficiently prompt-engineer it for a simple text-to-text transformation function. We can also use the LangChain batching primitives to greatly simplify our concurrency management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15264663-4e21-479f-9d16-3a7172975165",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from tqdm.auto import tqdm\n",
    "import threading\n",
    "\n",
    "batch_inputs = [stringify(entry_dict) for entry_dict in raw_entries]\n",
    "\n",
    "## Simple version of a batched process. No progress bar\n",
    "# summaries = RunnableLambda(summarize).batch(batch_inputs, config={\"max_concurrency\": 20})\n",
    "\n",
    "## Modified version which also has progress bars! Marginally-slower, same backbone\n",
    "def batch_process(fn, inputs, max_concurrency=20):\n",
    "    lock = threading.Lock()\n",
    "    pbar = tqdm(total=len(inputs))\n",
    "    def process_doc(value):\n",
    "        try:\n",
    "            output = fn(value)\n",
    "        except Exception as e: \n",
    "            print(f\"Exception in thread: {e}\")\n",
    "        with lock:\n",
    "            pbar.update(1)\n",
    "        return output\n",
    "    try:\n",
    "        lc_runnable = fn if hasattr(fn, \"batch\") else RunnableLambda(process_doc)\n",
    "        return lc_runnable.batch(inputs, config={\"max_concurrency\": max_concurrency})\n",
    "    finally:\n",
    "        pbar.close()\n",
    "\n",
    "summaries = batch_process(summarize, batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06df66-ec7d-4892-94c7-eb97df4558f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68b853-1c83-4ad6-98cf-22e970211229",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that we have this new summary, we can see what happens when we use this synthetic description instead of our original ones, and consider how our context length is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f79fb0-1b82-4ae8-b6c4-454d98d0be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "## Defined Earlier\n",
    "\n",
    "# def stringify(entry, description_key='description'):\n",
    "#     return (\n",
    "#         f\"{entry.get('name')}\"\n",
    "#         f\"\\nPresentors: {entry.get('instructors')}\"\n",
    "#         f\"\\nDescription: {entry.get(description_key)}\"\n",
    "#     )\n",
    "\n",
    "## Defined Earlier\n",
    "#############################################################################\n",
    "\n",
    "for summary, pres_entry in zip(summaries, raw_entries):\n",
    "    words = summary.split()\n",
    "    ## Remove \"summary\" or \"(summary)\" from text\n",
    "    if \"summary\" in words[0].lower():\n",
    "        words = words[1:]\n",
    "    pres_entry[\"summary\"] = \" \".join(words)\n",
    "\n",
    "print(stringify(raw_entries[0], \"summary\"))\n",
    "raw_entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ac496-bc50-4569-8cf0-7c815da7e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "contexts_with_summaries = [stringify(entry, \"summary\") for entry in raw_entries]\n",
    "contexts_with_descripts = [stringify(entry) for entry in raw_entries]\n",
    "\n",
    "def plot_token_len(entries, color=\"green\", alpha=1, len_fn=token_len):\n",
    "    plt.bar(x=range(len(entries)), height=[len_fn(v) for v in entries], width=1.0, color=color, alpha=alpha)    \n",
    "\n",
    "## Create arrays of the token lengths\n",
    "sorted_summs = [v for _,v in sorted(zip((token_len(x) for x in contexts_with_summaries), contexts_with_summaries))]\n",
    "sorted_origs = [v for _,v in sorted(zip((token_len(x) for x in contexts_with_descripts), contexts_with_descripts))]\n",
    "aligned_summs = [v for _,v in sorted(zip((token_len(x) for x in contexts_with_descripts), contexts_with_summaries))]\n",
    "plot_token_len(sorted_origs, alpha=0.6, color=\"green\")\n",
    "plot_token_len(sorted_summs, alpha=0.6, color=\"grey\")\n",
    "## Lightgreen bars represent the new context length for their respective original green bars\n",
    "plot_token_len(aligned_summs, alpha=0.6, color=\"lightgreen\")\n",
    "plt.xlabel(\"Entry Sample\")\n",
    "plt.ylabel(\"Token Length\")\n",
    "plt.show() \n",
    "\n",
    "print(\"Samples:\")\n",
    "sorted_raw_entries = sorted(raw_entries, key=(lambda v: token_len(str(v.get(\"description\")))))\n",
    "for entry in sorted_raw_entries[:3] + sorted_raw_entries[-3:]:\n",
    "    print(\n",
    "        f\"{entry.get('name')}\"\n",
    "        f\"\\nPresentors: {entry.get('instructors')}\"\n",
    "        f\"\\nDescription: {entry.get('description')}\"\n",
    "        f\"\\nSummary: {entry.get('summary')}\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5978a60-8458-4a65-aa42-d6dd7d2e5e40",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Sounds like a promising direction! Let's implement it in practice and apply this change over all of our entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094aa2f-3865-4485-bdcd-9ed2bcdf68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct full context string\n",
    "new_context = \"The following workshops are slated to be presented at NVIDIA's GTC 2025 Conference:\\n\\n\"\n",
    "new_context += \"\\n\\n\".join(contexts_with_summaries)\n",
    "print(\"New Context Length:\", len(new_context))\n",
    "print(f\"New Context Tokens: {token_len(new_context)}\")\n",
    "\n",
    "## Preview context\n",
    "print(new_context[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaaeaef-cb92-44ae-8016-8b3b84b9f890",
   "metadata": {},
   "source": [
    "And all of a sudden, we're below our input size threshold! Time to just swap out our context and test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e8c76-d2ad-4c08-a516-e16b336a6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "## Defined Earlier. Feel free to play around with this\n",
    "\n",
    "## Define an NVIDIA-backed LLM\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://nim-llm:8000/v1\")\n",
    "\n",
    "## Define a structured prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "     \"Assist users with their course-related queries using the provided context. \"\n",
    "     \"Do not reference the 'context' as 'context' explicitly.\"),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "## Construct the processing pipeline\n",
    "chat_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "## Initialize chatbot state\n",
    "state = {\n",
    "    \"messages\": [(\"ai\", \"Hello! I'm the NVIDIA DLI Chatbot! How can I help you?\")],\n",
    "    \"context\": \"\",  # Empty for now; will be updated later\n",
    "}\n",
    "\n",
    "## Wrap function to integrate AI response generation\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "\n",
    "## Defined Earlier\n",
    "#############################################################################\n",
    "\n",
    "state = {\n",
    "    \"messages\": [],\n",
    "    \"context\": new_context,\n",
    "}\n",
    "\n",
    "try:  ## HINT: Consider putting your call logic in the try-catch\n",
    "    chat(state)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c2d22-53b2-4d30-8c57-6ccbd156f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consider saving the material as well, since it will be useful for later\n",
    "## For those who may take this course over multiple sessions, a version is provided.\n",
    "with open(\"simple_long_context.txt\", \"w\") as f:\n",
    "    f.write(new_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5f6ba-f953-4cac-8ef9-fc63788fcd5e",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 4:** Reflecting On This Exercise\n",
    "\n",
    "This exercise was pretty easy, and did actually lead to an interesting system in some sense. \n",
    "- On a superficial level, all we did was take an overlong context and convert it down to a not-too-overlong context.\n",
    "- Using other terms, we \"canonicalized\" the elements in the global environment into a form that help make up a reasonable \"canonical context\" to our primary LLM.\n",
    "- Pessimistically, we have created a very short-term solution to our limited-context-space multi-turn problem by making the context *a little shorter* than our maximum input length.\n",
    "- Optimistically, we now have a reusable context which can help to keep our entire context within our model's input domain for most single-turn problems (including those that may complement a multi-turn solution).\n",
    "\n",
    "We will continue to use the results of this exercise throughout the coming notebooks, so hopefully the utility of this simple process becomes apparent as we go along!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59451af0-2bb8-430f-9b82-d76793421534",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
