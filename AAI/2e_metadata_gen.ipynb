{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Exercise 2:</b> Metadata Generation</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f765c0a-409f-4052-b3ed-7c69cfc477a0",
   "metadata": {},
   "source": [
    "**Welcome to the second exercise!**\n",
    "\n",
    "This is a lean exercise intended to reinforce the concepts of structured output to try and work with course material and even long-form markdown as an exercise medium. Specifically, we will consider how we can generate first realistic metadata, and then an actual jupyter notebook using the tools from our previous section.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "**In this notebook, we will:**\n",
    "\n",
    "- Consider a more involved example of structured output which could be directly applied to synthetic content (*if used responsibly*).\n",
    "- Push beyond the generative priors of your LLM system to improve a longer-form document in an iterative fashion.\n",
    "\n",
    "### **Setup**\n",
    "\n",
    "Before doing this, let's load in our setup from the previous notebook and continue working with it as useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d974c15-ef48-4b64-9006-331a2530f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from functools import partial\n",
    "\n",
    "from course_utils import chat_with_chain\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://nim-llm:8000/v1\")\n",
    "\n",
    "## Minimum Viable Invocation\n",
    "# print(llm.invoke(\"How is it going? 1 sentence response.\").content)\n",
    "\n",
    "## Back-and-forth loop\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "## Am LCEL chain to pass into chat_with_generator\n",
    "chat_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "with open(\"simple_long_context.txt\", \"r\") as f:\n",
    "    full_context = f.read()\n",
    "\n",
    "long_context_state = {\n",
    "    \"messages\": [],\n",
    "    \"context\": full_context,\n",
    "}\n",
    "\n",
    "# chat = partial(chat_with_chain, chain=chat_chain)\n",
    "# chat(long_context_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8501c5b-482d-437a-9c8f-7d9840648cb0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 1:** Generating Simple Metadata\n",
    "\n",
    "In the lecture notebook, we picked up some techniques to generate data simply by asking nicely and enforcing a style. This relied on the model's priors. We noted how every model has some kinds of limitations in this regard. To make things easy, let's start out with an actual productionalizable use-case where even the 8B model shines; **Short-Form Data Extraction**.\n",
    "\n",
    "Our dataset of workshops has a lot of natural-language descriptions and we have a website frontend that requires it to have some sort of a schema, so wouldn't it be great if we could use an LLM to initialize those values?\n",
    "\n",
    "Well, we could define a schema to help us generate these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11f050-592f-4edf-87e0-ee3b701f4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class MetaCreator(BaseModel):\n",
    "    short_abstract: str = Field(description=(\n",
    "        \"A concise, SEO-optimized summary (1-2 sentences) of the course for students.\"\n",
    "        \" Ensure accuracy and relevance without overstating the workshop's impact.\"\n",
    "    ))\n",
    "    topics_covered: List[str] = Field(description=(\n",
    "        \"A natural-language list of key topics, techniques, and technologies covered.\"\n",
    "        \" Should start with 'This workshop' and follow a structured listing format that lists at least 4 points.\"\n",
    "    ))\n",
    "    abstract_body: str = Field(description=(\n",
    "        \"A detailed expansion of the short abstract, providing more context and information.\"\n",
    "    ))\n",
    "    long_abstract: str = Field(description=(\n",
    "        \"An extended version of the short abstract, followed by the objectives.\"\n",
    "        \" The first paragraph should introduce the topic with a strong hook and highlight its relevance.\"\n",
    "    ))\n",
    "    objectives: List[str] = Field(description=(\n",
    "        \"Key learning outcomes that students will achieve, emphasizing big-picture goals rather than specific notebook content.\"\n",
    "    ))\n",
    "    outline: List[str] = Field(description=(\n",
    "        \"A structured sequence of key topics aligned with major course sections, providing a clear learning path.\"\n",
    "    ))\n",
    "    on_completion: str = Field(description=(\n",
    "        \"A brief summary of what students will be able to accomplish upon completing the workshop.\"\n",
    "    ))\n",
    "    prerequisites: List[str] = Field(description=(\n",
    "        \"Essential prior knowledge and skills expected from students before taking the course.\"\n",
    "    ))\n",
    "\n",
    "def get_schema_hint(schema):\n",
    "    schema = getattr(schema, \"model_json_schema\", lambda: None)() or schema\n",
    "    return ( # PydanticOutputParser(pydantic_object=Obj.model_schema_json()).get_format_instructions()\n",
    "        'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema'\n",
    "        ' {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}},'\n",
    "        ' \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema.'\n",
    "        ' The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n' + str(schema) + '\\n```'\n",
    "    )\n",
    "\n",
    "schema_hint = get_schema_hint(MetaCreator)\n",
    "# schema_hint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9789591-993f-4884-a66c-a215499394bf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Then, if we just bind our LLM client to abide by the schema, then we should be able to generate it. \n",
    "The code below not only does that, but also shows how one might go about streaming the data or even filtering the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540b23a-dcb0-489a-907a-515b4cf60f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(\n",
    "    schema=MetaCreator.model_json_schema(), \n",
    "    strict=True\n",
    ")\n",
    "\n",
    "meta_chain = prompt | structured_llm\n",
    "meta_gen_directive = (\n",
    "    # f\"Can you generate a course entry on the Earth-2 course? {schema_hint}\"\n",
    "    # f\"Can you combine the topics of the Earth-2 course and the NeRF/3DGS courses and generate a compelling course entry? {schema_hint}\"\n",
    "    f\"Can you combine the topics of the Earth-2 course and the NeRF/3DGS courses and generate a compelling course entry? Make sure to explain how they combine. {schema_hint}\"\n",
    ") \n",
    "meta_gen_state = {\n",
    "    \"messages\": [(\"user\", meta_gen_directive)],\n",
    "    \"context\": full_context,\n",
    "}\n",
    "\n",
    "# answer = meta_chain.invoke(meta_gen_state)\n",
    "# print(answer)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "answer = {}\n",
    "for chunk in meta_chain.stream(meta_gen_state):\n",
    "    clear_output(wait=True)\n",
    "    for key, value in chunk.items():\n",
    "        print(f\"{key}: {value}\", end=\"\\n\\n\", flush=True)\n",
    "        answer[key] = value\n",
    "\n",
    "# llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e1ba0-375b-42e3-a987-ceb205097278",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Ok! That's not bad! It's reflective of the same limitations that we discussed in the lecture, but it does seem to be making good use of its context (while not degenerating into nonsense). Maybe we can ask it to improve upon it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e8c76-d2ad-4c08-a516-e16b336a6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: See if you can't prompt-engineer this solution to lead to an improved autoregression.\n",
    "meta_gen_state = {\n",
    "    \"messages\": [\n",
    "        (\"user\", meta_gen_directive),\n",
    "        (\"ai\", str(answer)),\n",
    "        (\"user\", \"Great! Can you please correct any mistakes and flesh out some vagueness?\")\n",
    "    ],\n",
    "    # \"context\": full_context,  ## Maybe we don't need the full context\n",
    "    \"context\": \"\",\n",
    "}\n",
    "\n",
    "answer2 = {}\n",
    "for chunk in meta_chain.stream(meta_gen_state):\n",
    "    clear_output(wait=True)\n",
    "    for key, value in chunk.items():\n",
    "        print(f\"{key}: {value}\", end=\"\\n\\n\", flush=True)\n",
    "        answer2[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db1b05-f731-4982-954f-5875e7d667ed",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Yeah... it can get better to a point.** \n",
    "- If we incorporate chat history, you'll start running into issues fast as the model starts to reach its context limit.\n",
    "- If we don't, we can still squeeze some customization from the LLM and can reasonably generate a better or longer outline... to a point.\n",
    "\n",
    "For this use-case, this model actually isn't that bad, but for something a bit longer, the limitations clearly start to show..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83f222-4721-4ed5-8ffe-cf3ec7f31da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pick your preferred option\n",
    "final_answer = answer2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6945e34-1fda-47ed-a09d-692c9defa10c",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 2:** Generating A Notebook\n",
    "\n",
    "We've seen some fuzzy limitations when trying to generate our metadata, so let's see if we start to see more obvious problems when we get more ambitious. Below, we show an attempt at using the GPT-4o model to generate a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baec8fb-6f11-484f-a442-ed5bd2f5a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "with open(\"chats/make_me_a_notebook/input.txt\", \"r\") as f:\n",
    "    notebook_input_full = f.read()\n",
    "    notebook_input_prompt = notebook_input_full.split(\"\\n\\n\")[-1]\n",
    "# print(notebook_input_full)\n",
    "print(notebook_input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e21d42-cb10-4f9a-8c1b-228158f109ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat chats/make_me_a_notebook/output.txt\n",
    "display(Markdown(\"chats/make_me_a_notebook/output.txt\"))\n",
    "display(Markdown(\"<hr><br><br>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa761a6-60b2-43b5-ad1a-673f8a8185c1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The notebook output in [`chats/make_me_a_notebook/output.txt`](./chats/make_me_a_notebook/output.txt) is the first-attempt output that came out of GPT-4o when I asked it to generate a notebook per [`chats/make_me_a_notebook/input.txt`](./chats/make_me_a_notebook/input.txt). It's serviceable enough with such a vague input, and can be improved **to some point** by just asking it for better output, criticizing it, and giving it enough information to work with. \n",
    "\n",
    "The common anecdote \"garbage in, garbage out\" comes to mind, since the LLM is just mirroring the style of reasonable output given your input specific input. But due to the conversational nature of the training (not helped by the chat prompts into which the messages are being funneled), your output will usually be uncomfortably short and just imprecise enough for many advanced use cases.\n",
    "\n",
    "Still, let's see if we can improve on this output by giving our LLM a style reference and asking it to rephrase the notebook a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0c15a-9cde-46bb-9f82-88c919f4f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def notebook_to_markdown(path: str) -> str:\n",
    "    \"\"\"Load a Jupyter notebook from a given path and convert it to Markdown format.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        notebook = json.load(file)\n",
    "    markdown_content = []\n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type'] == 'code':          # Combine code into one block\n",
    "            markdown_content += [f'```python\\n{\"\".join(cell[\"source\"])}\\n```']\n",
    "        elif cell['cell_type'] == 'markdown':    # Directly append markdown source\n",
    "            markdown_content += [\"\".join(cell[\"source\"])]\n",
    "        # for output in cell.get('outputs', []):   # Optionally, you can include cell outputs\n",
    "        #     if output['output_type'] == 'stream':\n",
    "        #         markdown_content.append(f'```\\n{\"\".join(output[\"text\"])}\\n```')\n",
    "    return '\\n\\n'.join(markdown_content)\n",
    "\n",
    "notebook_example = notebook_to_markdown(\"extra_utils/general_representative_notebook.ipynb\")\n",
    "\n",
    "context = str(final_answer)\n",
    "# context = (\n",
    "#     f\"THE FOLLOWING IS AN EXAMPLE NOTEBOOK FOR STYLE ONLY: \\n\\n{notebook_example}\"\n",
    "#     \"\\n\\n=========\\n\\n\"\n",
    "#     f\"THE FOLLOWING IS THE TOPIC COURSE THAT WE ARE DISCUSSING:\\n\\n{final_answer}\\n\\n\"\n",
    "# )\n",
    "\n",
    "long_context_state = {\n",
    "    \"messages\": [],\n",
    "    \"context\": context,\n",
    "}\n",
    "\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "chat(long_context_state)\n",
    "\n",
    "## EXAMPLE INPUTS ##\n",
    "## Option: Can you please construct a good notebook in markdown format?\n",
    "## Option: That's great, but there is no code. Can you please flesh out each section within an end-to-end narrative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20a90b-d1ae-4bf8-aebc-ab19955344f1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In our case, our model is quite small and we're also limiting our endpoint to a short input and short output (for its own good), so the amount of content it can generate really is quite limited. This limitation does, however, manifest in all realistic scenarios regardless of the model quality. For any modern LLM:\n",
    "- Though straight decoding of the solution can work for some contexts, they cannot scale up to arbitrarily-large inputs or outputs. \n",
    "- The quality output length is generally shorter than the quality input length when we get to longer sequences. This is enforced during training and enforces good properties for efficient cost of generation and reduction in context accumulation.\n",
    "\n",
    "In other words, **the space of things that can be given to or expected of an LLM $>>$ the space of things that an LLM can actually understand well $>>$ the space of things that the LLM can actually output well.** *($>>$ = \"far greater than\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d2f4b-84bc-44c5-8ac6-cc9fc2fc59e5",
   "metadata": {},
   "source": [
    "Given this insight, we can understand that trying to force the LLM to produce a notebook all at once might lead to incoherence at the global scale. However, it seems to be starting off at least somewhat ok, so maybe there's some merit in the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53cca4-01c2-4156-a795-dbd548c9f4e0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 3:** Using an Agent Canvas\n",
    "\n",
    "When we observe that we can't directly output the thing that we want, the next question is \"can we take in what we want.\" \n",
    "- It seemed like the LLM was able to roughly follow along with the premise when we only gave it the premise as input, but started derailing when we gave it a representative example. \n",
    "- Furthermore, it was likely able to actually improve upon your notebook through conversation, so maybe we can start there.\n",
    "\n",
    "**Canvasing Approach:** Instead of getting the model to predict the full document, get it to treat the document as  an environment and propose one of the following to the LLM:\n",
    "> - ***\"Please propose a modification that will improve the state of the document. Here are your options. Pick one/several and they will be done.\"***\n",
    "> - ***\"Here is the whole state, and you are tasked with improving JUST THIS SUBSECTION OF IT. Please output your update to that section. No other sections will be modified.\"***\n",
    "> - ***\"This is the whole document. This section is bad because of one or more of the following: {criticisms}. Replace it with an improved version.\"***\n",
    "\n",
    "If the model is capable of understanding both the full environment and the instruction, then it can directly autoregress only a small section or even a strategic modification of the output. Combine this approach with structured output or chain of thought, and you're likely to get a formulation that, while not perfect, helps to approach the potential output length towards the potential input length of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a1c62-83c5-45e7-8422-f3a6d58d5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Insert a notebook of choice\n",
    "STARTING_NOTEBOOK = \"\"\"\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a3dde-3fbb-43f5-9060-244c05340dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"user\", (\n",
    "        \"The following section needs some improvements:\\n\\n<<<SECTION>>>\\n\\n{section}\\n\\n<<</SECTION>>>\\n\\n\"\n",
    "        \"Please propose an upgrade that would improve the overall notebook quality.\"\n",
    "        \" Later sections will follow and will be adapted by other efforts.\"\n",
    "        \" You may only output modifications to the section provided here, no later or earlier sections.\"\n",
    "        \" Follow best style practices, and assume the sections before this one are more enforcing that the latter ones.\"\n",
    "        \" Make sure to number your section, continuing from the previous ones.\"\n",
    "    )),\n",
    "])\n",
    "\n",
    "## An LCEL chain to pass into chat_with_generator\n",
    "sub_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "delimiter = \"###\"  ## TODO: Pick a delimiter that works for your notebook\n",
    "WORKING_NOTEBOOK = STARTING_NOTEBOOK.split(delimiter)\n",
    "output = \"\"\n",
    "for i in range(len(WORKING_NOTEBOOK)):\n",
    "    chunk = WORKING_NOTEBOOK[i]\n",
    "    ## TODO: Knowing that the state needs \"context\" and \"section\" values,\n",
    "    ## can you construct your input state?\n",
    "    chunk_refinement_state = {\n",
    "        \"context\": None,\n",
    "        \"section\": None,\n",
    "    }\n",
    "    for token in sub_chain.stream(chunk_refinement_state):\n",
    "        print(token, end=\"\", flush=True)\n",
    "        output += token\n",
    "    WORKING_NOTEBOOK[i] = output\n",
    "    print(\"\\n\\n\" + \"#\" * 64 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337613d-67c9-4b9f-95b5-15e526102846",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<details><summary><b>Solution</b></summary>\n",
    "\n",
    "```python\n",
    "chunk_refinement_state = {\n",
    "    \"context\": \"####\".join(WORKING_NOTEBOOK),\n",
    "    \"section\": chunk,\n",
    "}\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "<hr><br>\n",
    "\n",
    "### **Part 4:** Reflecting On This Exercise\n",
    "\n",
    "As we can see, this approach is quite promising in that it's able to extend the output of the model towards a large context with only local modifications. This 8B model was pretty quickly pushed out of its training distribution with this approach, and it also likely started to go pretty aggressively into hallucination mode due to its vague inputs, but a larger model would be able to iterate on this process for much longer and could even have some error-correcting or randomization efforts thrown in to stabilize the process. \n",
    "\n",
    "This technique is also used in the wild to implement features like codebase modification and collaborative document editing (i.e. OpenAI Canvas). Additionally, even minor modifications to this approach can help you implement some surprisingly-effective and efficient solutions:\n",
    "- **Find-Replace Canvas:** Instead of autoregressing the sections of a document, you can generate find-replace pairs. Executing this process on the chunks, you will wind up with a much safer formulation as well as an easier-to-track footprint. This kind of system can be used to implement AI-enabled spell-checkers and other forms or strategic error correction.\n",
    "- **Document Translation:** More generally, this approach can also be used to translate a document, one section at a time, from one format to another. A similar approach to the one above can be used to translate a document from one language to another, with a bit of context injection thrown in to help give the translating model pipeline some style to guide it.\n",
    "\n",
    "Note that while we call this process *\"canvasing,\"* you may also run into the same or similar idea under the term *\"iterative refinement.\"* They are pretty much one-and-the-same, except the latter is much more general and could technically be applied to any LLM-enabled loop that progresses the input into the output over many iterations. Canvasing implies more strongly that you're using the current environment as a playground and can make strategic modifications to improve the state.\n",
    "\n",
    "----\n",
    "\n",
    "In any case, we've now tested out how our little model can actually help us do some surprisingly-interesting things, while also reflecting on the fact that it has clear limitations. This marks the end of our \"simple pipeline\" exercises for this course. In the next section, we will be using the primitives we've picked up to start working with a proper agents framework while sticking to our very-limited but surprisingly-flexible Llama-8B model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9454b-ce16-4a8e-bc4c-ecc406e4c7b3",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
