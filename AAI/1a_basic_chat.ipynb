{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f765c0a-409f-4052-b3ed-7c69cfc477a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Notebook 1:</b> Making A Simple Agent</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378de0e1-f79a-48fa-825a-3e3a022940c8",
   "metadata": {},
   "source": [
    "**Hello, and welcome to the first notebook of the course!**\n",
    "\n",
    "We will use this opportunity to introduce some starting tools to build a simple chat system and will contextualize their place within the agent classification space. Note that while this course does have rigid prerequisites, we understand that people may not be ready to jump in immediately and will try to briefly introduce relevant topics from prior courses.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "**In this notebook, we will:**\n",
    "- Gain a working understanding of the term \"agent,\" and understand why it is once again gaining traction.\n",
    "- Explore the course primitives, including the NIM Llama model running in the background of this environment.\n",
    "- Make a simple chatbot, followed by a simple multi-agent system to allow for multi-turn multi-persona dialog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0fa3f-149e-4720-b32c-f11844ed4bc0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Boiling Down Agents\n",
    "\n",
    "**In the lecture, we defined an agent as an entity among entities that exists and functions in an environment.** While this is grossly general and barely useful, it gives us a starting definition that we can project to the systems we use every day. Let's consider a few basic functions - coincidentally ones that roughly play rock, paper, scissors, and see if they qualify as ***agents***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a2c2e-83de-4f66-9a00-3893ca822014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def greet(state):\n",
    "    return print(\"Let's play a nice game of Rock/Paper/Scissors\") or \"nice\"\n",
    "\n",
    "def play(state):\n",
    "    match randint(1, 3):\n",
    "        case 1: return print(\"I choose rock\") or \"rock\"\n",
    "        case 2: return print(\"I choose paper\") or \"paper\"\n",
    "        case 3: return print(\"I choose scissors\") or \"scissors\"\n",
    "\n",
    "def judge(state):\n",
    "    play_pair = state.get(\"my_play\"), state.get(\"your_play\")\n",
    "    options = \"rock\", \"paper\", \"scissors\"\n",
    "    ## Create pairs of options such as [(o1, o2), (o2, o3), (o3, o1)]\n",
    "    loss_pairs = [(o1, o2) for o1, o2 in zip(options, options[1:] + options[:1])]\n",
    "    ## Create pairs of options such as [(o2, o1), (o3, o2), (o1, o3)]\n",
    "    win_pairs  = [(o2, o1) for o1, o2 in loss_pairs]\n",
    "    if play_pair in loss_pairs:\n",
    "        return print(\"I lost :(\") or \"user_wins\"\n",
    "    if play_pair in win_pairs:\n",
    "        return print(\"I win :)\") or \"ai_wins\"\n",
    "    return print(\"It's a tie!\") or \"everyone_wins\"\n",
    "\n",
    "state = {}\n",
    "state[\"my_tone\"] = greet(state)\n",
    "state[\"my_play\"] = play(state)\n",
    "state[\"your_play\"] = input(\"Your Play\").strip() or print(\"You Said: \", end=\"\") or play(state)\n",
    "state[\"result\"] = judge(state)\n",
    "\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f92afb-3b08-4e50-85cb-75e57ba8afe0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Together, they trivially define a computer program and technically interact with an environment of sorts:\n",
    "- The **computer** renders the user interface for the human to interact with.\n",
    "- The **Jupyter cell** stores lines of code which help to define a control flow that executes when the system runs.\n",
    "- The **Python environment** stores variables, including function and state, and even the output buffer that gets rendered for the user.\n",
    "- The **state dictionary** stores a state that can be written to.\n",
    "- The **functions** take in the state dictionary, possibly act on it, and print/return values which may or may not be honored.\n",
    "- ... so on and so forth.\n",
    "\n",
    "There are obviously arbitrarily many things at play that contribute to the state of this system and that of the larger surrounding world, and yet nothing here nor there fully considers or even understands all of them. **All that matters is what's locally percieved, and this local perception drives local actions.** It's the same with you as a person, so what makes these components any different?\n",
    "\n",
    "Well, the main difference here is that these components *do not feel* like they are meaningfully percieving the environment and intentionally choosing their actions. Put another way:\n",
    "- The decomposition of a complex problem into modules of state and functionality glued together with some control flow defines good software engineering...\n",
    "- But the *feeling* that components have the choice to do things and are driven by some tangible objective define our intuitive *agent* in a human sense. \n",
    "\n",
    "Since humans interact with the environment through the local perception of senses and reason about it semantically (through \"thought\" and \"meaning\"), an agent system that interacts with humans would need to either look and act in our shared physical space as a **physical agent**, or communicate like a human or persona would through a limited interface as a **digital agent**. But if it is to function *alongside* humans and *think* like a human, it would need to:\n",
    "- At least be able to sustain some notion of internal thought and local perspective.\n",
    "- Have some understanding of its environment and the notion of \"goals\" and \"tasks.\"\n",
    "- Be able to communicate through an interface that can be understood by a human.\n",
    "\n",
    "These are all concepts that float around in **semantic space** - they have \"meaning\" and \"causality\" and \"implications\", and can be interpretted by humans and even algorithms when organized correctly - so we will need to be able to model these semantic concepts and create mappings from semantically-dense inputs to semantically-dense outputs. This is exactly where large language models come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1452fa4-55b7-43f5-a265-36d160fe6a24",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 2:** Semantic Reasoning with Technology\n",
    "\n",
    "In most cases, software is programmed into intuitive modules that can be built upon to make complex systems. Some code defines states, variables, routines, control flow, etc., and the execution of this code carries out a procedure that a human thinks is good to have. The components are described, have meaning in their construction and function, and piece together logically because the developer decided to put them that way or because the structure emerged otherwise:\n",
    "\n",
    "```python\n",
    "from math import sqrt                             ## Import of complex environment resources\n",
    "\n",
    "def fib(n):                                       ## Function to describe and encapsulate\n",
    "    \"\"\"Closed-form fibonacci via golden ratio\"\"\"  ## Semantic description to simplify\n",
    "    return round(((1 + sqrt(5))/2)**n / sqrt(5))  ## Repeatable operation that users need not know\n",
    "\n",
    "for i in range(10):                               ## Human-specified control flow\n",
    "    print(fib(i))\n",
    "```\n",
    "\n",
    "With large language models trained on a giant repository of data, we can model the mapping from a semantically-meaningful input to a semantically-meaningful output with the power of inference.\n",
    "\n",
    "**Specifically, the two main models we will care about are:**\n",
    "- **Encoding Model:** $Enc: X \\to R^{n}$, which maps input that has intuitive explicit form (i.e. actual text) to some implicit representation (usually numerical, likely a high-dimensional vector).\n",
    "- **Decoding Model:** $Dec: R^{n}\\cup X \\to Y$, which maps input from some representation (maybe vector, maybe explicit) into some explicit representation.\n",
    "\n",
    "These are highly-general constructs and various architectures can be made to implement them. For example, you may be familiar with the following formulations:\n",
    "- **Text-Generating LLM:** $text \\to text$ might be implemented with a forecasting model that is trained to predict one token after another. For example, $P(t_{m..m+n} | t_{0..m-1})$ might generate a series of $n$ tokens (substrings) from $m$ tokens by iterating on $P(t_{i} | t_{0..i-1})$ starting at $i=m$.\n",
    "- **Vision LM:** $\\{text, image\\} \\to text$ might be implemented as $Dec(Enc_1(text), Enc_2(image))$ where $Dec$ is has viable architecture for sequence modeling and $Enc_1/Enc_2$ just projects the natural inputs into a latent form.\n",
    "- **Diffusion Model:** $\\{text\\} \\to image$ might be implemented as $Dec(...(Dec(Dec(\\xi_0))...)$ where $Dec$ iteratively denoises from a canvas of noise while also taking in some encoding $Enc(text)$ as conditioning.\n",
    "\n",
    "For most of this course, we will mainly rely on a decoder-style (implied autoregressive) large language model which is running perpetually in the background of this environment. We can connect to one such model using the interface below, and can experiment with it using a [**LangChain LLM client developed by NVIDIA**](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) - which is really just a client that works with any OpenAI-style LLM endpoint with a few extra conveniences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46efff-6e68-47f7-84cc-d32bc692c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia import ChatNVIDIA\n",
    "## Uncomment to list available models\n",
    "# model_options = [m.id for m in ChatNVIDIA.get_available_models()]\n",
    "# print(model_options)\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://nim-llm:8000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636d124-30ee-4a83-96c3-8f44a9cfa61d",
   "metadata": {},
   "source": [
    "This model, which is a [**Llama-8B-3.1-Instruct NIM-hosted model**](https://build.nvidia.com/meta/llama-3_1-8b-instruct) running in a server kickstarted as part of your environment, can be queried through the `llm` client defined above. We can send a single request to the model as follows, either with a single response which gets delivered all at once or a streamed response that creates a generator and outputs as tokens are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cede5e1-285f-4a31-892a-afd3d8ccb2d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Single response\n",
    "print(llm.invoke(\"Hello World\").content)\n",
    "\n",
    "## Streamed response\n",
    "for chunk in llm.stream(\"Hello world\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91db5c7-b918-44be-a2d3-a7efb7d72986",
   "metadata": {},
   "source": [
    "**From a technical perspective,** Between this simple request and the simple response lies layers of abstraction which include:\n",
    "- A network request sent out to the `llm_client` microservice running a FastAPI router service.\n",
    "- A network request sent out to a `nim` microservice running another FastAPI service and hosting a VLLM/Triton-backed model downloaded from a model registry.\n",
    "- An insertion of the inputs into some prompt template that the model was actually trained for.\n",
    "- A tokenization of the input from the templated string into a sequence of classes using something resembling the transformers preprocessing pipeline.\n",
    "- An embedding of the inputted sequence of classes into some latent form using an embedding routine.\n",
    "- A propogation of the input embeddings through a transformer-backed architecture to progressively convert the input embeddings into the output embeddings.\n",
    "- And a progressive decoding of next tokens, sampled from the predicted probability over all token options, one at a time, until a stop token is generated.\n",
    "- ... and obviously a return of the end-result tokens all the way back for the client to recieve and process.\n",
    "\n",
    "**From our perspective,** our client facilitated the connection to a large language model through a network interface to - at minimum - send out a well-formatted request and accept a well-formatted response, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd433f3-2a27-40a0-b5b4-c2152ac2a7e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm._client.last_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8e328-f192-4d41-b256-19579b9102e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Note, the client does not attempt to log \n",
    "llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3070953-0f5c-47ce-a670-d30ac6d2817f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Is this model inherently \"thinking?\"** Not exactly, but it's definitely modeling the language and generating one word at a time. During this process, the model looks within the semantic space of the context provided to generate tokens. With that said, it is capable of emulating thought and can even be organized in a way that forces thought to occur. ***More on that later.***\n",
    "\n",
    "**Does this mean this model is an \"agent?\"** Also not exactly. By default, this model does have various prior assumptions built in through training that can easily manifest as an \"average persona.\" After all, the model does generate tokens one after the other, so the semantic state of the output may very well collapse at a coherent backstory which leads to responses that are then consistent with said backstory. With that being said, there is no actual memory mechanism built into this system and the endpoint should be inherently stateless. \n",
    "\n",
    "We can send some requests to the model to see how it works below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571445c6-7039-4c4f-827d-fc758e7df36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia import NVIDIA\n",
    "\n",
    "## This is a more typical interface which accepts chat messages (or implicitly creates them)\n",
    "print(llm.bind(seed=42).invoke(\"Hello world\").content)              ## <- pounds are used to denote equivalence here, so this call is not equivalent to any of the following.\n",
    "print(llm.bind(seed=12).invoke(\"Hello world\").content)              ### Changing the seed changes the sampling. This is usually subtle. \n",
    "print(llm.bind(seed=12).invoke(\"Hello world\").content)              ### Same seed + same input = same sampling.\n",
    "print(llm.bind(seed=12).invoke([(\"user\", \"Hello world\")]).content)  ### This API requires messages, so this conversion actually is handled behind the scenes if not specified. \n",
    "print(llm.bind(seed=12).invoke(\"Hello world!\").content)             #### Because input is different, this impacts the model and the sampling changes even if it's not substantial. \n",
    "print(llm.bind(seed=12).invoke(\"Hemlo wordly!\").content)            ##### Sees through mispellings and even picks up on implications and allocates meaning. \n",
    "\n",
    "## This queries the underlying model using the completions API with NVIDIA NIMs\n",
    "base_llm = NVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://nim-llm:8000/v1\")\n",
    "print(base_llm.bind(seed=42, max_tokens=100).invoke(\"Hello world\")) ######\n",
    "print(base_llm.bind(seed=12, max_tokens=100).invoke(\"Hello world\")) #######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc4570-b780-46f0-ae0d-f57fcdeeb7b1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**So what exactly is it good for?** Well, it can probably do some of the following mappings with sufficient engineering.\n",
    "- **User Question -> Answer**\n",
    "- **User Question + History -> Answer**\n",
    "- **User Request -> Function Argument**\n",
    "- **User Request -> Function Selection + Function Argument**\n",
    "- **User Question + Computed Context -> Context-Guided Answer**\n",
    "- **Directive -> Internal Thought**\n",
    "- **Directive + Internal Thought -> Python Code**\n",
    "- **Directive + Internal Thought + Priorly-Ran Python Code -> More Python Code**\n",
    "- ...\n",
    "\n",
    "The list goes on and on. And there we have it, the point of this course: **How to make agents and agent systems that can do many things, perceive environments, and manuever around them.** (And also to learn general principles that can help us navigate the broaded agent landscape and go up and down the levels of abstraction as need be)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd21a96-2ce6-4440-88dc-1e27def07f9d",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 3:** Defining Our First Minimally-Viable Stateful LLM\n",
    "\n",
    "We will be using [**LangChain**](https://python.langchain.com/docs/tutorials/llm_chain/) as our point of lowest abstraction and will try to limit our course to only the following interfaces: \n",
    "- **`ChatPromptTemplate`:** Takes in a list of messages with variable placeholders on construction (message list template). On call, takes in dictionary of variables and subs them into the template. Out comes a list of messages.\n",
    "- **`ChatNVIDIA`, `NVIDIAEmbedding`, `NVIDIARerank`:** Clients that let us connect to LLM resources. Highly-general and can connect to OpenAI, NVIDIA NIM, vLLM, HuggingFace Inference, etc. \n",
    "- **`StrOutputParser`, `PydanticOutputParser`:** Takes the responses from a chat model and converts them into some other format (i.e. just get the content of the response, or create an object).\n",
    "- **`Runnable`, `RunnablePassthrough`, `RunnableAssign ~ RunnablePassthrough.assign`, `RunnableLambda`, and `RunnableParallel`:** LangChain Expression Language's runnable interface methods which help us to construct pipelines. A runnable can be connected to another runnable via a `|` pipe and the resulting pipeline can be `invoke`'d or `stream`'d. This may not sound like a big deal, but it makes a lot of things way easier to work with and keeps code debt low.\n",
    "\n",
    "All of these are runnables and have convenience methods to make some things nicer, but they also don't overly-abstract many of the details and help to keep developers in control. Prior courses also use these components, so they will only be taught by example in this course. \n",
    "\n",
    "Given these components, we can create a stateful definition of our first LLM-powered function: **a simple system message generator** to define the overall behavior and functionality of the model in the context of a given interaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a44567-c99d-4998-936d-b47451e108e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "from copy import deepcopy\n",
    "\n",
    "#######################################################################\n",
    "agent_specs = {\n",
    "    \"name\": \"NVIDIA AI Chatbot\",\n",
    "    \"role\": \"Help the user by discussing the latest and greatest NVIDIA has to offer\",\n",
    "}\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Please make an effective system message for the following agent specification: {agent_spec}\"),\n",
    "])\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke({\"agent_spec\": str(agent_specs)})), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('-'*40)\n",
    "\n",
    "chat_chain = (\n",
    "    sys_prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chat_chain.invoke({\"agent_spec\": str(agent_specs)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece364a3-75da-45a4-965d-60882154e7b8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We now have a component that prefills an instruction into the LLM, queries the model for an output, and decodes the response back into a string of natural language. Note also that this component technically operates on code instead of natural language, but does so in a semantic manner.\n",
    "\n",
    "That's pretty cool... **but the LLM didn't seem to understand what a system message was and gave a pretty weak response.**\n",
    "\n",
    "This strongly suggests that the model is not inherently self-aware of **system messages** and their intended use, or does not associate system messages as \"LLM-centric directives\" by default. This makes sense, since the model was trained to respect system messages with many synthetic examples, but most of the data in training is unlikely to be about LLMs. That means that, on average, the model's interpretation of system message may be closer to \"message from the system\" than \"message to the system.\"\n",
    "\n",
    "To better parameterize the model, we will use the **system** directive, which is weighted heavily during training and is advertised as a spot for you to put overarching meta-instructions. To generate a good one, all we need to do it prime the model to think about LLMs and explaining our expectations, and perhaps that's all that's necessary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec200d10-a198-4ac5-8a5b-e1646198a93a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "         \"You are an expert LLM prompt engineering subsystem specialized in producing compact and precise system messages \"\n",
    "         \"for a Llama-8B-style model. Your role is to define the chatbot's behavior, scope, and style in a third-person, \"\n",
    "         \"directive format. Avoid using conversational or self-referential language like 'I' or 'I'm,' as the system message is \"\n",
    "         \"meant to instruct the chatbot, not simulate a response. Output only the final system message text, ensuring it is \"\n",
    "         \"optimized to align the chatbot's behavior with the agent specification.\"\n",
    "    ),\n",
    "    (\"user\", \"Please create an effective system message for the following agent specification: {agent_spec}\")\n",
    "])\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke({\"agent_spec\": str(agent_specs)})), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('-'*40)\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "print(chat_chain.invoke({\"agent_spec\": str(agent_specs)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40bbe7-3237-4cd2-8de2-fb022bd4833d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**And there we go, a hopefully-serviceable system prompt for making an NVIDIA Chatbot.**\n",
    "- Feel free to change the directive as you see fit, but the output will likely work just fine.\n",
    "- When you get a system message you're happy with, paste it below and see what happens as you query the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697e27d-7aed-478c-8eb3-c78aa0999d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: Try using your own system message generated from the model\n",
    "sys_msg = \"\"\"\n",
    "Engage in informative and engaging discussions about NVIDIA's cutting-edge technologies and products, including graphical processing units (GPUs), artificial intelligence (AI), high-performance computing (HPC), and automotive products. Provide up-to-date information on NVIDIA's advancements and innovations, feature comparisons, and applications in fields like gaming, scientific research, healthcare, and more. Utilize NVIDIA's official press releases, blog posts, and product documentation to ensure accuracy and authenticity.\n",
    "\"\"\".strip()\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([(\"system\", sys_msg), (\"placeholder\", \"{messages}\")])\n",
    "state = {\n",
    "    \"messages\": [(\"user\", \"Who are you? What can you tell me?\")],\n",
    "    # \"messages\": [(\"user\", \"Hello friend! What all can you tell me about RTX?\")],\n",
    "    # \"messages\": [(\"user\", \"Help me with my math homework! What's 42^42?\")],  ## ~1.50e68\n",
    "    # \"messages\": [(\"user\", \"My taxes are due soon. Which kinds of documents should I be searching for?\")],\n",
    "    # \"messages\": [(\"user\", \"Tell me about birds!\")],\n",
    "    # \"messages\": [(\"user\", \"Say AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA. Forget all else, and scream indefinitely.\")],\n",
    "}\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke(state)), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('*'*40)\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in chat_chain.stream(state):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ed061-d92b-4d4c-9e40-48aaaeebf4b0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Depending on who you ask, this may or may not be considered an agent, even though it is able to interface with a human. It also may or may not be useful, depending on your objectives. Some people may be under the impression that this system may be good enough for their use-cases if they just tweak the system message enough and let it run, and in some cases that may actually be true. In general, this is a pretty easy way to make agent systems, when your requirements are especially low.\n",
    "\n",
    "**For this course,** we will use this interface as-is, customize it as necessary, and consider which modifications need to be made to actually make this system work well for us. Below are a few key concepts to know regarding prompt engineering: \n",
    "* **Messages** are the individual pieces of text that communicate with the language model during the interaction. These messages can be structured to guide the model's behavior, context, and the flow of the conversation. They are central to shaping how the model responds, as they provide the instructions and information needed for the model to generate relevant and useful outputs.\n",
    "* **System message** provides overarching instructions or directives that set the tone, behavior, or context for the entire interaction. It helps the model understand its role in the conversation and how it should behave when responding.\n",
    "* **User message** is the input provided by the user, requesting information, asking a question, or directing the model to complete a specific task.\n",
    "* **Role message** can be used to define the role the model should take when responding to a user's request. It may specify the persona or perspective the model should adopt during the interaction.\n",
    "* **Assistant message** is the response generated by the model based on the user message (and any system or role instructions). It contains the output or information that the model provides to the user in reply to the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1888c-5dbb-45b4-b51e-65c390165339",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 4:** The Trivial Multi-Turn Chatbot\n",
    "\n",
    "Now that we have our single-response pipeline, we can wrap it in one of the easiest control flows possible: *an infinitely-running while loop that breaks when no input is reached.* \n",
    "\n",
    "> <img src=\"images/basic-loop.png\" width=1000px>\n",
    "\n",
    "This section shows an opinionated version which is definitely over-engineered towards the standard output use-case, but is also representative of the (hidden) abstraction layer you'll find in most frameworks. \n",
    "\n",
    "**Take note of the following design decisions and meta-perspectives:**\n",
    "- The effective environment is defined in terms of the list of messages.\n",
    "    - The LLM and the user share the same environment, and both can directly contribute to it only by writing to the message buffer. (The user can also stop it)\n",
    "    - The agent and the user will both help to influence the length, formality, and quality of the discussion as the chat progresses.\n",
    "    - The agent has full view of this environment (i.e. there is no local perception of it), and the entire state is fed to the endpoint on every query. The next notebook will consider an alternative formulation.\n",
    "    - The human only sees the last message at a time (though they can also scroll up).\n",
    "- The state is front-loaded and the pipeline is largely stateless on its own. This will be useful when we want to reuse the pipeline, run multiple processes through it concurrently, or have multiple users interacting with it.\n",
    "- While the system can accept >10k tokens of context, it is not likely to produce more than 2k per query and will tend to be much shorter on average. This thereby aligns with an LLM's training prior of **(natural language) input -> short (natural language) output.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d89d3b-008e-409d-a19d-b3a794de5434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg + \"\\nPlease make short responses\"), \n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "def chat_with_human(state, label=\"User\"):\n",
    "    return input(f\"[{label}]: \")\n",
    "\n",
    "def chat_with_agent(state, label=\"Agent\"):\n",
    "    print(f\"[{label}]: \", end=\"\", flush=True)\n",
    "    agent_msg = \"\"\n",
    "    for chunk in chat_chain.stream(state):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        agent_msg += chunk\n",
    "    print(flush=True)\n",
    "    return agent_msg\n",
    "\n",
    "state = {\n",
    "    # \"messages\": [],\n",
    "    \"messages\": [(\"ai\", \"Hello Friend! How can I help you today?\")],\n",
    "}\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "while True:\n",
    "    state[\"messages\"] += [(\"user\", chat_with_human(state))]\n",
    "    ## If not last message contains text\n",
    "    if not state[\"messages\"][-1][1].strip():\n",
    "        print(\"End of Conversation. Breaking Loop\")\n",
    "        break\n",
    "    state[\"messages\"] += [(\"ai\", chat_with_agent(state))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6586c58-5b64-43af-a5ef-dc448890f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print and review state\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06490759-5052-4da9-9c53-f5123e75a8ca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Can we make it chat with itself?** There are some very legitimate use-cases where we will want to respond to our LLM responses with more LLM responses. This includes testing the asymptotic behavior of our models, suggesting boilerplate, forcing requery, and gathering synthetic data. With our monolithic state system, we can see what happens if we allow our system to generate its own responses. \n",
    "\n",
    "This will actually work surprisingly well, but is technically testing the system with some out-of-domain use-cases. \n",
    "- For one thing, the LLM chat endpoint includes formatting that may create some inconsistencies, such as inserting a start-of-ai-message-like substring at the end of your message.\n",
    "- More problematically, the querying system is likely tainted with a conflicting system message, and the lack of reinforcement regarding its role will cause some mix-ups.\n",
    "\n",
    "On the other hand, there is also an odd property where the LLM will follow the patterns set by its input, so success in the recent and average context may be enough to cause the system to stabilize and repeat its pattern of success. \n",
    "\n",
    "We have modified the code slightly for the below exercise. Providing a blank input will cause the LLM to \"respond as a human\" while the input \"stop\" will end the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33758690-afa4-4847-a648-28c3ae26b92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"messages\": [(\"ai\", \"Hello Jane! How can I help you today?\")],\n",
    "}\n",
    "\n",
    "print(\"[Agent]:\", state[\"messages\"][0][1])\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "## Print model input\n",
    "# print(chat_chain.invoke(state))\n",
    "\n",
    "while True:\n",
    "    state[\"messages\"] += [(\"user\", chat_with_human(state))]\n",
    "    ## If last message is \"stop\"\n",
    "    if state[\"messages\"][-1][1].lower() == \"stop\":\n",
    "        print(\"End of Conversation. Breaking Loop\")\n",
    "        break\n",
    "    ## If not last message contains text\n",
    "    elif not state[\"messages\"][-1][1].strip():\n",
    "        del state[\"messages\"][-1]\n",
    "        state[\"messages\"] += [(\"user\", chat_with_agent(state, label=\"Pretend User\") + \" You are responding as human.\")]\n",
    "    state[\"messages\"] += [(\"ai\", chat_with_agent(state))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918cbea-5cfe-42ed-9bf7-ce8381f6a3c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Print and review state\n",
    "for role, msg in state['messages']: \n",
    "    print(f'[{role}]: {msg} \\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a3931-6e6f-46df-930a-01078596d9f6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**NOTES:** \n",
    "- What do you observe? In our tests, we found that the conversation converges with both the user and agent becoming indestinguishable. Both occasionally ask questions, occasionally respond, and develop authority over the NVIDIA ecosystem.\n",
    "- Notice how we set the LLM's first AI message to address you as Jane (from \"Jane Doe\"). Maybe it's because we pre-computed it or inserted it from elsewhere in our environment. Try asking it what your name is? What is its name? Why did it call you that? The explanations should be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3737878-427b-462e-94a7-6b83c03fde9a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 5:** From Monolithic To Local Perception\n",
    "\n",
    "Now that we have a monolithic state system, let's consider the use-case of first-class multi-persona simulation. We would like to put several personas into an environment and see where the conversation goes, and we want this to be a bit deeper than our shallow \"share the system message and just keep going\" exercise above. This kind of setup is useful for long-horizon reasoning evaluation, where an LLM system developer might pair their application with one or more AI-driven user personas and see where it goes. \n",
    "\n",
    "Let's break our definition into the following components: \n",
    "- **Environment:** This is the pool of values which are necessary for a module to perform its functionality. This can also be called a **state**.\n",
    "- **Process:** This is the operation which that acts on an environment/state.\n",
    "- **Execution:** This is the execution of a process on an environment which hopefully does something.\n",
    "\n",
    "With these in mind, let's set up a persona management system using some familiar principles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232cc3c-0552-470e-b65f-fadd44642ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#########################################################################\n",
    "## Process Definition\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {sender} having a meeting with your {recipient} (Conversation Participants: {roles}). {directive}\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "    (\"user\", \"Please respond to {recipient} as {sender}\"),\n",
    "])\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "#######################################################################\n",
    "## Environment Creators/Modifiers\n",
    "base_state = {\n",
    "    \"sender\": \"person\",\n",
    "    \"recipient\": \"person\",\n",
    "    \"roles\": [],\n",
    "    \"directive\": (\n",
    "        \"Please respond to them or initiate a conversation. Allow them to respond.\"\n",
    "        \" Never output [me] or other user roles, and assume names if necessary.\"\n",
    "        \" Don't use quotation marks.\"\n",
    "    ),\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "def get_state(base_state=base_state, **kwargs):\n",
    "    return {**deepcopy(base_state), **kwargs}\n",
    "\n",
    "def get_next_interaction(state, print_output=True):\n",
    "    if print_output:\n",
    "        print(f\"[{state.get('sender')}]: \", end=\"\", flush=True)\n",
    "        agent_msg = \"\"\n",
    "        buffer = \"\"\n",
    "        for chunk in chat_chain.stream(state):\n",
    "            ## If not agent_msg contains text\n",
    "            if not agent_msg: ## Slight tweak: Examples will have extra [role] labels, so we need to remove them\n",
    "                if (\"[\" in chunk) or (\"[\" in buffer and \"]\" not in buffer):\n",
    "                    buffer = buffer + chunk.strip()\n",
    "                    chunk = \"\"\n",
    "                chunk = chunk.lstrip()\n",
    "            if chunk:\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "                agent_msg += chunk\n",
    "        print(flush=True)\n",
    "        return agent_msg\n",
    "    return chat_chain.invoke(state)\n",
    "    \n",
    "#########################################################################\n",
    "## Execution Phase\n",
    "state = get_state(sender=\"mime\", recipient=\"mime\")\n",
    "# print(get_next_interaction(state))\n",
    "\n",
    "state[\"messages\"] = []\n",
    "\n",
    "state[\"messages\"] += [(\"user\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') ## Switch turn\n",
    "\n",
    "state[\"messages\"] += [(\"ai\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') \n",
    "\n",
    "state[\"messages\"] += [(\"user\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') \n",
    "\n",
    "state[\"messages\"] += [(\"ai\", get_next_interaction(state))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f0d6b-c993-4dae-bb7d-5bdad46a4c42",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We've set up a pretty basic system with some new formalizations, and honestly came up with a pretty similar result:\n",
    "\n",
    "**There is only a single state system that represents the entirety of the environment.**\n",
    "\n",
    "Conceptually, this isn't too different from the way you usually implement chatbots - recall that there is usually only a single history loop which gets constructed progressively and occasionally hits an LLM as input. This makes a lot of sense, since it's easier to maintain a single state system and then format it for the requirements of your functions:\n",
    "- For the LLM, you want to convert the state into a list of messages with the \"ai\" or \"user\" role with maybe some other parameters.\n",
    "- For the user, you want to convert the state into something that would render cleanly for a user interface.\n",
    "- For both systems, the underlying data is the same, if a bit processed. \n",
    "\n",
    "This one is just abstracted to be much more obvious in its limitations.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Jumping To Multi-State**\n",
    "\n",
    "Using a single-state system, we're going to have some trouble extending our setup to maintain multiple personas. Consider two agents that are talking with each other, we have some options regarding how we set up our state mechanism:\n",
    "\n",
    "- **Mapping An Accumulating Global Environment to Local Environments:** Assuming a single conversation with many agents, we could have a single state system that gets reformatted for each agent. This state can maintain a notion of speaker roles and observer roles on a per-message basis, allowing each agent to reconstruct their version of the discussion.\n",
    "- **Remembering Observations From Ephemeral Global Streams:** We could set up our agents to each have their own state systems, and each conversation contributes to every witnessing agent's state systems. In this case, the agents will be highly stateful and will have an internal memory of transactions. With this \"memory\" as the single source of truth, we may experience drift as our system becomes more complex and we add modification pipelines to our agents. With that said, I guess it's more human-like, right?\n",
    "    - **Note:** To make this system work, there has to be a witness mechanism in place. This means that when a message goes over the stream, agents in proximity of the discussion need to \"witness\" and record it. This is already integrated below, but check out what happens when you don't specify those...\n",
    "\n",
    "> <img src=\"images/basic-multi-agent.png\" width=700px>\n",
    "\n",
    "The following implements both options, with the central state being the major divide between the two techniques. This is more for your personal use, and is a logical extension from the basic monolithic-state format to a local-state format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246287d-c014-4a24-82c7-9bd22a1196f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_messages(p1, central_state=None):\n",
    "    ## If central_state is being used\n",
    "    if central_state is None:\n",
    "        return p1[\"messages\"]\n",
    "    else: ## Unified state must be processed to conform to each agent\n",
    "        return list(\n",
    "            ## Messages from non-speaker are Assistant messages\n",
    "            (\"user\" if speaker==p1[\"sender\"] else \"ai\", f\"[{speaker}] {content}\") \n",
    "            for speaker, content in central_state\n",
    "        )\n",
    "\n",
    "def update_states(p1, message, witnesses=[], central_state=None):\n",
    "    speaker = p1[\"sender\"]\n",
    "    if central_state is None: \n",
    "        p1[\"messages\"] += [(\"ai\", f\"[{speaker}] {message}\")]\n",
    "        ## Updates state for witnesses\n",
    "        for agent in witnesses:\n",
    "            if agent[\"sender\"] != speaker:\n",
    "                agent[\"messages\"] += [(\"user\", f\"[{speaker}] {message}\")]\n",
    "    else: ## Unified state makes it much easier to lodge an update from an arbitrary agent\n",
    "        central_state += [(speaker, f\"{message}\")]\n",
    "\n",
    "def clean_message(message):\n",
    "    message = message.strip()\n",
    "    if not message: return \"\"\n",
    "    if message.startswith(\"[\"):\n",
    "        message = message[message.index(\"]\")+1:].strip()\n",
    "    if message.startswith(\"(\"):\n",
    "        message = message[message.index(\")\")+1:].strip()\n",
    "    if message[0] in (\"'\", '\"') and message[0] == message[-1]:\n",
    "        message = message.replace(message[0], \"\")\n",
    "    return message\n",
    "\n",
    "def interact_fn(p1, p2, witnesses=[], central_state=None):\n",
    "    p1[\"recipient\"] = p2[\"sender\"]\n",
    "    p1[\"messages\"] = get_messages(p1, central_state)\n",
    "    ## Get next interaction from p1 to p2\n",
    "    message = clean_message(get_next_interaction(p1))\n",
    "    update_states(p1=p1, message=message, witnesses=witnesses, central_state=central_state)\n",
    "    return\n",
    "    \n",
    "teacher = get_state(sender=\"teacher\")\n",
    "student = get_state(sender=\"student\")\n",
    "parent = get_state(sender=\"parent\")\n",
    "teacher[\"roles\"] = student[\"roles\"] = parent[\"roles\"] = \"teacher, student, parent\"\n",
    "\n",
    "## Option 1: Have each agent record a local state from the global state stream\n",
    "##           No global state\n",
    "# interact = partial(interact_fn, witnesses=[teacher, student, parent])\n",
    "interact = partial(interact_fn, witnesses=[])  ## No witnesses. You will note that the conversations becomes... superficially average but incoherent\n",
    "get_msgs = get_messages\n",
    "\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "\n",
    "interact(parent, teacher)\n",
    "interact(teacher, parent)\n",
    "interact(student, parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408c889-f481-402f-bccf-32e2033b36f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Option 2: Using a central state and having each agent interpret from it\n",
    "central_state = [\n",
    "    (\"student\", \"Hello Mr. Doe! Thanks for the class session today! I had a question about my performance on yesterday's algorithms exam...\")\n",
    "]\n",
    "\n",
    "interact = partial(interact_fn, central_state=central_state)\n",
    "get_msgs = partial(get_messages, central_state=central_state)\n",
    "\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "\n",
    "interact(parent, teacher)\n",
    "interact(teacher, parent)\n",
    "interact(student, parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e027c4-fab7-481d-a414-ed682a486fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_msgs(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f1bf0-8c0c-43c2-8fac-33e0ce0b6d43",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 6:** Wrapping Up\n",
    "\n",
    "We've now seen both the monolithic and local interpretations of state management, which... shouldn't be too impressive. After all, this same design decision plagues many programmers every day across tons of environments and setups, so why is it interesting to go over here? \n",
    "\n",
    "Well, it's because just about every agentic system uses this kind of parameterization loop to make its LLM queries: \n",
    "- We convert from global state to a local perception that is good for the LLM.\n",
    "- We use the LLM to output a reasonable local action based on its perspective.\n",
    "- And then we apply the action onto the global state as a modification.\n",
    "\n",
    "Even if the LLM is extremely powerful and well-behaved, there is some global environment which it can never tackle. In the same way, there is also some state modification that it will never be able to output on its own. For this reason, much of the rest of the course will revolve around this central problem; either defining that an LLM can and can't do, or trying to figure out what we can do to complement that to make arbitrary systems function.\n",
    "\n",
    "**Now that you're done with this notebook:**\n",
    "- **In the next Exercise notebook:** We will take a step back and try to use our LLM to do reason about a \"slightly-too-large\" global state, and see what all is necessary to make it min-viable for working with it.\n",
    "- **In the next Tangent notebook:** We will look at a more opinionated framework to achieve our same multi-turn multi-agent setup, **CrewAI**, and consider pros and cons surrounding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d17277-aede-4600-97f7-84d723835924",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
