{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# <font color=\"#76b900\">**Notebook 4:** Encoders and Decoders for Sequence Generation</font>\n",
    "\n",
    "In the previous notebook, we explored tasks where \"BERT-like\" encoder-only models showed their strength in understanding static inputs. These models excel in tasks where the input does not need to be transformed or dynamically altered, such as text classification, sentiment analysis, and named entity recognition. However, their ability to generate novel or dynamic outputs is limited. \n",
    "\n",
    "For tasks like machine translation, summarization, and question-answering, where the output needs to be dynamically generated in a structured form, we require more complex architectures that go beyond encoding input.\n",
    "\n",
    "In this notebook, we expand our architectural toolkit to explore **encoder-decoder** and **decoder-only** models, which are designed to generate ordered sequences based on input contexts:\n",
    "\n",
    "- **Encoder-Decoder Models (e.g., Flan-T5)**: These models first encode the input into a fixed representation and then decode it into a new, ordered sequence. For example, in translation, the encoder understands the source sentence, while the decoder generates the translated sentence.\n",
    "- **Decoder-Only Models (e.g., GPT-2)**: Unlike encoder-decoder models, decoder-only models predict tokens based on previous context and excel at tasks that where the input and output distributions blur together, such as in open-ended text generation and dialogue systems.\n",
    "\n",
    "#### **Learning Objectives:**\n",
    "\n",
    "- Learn about encoder-decoder models that use an encoder for static context and a decoder for generating ordered sequences.\n",
    "- Understand how decoder-only models, like GPT-2, excel in generative tasks by predicting tokens based only on previous decoder contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ii0e8yi-u_v"
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 4.1:** The Machine Translation Task\n",
    "\n",
    "[**Machine Translation**](https://huggingface.co/tasks/translation) is the task of automatically translating text from one language to another using software. While the term may sound straightforward, machine translation is an extremely complex task that requires models to understand and generate languages with vastly different grammar, syntax, word order, and even cultural nuances.\n",
    "\n",
    "For instance, translating from a language like Japanese, which often places the verb at the end of the sentence, to English, which follows a subject-verb-object (SVO) structure, can be tricky. Additionally, languages often have unique idiomatic expressions and contextual meanings that must be accurately captured by the model.\n",
    "\n",
    "### **Shifting from Encoders to Decoders**\n",
    "The complexity of translation arises from the need to not just understand the source language (encoding) but also to generate a fluent and coherent translation in the target language (decoding). This is where **encoder-decoder models** shine. For natural language specifically:\n",
    "\n",
    "- The **encoder** processes the source language sentence, turning it into a fixed representation that captures its meaning.\n",
    "- The **decoder** takes this representation and generates the translated sentence in the target language, word by word.\n",
    "\n",
    "This combination allows the model to first \"understand\" the meaning of a sentence before attempting to \"generate\" the translation. Such a structured approach leads to more accurate translations, especially in languages with drastically different sentence structures.\n",
    "\n",
    "However, for other generative tasks, like creating open-ended text or dialogues, we can bypass the need for a fixed input representation and instead rely on a model that generates outputs one token at a time. This leads us to the next section, where we explore **decoder-only** models like GPT-2, which are designed for these more open-ended tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 4.2:** Pulling In A GPT-style model\n",
    "\n",
    "As we’ve discussed, architectures like BERT are excellent for understanding input data but fall short when it comes to generating text. For tasks that require the generation of novel sequences—whether it's storytelling, dialogue, or open-ended text completion—we need a model that can predict and generate tokens sequentially. This is where the **GPT-style architecture** prevails.\n",
    "\n",
    "#### **Why Decoder-Only Models?**\n",
    "GPT-2 is an example of a **decoder-only** model, which is designed for **autoregressive text generation**. Unlike encoder-decoder models, which use a separate encoder to understand the input, decoder-only models generate text one token at a time. The model is trained to predict the next token in a sequence based on the previous ones, a process known as **autoregression**.\n",
    "\n",
    "#### **How Does Autoregressive Generation Work?**\n",
    "1. The model is given an initial prompt like `\"Hello world,\"`.\n",
    "2. It predicts the most likely next token based on the prompt.\n",
    "3. The predicted token is added to the input sequence, and the process is repeated until the model generates a complete output.\n",
    "\n",
    "This method allows the model to generate open-ended sequences without needing a fixed input context, making it ideal for tasks like dialogue generation, storytelling, and text completion.\n",
    "\n",
    "**Let’s see GPT-2 in action with a simple text generation example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Create a text-generation pipeline using GPT-2\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generate 5 sequences of text starting with the prompt \"Hello world\"\n",
    "generator(\"Hello world,\", max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Interpreting the GPT-2 Output**\n",
    "When the model generates text, it’s creating novel content based on the prompt \"Hello world,\" by predicting one token at a time. Each sequence will be different due to the probabilistic nature of the model.\n",
    "\n",
    "- **Why 5 sequences?**: By specifying `num_return_sequences=5`, we ask the model to produce 5 variations of text. GPT-style models use atteibutes like `temperature` and `top_k`/`top_p` to modulate sampling strategy, causing results to be non-deterministic by default.\n",
    "- **Max Length**: The parameter `max_length=20` limits the output to 20 tokens, which can help manage the size of generated sequences when experimenting. Before this, the model can also stop with a dedicated stop token or a manually-specified stop string. \n",
    "\n",
    "#### **Investigating The Forward Pass**\n",
    "\n",
    "Going a bit below the pipeline, we can use the same principles as before to investigate how the data is actually generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"GENERATING ALL AT ONCE:\")\n",
    "input_str = \"Hello world\"\n",
    "print(f\"{(x := generator.preprocess(input_str))}\")\n",
    "print(f\"{(x := generator.forward(x, max_new_tokens=20))}\")\n",
    "print(f\"{(x := generator.postprocess(x))}\")\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "print(\"\\nGENERATING ONE TOKEN AT A TIME (prep+forward+post):\")\n",
    "print(input_str := \"Hello world\", end=\"\")\n",
    "output_buffer = \"\"\n",
    "for i in range(20):\n",
    "    x = generator.preprocess(input_str + output_buffer)\n",
    "    x = generator.forward(x, max_new_tokens=1)\n",
    "    x = generator.postprocess(x)\n",
    "    next_word = x[0].get(\"generated_text\")[len(input_str + output_buffer):]\n",
    "    output_buffer += next_word\n",
    "    print(next_word.replace(\"\\n\", \"\\\\n\"), end=\"\")\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "print(\"\\n\\nGENERATING ONE TOKEN AT A TIME (manually, greedily-sampled):\")\n",
    "\n",
    "model_body = generator.model.transformer\n",
    "model_head = generator.model.lm_head\n",
    "tknzr_encode = generator.tokenizer.encode\n",
    "tknzr_decode = generator.tokenizer.decode\n",
    "\n",
    "def compute_embed(token_id):\n",
    "    return model_body.wte(torch.tensor([token_id])).view(1, -1, 768)\n",
    "\n",
    "# PREFILL stage: Processing the initial input string\n",
    "print(input_str := \"<|endoftext|> Hello world\", end=\"\")\n",
    "embed_buffer = compute_embed(tknzr_encode(input_str))\n",
    "attention_mask = torch.ones(embed_buffer.shape[:2], dtype=torch.long)\n",
    "past_key_values = None\n",
    "\n",
    "# PREFILL - running the model for the input string, getting kv cache and embeddings\n",
    "prefill_output = model_body.forward(\n",
    "    inputs_embeds=embed_buffer, \n",
    "    attention_mask=attention_mask,\n",
    "    past_key_values=past_key_values,\n",
    ")\n",
    "past_key_values = prefill_output.get(\"past_key_values\")\n",
    "predicted_embed = prefill_output.get(\"last_hidden_state\")\n",
    "\n",
    "# DECODE stage: Start the token-by-token generation process\n",
    "for i in range(100):\n",
    "    predicted_probs = model_head(predicted_embed[:, -1, :])\n",
    "    predicted_token = torch.argmax(predicted_probs, dim=-1).item()\n",
    "    print(tknzr_decode(predicted_token), end=\"\")\n",
    "\n",
    "    # Update attention mask and run model with past_key_values for next token\n",
    "    decode_output = model_body.forward(\n",
    "        inputs_embeds=compute_embed(predicted_token), \n",
    "        attention_mask=torch.ones([1,1], dtype=torch.long),\n",
    "        past_key_values=past_key_values,\n",
    "    )\n",
    "    predicted_embed = decode_output.get(\"last_hidden_state\")\n",
    "    past_key_values = decode_output.get(\"past_key_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Seeing how the system works, it *seems* like this process could all be solved with a simple $n \\to 1$ encoder model repeated over and over until a max length or stop token is reached. And you would be mostly right *with one caveat*: **training inefficiency.**\n",
    "\n",
    "Given a training example `\"Hello world and all who live in it\"`, an $n \\to 1$ training formulation would require the following training examples:\n",
    "```sh\n",
    "\"<CLS> Hello\" -> \"world\"\n",
    "\"<CLS> Hello world\" -> \"and\"\n",
    "\"<CLS> Hello world and\" -> \"all\"\n",
    "\"<CLS> Hello world and all\" -> \"who\"\n",
    "\"<CLS> Hello world and all who\" -> \"live\"\n",
    "\"<CLS> Hello world and all who live\" -> \"in\"\n",
    "\"<CLS> Hello world and all who live in\" -> \"it\"\n",
    "\"<CLS> Hello world and all who live in it\" -> \"<PAD>\"\n",
    "```  \n",
    "\n",
    "Assuming we have a regular encoder formulation with **bi-directional reasoning**, the attention would have to be fully recomputed for every new word generation. In contrast, if our attention mechanisms were only **uni-directional**, then we could have the following training formulation from just a restricted attention matrix:\n",
    "\n",
    "```sh\n",
    "INPUT=\"<s> Hello world and all who live in it\"\n",
    "         \\    -\\   -\\   -\\  -\\  -\\  -\\  -\\ -\\\n",
    "OUTPUT=\"Hello world and all who live in  it </s>\"\n",
    "```\n",
    "\n",
    "\n",
    "**Simplified Intuition:** In general, unidirectional reasoning is better when the input sequence grows progressively, since each the input-output maps can be trained in parallel for the whole sequence. Bidirectional reasoning is better when the input sequence does not grow, since each prediction can consider its the entries before and after it. \n",
    "\n",
    "In common nomenclature, this attribute - or, rather, the task of next-token prediction that warrants this attribute - is the difference between **encoder** and **decoder** transformer architectures. \n",
    "> <div><img src=\"imgs/bert-vs-gpt.png\" width=\"600\"/></div>\n",
    ">\n",
    "> **Source: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019)](https://arxiv.org/abs/1810.04805)**-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<details>\n",
    "<summary><b>Math Details:</b></summary>\n",
    "\n",
    "When setting up an autoregressive problem with the end-goal of generating some novel series $x_{a\\ldots b}$ from some conditioning series $x_{0 \\ldots a-1}$, we'd be generating $x_t$ at every timestep for $a \\leq t \\leq b$. As such:\n",
    "\n",
    "- With uni-directional, we're modeling $P(x_{a\\ldots t} \\ | \\ x_{0\\ldots t-1})$ from some starting spot $a$ such that $P(x_{s} \\ | \\ x_{0\\ldots s-1})$ for all $s \\leq t$, so previous predictions $P(x_s)$ do not have to be recomputed for every $t$ increment. This is better when $t$ is growing, since each past generation $P(x_s)$ has a constant definition defined relative to $s$.\n",
    "\n",
    "- With bi-directional reasoning, we'd be modeling $P(x_{a\\ldots t} \\ | \\ x_{0\\ldots t-1})$ such that $P(x_s \\ | \\ x_{0\\ldots t-1})$ for all $s \\leq t$, which would make each and every $P(x_s)$ depend on both past and future token predictions. This is better when $t$ is static, since each $P(x)$ has more information conditioning it but also has a static definition which doesn't change/require recomputation as more entries are introduced.\n",
    "\n",
    "**NOTE:** Bi-directional reasoning is a lot more compelling when the output range is actually a different distribution - such as when we formulate $P(y | x)$ instead of $P(x_b | x_a)$ for disjoint $x$ and $y$ distributions - since our network could reuse generalizable intutions and benefit from the added training exposure. In a more grounded note, recall natural language translation where you just want your input substring translated and nothing more. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 4.3:** Encoders, Decoders, and Encoder-Decoders\n",
    "\n",
    "We previously oversimplified encoders and decoders to their natural language application, so this section hopes to define the terms more generally. Within any machine learning formulation, you deal with the following types of data representations:\n",
    "- **Explicit (Observed) Representation:** The form of data that is human/software interpretable.\n",
    "    - i.e. actual language/encoding of language/data points that go into a model, etc.\n",
    "- **Implicit (Latent) Representation:** The form of data that is optimized into existence for an end-goal.\n",
    "    - i.e. intermediate representation in a multi-layered pipeline, embeddings optimized for similarity search, etc.\n",
    "\n",
    "From that perspective, you generally interact with two kinds of macro-structures in your deep learning pipelines:\n",
    "- **Encoders:** Transform input into some implicit representation with desirable properties (i.e. dimension, semantics, range, etc).\n",
    "- **Decoders:** Transform input directly into some explicit representation (i.e. human/software reasonable data format).\n",
    "\n",
    "With that said, why is BERT considered an encoder architecture? The reason is a bit historical and boils down to the following intuition:\n",
    "- **If our task is to create a simple $n \\to n$ or $n \\to 1$ mapping**, the BERT-like architecture from before is sufficient. It can then be argued that a BERT-backed pipeline has a token-wise MLP which functions as a ***per-token/per-sequence decoder***. But that's confusing, so our previous terms of task-specific heads or classification heads are more common.\n",
    "- **If our task is to generate a new $m$-sequence progressively,** the architecture is insufficient and needs a secondary ***text-generating decoder*** structure which helps to model our data in a different way.\n",
    "\n",
    "In general, a dedicated decoder component has some property that is more conducive to generating the explicit output representation. This could be a sequence, an image, a graph, a physically-constrained system, etc. ***In this case, the point of the decoder is to be more suited for one-token-at-a-time *autoregressive* generation of outputs.***\n",
    "\n",
    "In the original [**\"Attention Is All You Need\" (2017) paper**](https://arxiv.org/abs/1706.03762), the two structures were played to their presumed strengths and were combined together in a form of \"conditioned decoding\" that uses the bidirectional reasoning of one sequence to help generate another sequence with unidirectional one-token-at-a-time generation. The strategy that they use is called **cross-attention**, which is simply attention that factors components from two sequences together. \n",
    "\n",
    "**The end result is an architecture that has two key functionalities:**\n",
    "- Generate token after token autoregressively from the decoder architecture, where each new generated token is included in the input for predicting the one after it.\n",
    "- Frequently inject context from the encoder to the decoder, making sure the generation stays in line with the overall objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<details>\n",
    "<summary><strong>Math Details:</strong></summary>\n",
    "\n",
    "We can show that the attention mechanism from the last notebook can be used to take in both an $n$-element and $m$-element sequence if we select out inputs properly. Consider the case when you have queries/values $K_{1..m}$/$V_{1..m}$ coming from the encoder and keys $Q_{1..n}$ coming from the decoder.\n",
    "\n",
    "> \n",
    "- If $K_i$ and $Q_i$ have the same embedding dimension, then $Q_iK_i^T$ is an $n\\times m$ matrix, as are that matrix's softmax values. In other words:\n",
    " $$\\text{Attention}(K_{1..m}, Q_{1..n}) \\text{ is } n\\times m.$$\n",
    "\n",
    "- Since $V_{1..m}$ is of dimension $m \\times d$ and therefore is multiplicatively compatible with an $n\\times m$ attention matrix:\n",
    " $$\\text{Attention}(K_{1..m}, Q_{1..n}) V_{1..m} \\text{ is } n\\times d$$\n",
    "\n",
    "- Since $\\text{Attention}(K_{1..m}, Q_{1..n}) V_{1..m}$ and $\\text{Attention}(K_{1..n}, Q_{1..n}) V_{1..n}$ are of the same dimension, both can be used interchangeably and/or in series.\n",
    "\n",
    "Therefore, we can use an attention interface to incorporate an $m$-element sequence as context for an $n$-element sequence! Just do that many times over, and you have strong context-driven generation.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 4.4:** Machine Translation with T5-style Encoder-Decoders\n",
    "\n",
    "Since the original 2017 paper, the prominence of the combined encoder-decoder architecture has waned in and out of popularity on a per-application basis. **The use-cases that have remained encoder-decoder-dominant are those where the following criteria are met:**\n",
    "\n",
    "1. **The problem requires reasoning from at least two sequences with potentially-differing lengths.**\n",
    "    - If they're the same length or the output is a strict subset, then an $n \\to n$ mapping with an encoder is sufficient.\n",
    "2. **The problem requires you to progressively generate or continue at least one sequence.**\n",
    "    - If the input sequence does not grow, then an $n \\to 1$ mapping with an encoder is sufficient.\n",
    "3. **The sequences follow disjoint distributions (formats, purposes, modalities, etc).**\n",
    "    - If they follow the same distribution, it's better to feed them both through the same network pathway.\n",
    "4. **The model needs to be lightweight and task-specific for training and/or performance.**\n",
    "    - If the model is allowed to be general/multipurpose, we could feed both sequences through the decoder pathway. *More on that later.*\n",
    "\n",
    "Coincidentally, high-speed machine translation is still one such application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344,
     "referenced_widgets": [
      "a5b3b7a8120a44bdbe6144eb6d1ef3a3",
      "cd0109e8926d4e8f8f7264af1e174c1c",
      "412ae74c61eb49bea9c3658858b50ea9",
      "688beaaf66a34116825763ba491faf22",
      "48f31a3511be4aeab3a3568503898ff2",
      "b2c008e054eb4c3abadf3e7751e195c2",
      "7b80c76e65e241bc9335afb6cb6f76e9",
      "2a85d82c33e04d8b8ab62996afd45a78",
      "ccfb9137b09a4087b169e461c5dc8e0c",
      "8605b133811c409d9046bc666b142ce8",
      "3a26403e3b0d4b19ba96291f985fb41c",
      "9cb6bc84ce3548c88fdfee89ebd16027",
      "1043eede14a046aa9c08fc5310a309ee",
      "c4c9b0a00763411f84251d2bd5ea4be7",
      "41269f96f6e147c9a62b30e050230ae9",
      "acbc3116f8e64e379e8325e1eb23408c",
      "87211ecf012f465ca61576405716f4ae",
      "0f033d878a154a3e9595e2b98748c6a4",
      "c0e882c8aa444dac8965fed6b124366b",
      "e63ab680776747a9a37b1ae573da522d",
      "df520fb7f428449dbcc76a7950656ebe",
      "9f4130da5490446a9db325bf624360ec",
      "ad71c7b089574e9f97b341131cdd7735",
      "c492bf8ffe9e4823878f836fe63c081e",
      "f108e13198ce4b979503bfde278a6040",
      "88e90d2702c94304955179e36e189d6a",
      "6ab7f00200014e96a5793d996d918dc5",
      "337a2cb3aaff41ee87246e4f1ba23826",
      "0dc8b4f1b3f846a2870bacf7e475c875",
      "90e353a41cf342bab3f56d754fa6917d",
      "f526829ec1d345c38cc55c47b0a687d5",
      "9f15ecb854e84457b4581199f4286b31",
      "c44bae85439242da81b09de25e3dd6a9",
      "5ca5fe70f60542bab3fcb8cd63a32596",
      "e822f482c82e45888cdea1043f0b946f",
      "cac4e777e034464abbc7601f6c3e05bc",
      "e933e87300a3461c86338eb6e7e4b152",
      "dac3d16efe2a464db37e98f5d091193c",
      "fdd4ca3271354bfe86245737ca650e22",
      "1e779860fad44264b25b1d0ccfcdb5c3",
      "d08ac1efa02443ac84151ce2ba3d54c0",
      "93a6593a6f5d48fa82492cc964574063",
      "d88d2802236846359d6d3f9b1fa68fa9",
      "b05fb3775acf47d894871a3f95fe2fd6",
      "9e9b6b8c170041bda5231cade5e11d48",
      "4b699788e93e4b5281f550ee78ed0ddd",
      "de23c16f531c4d17bf5d8a1b82880183",
      "fe545047949445b291f79d0c8a0cab4b",
      "625fbcb95f1241f2a528025115cccb23",
      "a67f39d8379d4024957c8cd5dacbe5cb",
      "b6f917a0813343aea573d1488ebc7b14",
      "ece679963bed4fffa680647163d1f37d",
      "0afbfc6656cc4805a588fcc77b29ac45",
      "0c7af8fb4cb04ffd8b67be6bff90f53a",
      "07c61f5fa45842e19156067d4055c0bb"
     ]
    },
    "executionInfo": {
     "elapsed": 37923,
     "status": "ok",
     "timestamp": 1694463155452,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "NzM4kxtk-uMO",
    "outputId": "9c283cab-592e-4a46-8a3f-08841bb4379e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline('translation_en_to_fr', model='t5-base', device=\"cuda\")\n",
    "translator([\"Hello World! How's it going?\", \"What's your name?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this problem meets all of our requirements:\n",
    "- The translation of a passage is neither inherently $n \\to n$ nor $n \\to 1$.\n",
    "- The output needs to be generated one token at a time while being a cohesive sequence.\n",
    "- The two languages likely follow fundamentally different distributions.\n",
    "- The model should be fast, lightweight, and inherently limited in purpose, with flexibility being a secondary concern. \n",
    "\n",
    "#### Preprocessing and Postprocessing\n",
    "\n",
    "We can investigate where the model begins and ends to get some insight into how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1468,
     "status": "ok",
     "timestamp": 1694463156899,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "eKS8JpQQVAOi",
    "outputId": "be756a07-d894-4e4f-d8fe-a5c3f7db774c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_en = \"Hello World! How's it going?\"\n",
    "resp_fr = translator(text_en)\n",
    "text_fr = resp_fr[0]['translation_text']\n",
    "\n",
    "tknzr = translator.tokenizer\n",
    "tokens_ins = [tknzr.decode(x) for x in tknzr.encode(text_en)]\n",
    "tokens_in2 = [tknzr.decode(x) for x in translator.preprocess(text_en)['input_ids'][0]]\n",
    "tokens_out = [tknzr.decode(x) for x in tknzr.encode(text_fr)]\n",
    "print(f\"Inputs Into Preprocessing: {' | '.join(tokens_ins)}\")\n",
    "print(f\"Inputs Into Model Forward: {' | '.join(tokens_in2)}\")\n",
    "print(f\"Output From Model Forward: {' | '.join(tokens_out)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQPlY5-_Xi1P"
   },
   "source": [
    "**On observation, we can see that:**\n",
    "- The model uses the same tokenizer for both input and output language.\n",
    "- The proprocessing pipeline adds extra task instructions to explain the task.\n",
    "\n",
    "This is because the T5 model on its own was trained for multiple tasks, and the en2fr translation task is merely one of its objectives. A custom-made model may or may not train for tasks outside of its immediate objective, but doing so does help the model learn transferable intuitions. Depending on the training/architecture details and learning capacity:\n",
    "\n",
    "- **The model *could* be overloaded**, unable to latch on to the multi-task formulation and experiencing degredation or overfitting. \n",
    "- **The model *could* learn a reasonable language prior**, leveraging the shared structure of the tasks and instructions to make further fine-tuning easier and quicker.\n",
    "- **The model *could* generalize beyond its original training** with the ability to reason about new instructions, solve novel tasks, and make non-trivial connections. In other words, exhibiting emergent behaviors like **in-context learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder Forward Pass\n",
    "\n",
    "We can investigate this architecture further by displaying the model descriptions, but you'll notice they're quite verbose. We've synthesized them here for ease of viewing:\n",
    "\n",
    "```python\n",
    "translator.model           ## See that there's a lot of stuff going on here\n",
    "translator.model.encoder   ## See that this looks a lot like the BERT model\n",
    "translator.model.decoder   ## See that this looks roughly the same and wonder what changed\n",
    "```\n",
    "\n",
    "<div><img src=\"imgs/t5-architecture.png\" \n",
    "     alt=\"Encoder-Decoder Architecture\"\n",
    "     width=\"1200\"/></div>\n",
    "\n",
    "Recall that in the previous forward pass deconstructions, it was pretty easy to exhibit the following properties: \n",
    "- We could manually specify the inputs on a per-component basis by investigating the model definitions.\n",
    "- We could stream generation by repeating the forward pass one token at a time, accumulating the results in a buffer and modifying/printing things as necessary.\n",
    "\n",
    "In this section, we will repeat the process again but modularize it into a **stream generator** format for easy of use. The goal here is to have a system that feeds back the generated tokens - effectively hiding the complexity therein - while allowing the user to iterate over and customize the results as soon as they come in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_token_generator(pipeline, model=None, tokenizer=None, max_tokens=50):\n",
    "    \n",
    "    ## This method initializes a generator which will yield a stream of tokens\n",
    "    model = pipeline.model or model\n",
    "    tknzr = pipeline.tokenizer or tokenizer\n",
    "    encoder = getattr(model, \"encoder\", None)\n",
    "    decoder = ( ## Non-exhaustive resolution\n",
    "        getattr(model, \"decoder\", None) \n",
    "        or getattr(model, \"model\", None) \n",
    "        or getattr(model, \"transformer\", None)\n",
    "    )\n",
    "    lm_head = getattr(model, \"lm_head\", None)\n",
    "    dev = decoder.device\n",
    "    \n",
    "    def token_generator(\n",
    "        encoder_input: str = \"\",\n",
    "        decoder_input: str = \"\",\n",
    "        max_tokens: int = max_tokens\n",
    "    ):\n",
    "        encoder_input_idxs = tknzr.encode(encoder_input)[:-1] * bool(encoder_input)\n",
    "        decoder_input_idxs = tknzr.encode(decoder_input)[:-1] * bool(decoder_input)\n",
    "        decoder_inputs = {}\n",
    "\n",
    "        ## [EncDec] Convert our context into conditioning hidden state for decoder\n",
    "        if encoder:\n",
    "            encoder_inputs = {\"input_ids\": torch.tensor([encoder_input_idxs], device=dev)}\n",
    "            encoder_outputs = encoder(**encoder_inputs)\n",
    "            decoder_inputs[\"encoder_hidden_states\"] = encoder_outputs.last_hidden_state\n",
    "        elif encoder_input_idxs:\n",
    "            print(\"`encoder_input` specified despite no encoder being available. Ignoring\")\n",
    "            \n",
    "        ## [EncDec/Dec] Accumulate decoding starting from <pad> until </s> (eos) is reached.\n",
    "        buffer_token_idxs = [] if (tknzr.pad_token_id is None) else [tknzr.pad_token_id]\n",
    "        buffer_token_idxs += decoder_input_idxs\n",
    "        buffer_token_str = \"\"\n",
    "        max_length = len(buffer_token_idxs) + max_tokens\n",
    "        while len(buffer_token_idxs) < max_length:\n",
    "            \n",
    "            ## Pass the current buffer into the decoder, along with encoder states\n",
    "            ##   NOTE: This one just uses the last hidden state, but some use many more...\n",
    "            \n",
    "            decoder_inputs[\"input_ids\"] = torch.tensor([buffer_token_idxs], device=dev).long()\n",
    "            decoder_outputs = decoder(**decoder_inputs)\n",
    "            model_outputs = lm_head(decoder_outputs.last_hidden_state)\n",
    "        \n",
    "            ## Get the most likely next token and add it to the buffer\n",
    "            try: sampled_token_idx = torch.argmax(model_outputs, -1)[0][-1].item()\n",
    "            except: break\n",
    "            buffer_token_idxs += [sampled_token_idx]\n",
    "            buffer_token_old = buffer_token_str\n",
    "            buffer_token_str = tknzr.decode(buffer_token_idxs)\n",
    "            buffer_token_new = buffer_token_str[len(buffer_token_old):]\n",
    "\n",
    "            ## Yield (output while keeping spot in the generator call) next token.\n",
    "            ## If it's end-of-string </s>, break the loop.\n",
    "            if sampled_token_idx == tknzr.eos_token_id:\n",
    "                break\n",
    "            if buffer_token_new:\n",
    "                yield buffer_token_new\n",
    "    \n",
    "    return token_generator\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "streamer = get_token_generator(translator)\n",
    "input_raw_str = \"translate English to French: Hello World! How's it going?</s>\"\n",
    "\n",
    "for token in streamer(encoder_input = input_raw_str):\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH48v0hkDLTG"
   },
   "source": [
    "<br>\n",
    "\n",
    "## **Part 4.5:** Creating More General-Purpose Models\n",
    "\n",
    "Recall our assumptions about when to use encoder-decoders, and focus in on 3 and 4.  \n",
    "\n",
    "1. The problem requires reasoning from at least two sequences with potentially-differing lengths.\n",
    "2. The problem requires you to progressively generate or continue at least one sequence.\n",
    "3. **The sequences follow disjoint distributions (formats, purposes, modalities, etc).**\n",
    "    - If they follow the same distribution, it's better to feed them both through the same network pathway.\n",
    "4. **The model needs to be lightweight and task-specific for training and/or performance.**\n",
    "    - If the model is allowed to be general/multipurpose, we could feed both sequences through the decoder pathway.\n",
    "  \n",
    "With this in mind, let's pull in an encoder-decoder model which attempts to be less lightweight and more general: [**Google's Flan-T5 class of models**](https://huggingface.co/docs/transformers/en/model_doc/flan-t5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "flan_t5_pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
    "\n",
    "streamer = get_token_generator(flan_t5_pipe)\n",
    "input_raw_str = \"translate English to French: Hello World! How's it going?</s>\"\n",
    "\n",
    "for token in streamer(encoder_input = input_raw_str):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these, let's list out the possible options. Let's assuming we have an encoder-decoder formulation with premise $P$, question $Q$, and answer $A$. The T5 model is advertised to reason about it as follows: \n",
    "\n",
    "> `Encoder(\"{P}: {Q}\")` conditions the generation of `Decoder(\"<pad>\")`, which itself is optimized to produce the answer $A$ by generating one token at a time until the stop token is reached.\n",
    "\n",
    "This format has been reinforced through significant pretraining by formatting the training data to subscribe to this format. This results in a strong **inductive bias** (or **prior**) to understand and respect the format.\n",
    "\n",
    "> <div><img src=\"imgs/t5-pic.jpg\" width=\"800\"/></div>\n",
    ">\n",
    "> **Source: [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683v4)**\n",
    "\n",
    "Furthermore, the Flan version of the T5 model is further trained with more complex tasks so that the model may generalize beyond its trained objective towards **in-context learning**, which is the ability to solve novel tasks merely by being told what to do as part of the context.\n",
    "\n",
    "> <div><img src=\"imgs/t5-flan2-spec.jpg\" width=\"1000\"/></div>\n",
    ">\n",
    "> **Source: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416v5)**\n",
    "\n",
    "To test out these capabilities, we can create a simple dataset of questions that we'd like to test our model against in a sort of **evaluation process**. See how it performs, and see if you can't implement the exercise TODOs at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {   # Simple Translation\n",
    "        \"premise\": \"Translate from English to Spanish\",\n",
    "        \"question\": \"The book is on the table.\",\n",
    "        \"answer\": \"El libro está en la mesa.\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"The cat is sleeping on the sofa.\",\n",
    "            \"answer\": \"El gato está durmiendo en el sofá.\"\n",
    "    }},{# Commonsense Reasoning\n",
    "        \"premise\": \"Answer the question using commonsense knowledge\",\n",
    "        \"question\": \"Why can't a fish live out of water?\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"Why can't humans breathe underwater?\",\n",
    "            \"answer\": \"Humans can't breathe underwater because they need air, not water, to fill their lungs.\"\n",
    "    }},{# Creative Story Generation\n",
    "        \"premise\": \"Continue the story with a creative twist\",\n",
    "        \"question\": \"Once upon a time, in a forest far away, there was a small bear named Timmy who loved honey.\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"A princess woke up one day to find her castle floating in the sky.\",\n",
    "            \"answer\": \"As she looked outside, she saw a giant eagle carrying the castle on its back, flying towards the mountains.\"\n",
    "    }},{  # Mathematical Problem Solving\n",
    "        \"premise\": \"Solve the mathematical problem\",\n",
    "        \"question\": \"What is the square root of 144?\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"What is the cube of 3?\",\n",
    "            \"answer\": \"27.\"\n",
    "    }},{# Fact-based Question Answering\n",
    "        \"premise\": \"Answer based on factual knowledge\",\n",
    "        \"question\": \"Who was the first person to walk on the moon?\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"Who was the first president of the United States?\",\n",
    "            \"answer\": \"The first president of the United States was George Washington.\"\n",
    "    }},{# Conversational Continuation\n",
    "        \"premise\": \"Continue the conversation naturally\",\n",
    "        \"question\": \"User: Can you help me with directions? Agent:\",\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"User: What’s the weather like today? Agent:\",\n",
    "            \"answer\": \"It’s sunny and warm with a light breeze.\"\n",
    "    }},{# Conversational Continuation\n",
    "        \"premise\": \"Continue the conversation naturally\",\n",
    "        \"question\": (\n",
    "            \"User: Can you help me with directions? Agent: Sure, where to and where from?\"\n",
    "            \" User: I'd like to get from LA to San Jose today. What's the best road? Agent: \"\n",
    "        ),\n",
    "        \"few_shot\": {\n",
    "            \"question\": \"User: What’s the weather like today? Agent:\",\n",
    "            \"answer\": \"It’s sunny and warm with a light breeze.\"\n",
    "    }}\n",
    "]\n",
    "\n",
    "streamer = get_token_generator(flan_t5_pipe)\n",
    "\n",
    "for entry in dataset:\n",
    "    P, Q = entry['premise'], entry['question']\n",
    "    FSP, FSQ = entry['few_shot']['question'], entry['few_shot']['answer']\n",
    "    inputs = {\n",
    "        \"encoder_input\": f\"{P}: {Q}</s>\",\n",
    "        \"decoder_input\": \"\",\n",
    "        # \"encoder_input\": f\"{P}: \",\n",
    "        # \"decoder_input\": f\"{FSQ}? {FSA}</s>{Q}? \",\n",
    "    }\n",
    "    print(f\"{P}: {Q}\")\n",
    "    for token in streamer(**inputs):\n",
    "        print(token, end=\"\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "## EXERCISE 1: Incorporate Few-Shot (in this case just one-shot) conditioning.\n",
    "## EXERCISE 2: Remove the encoder from the equation and progress towards in-context learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You'll notice that the base Flan-T5 model is a solid alright-at-best in terms of consistent text generation and pretty poor in terms of out-of-the-box utility. **The real use-case that the T5 family excels at is task-specific fine-tuning:**\n",
    "- Because the models are actually pretty small, they're fairly straight-forward to fine-tune.\n",
    "- Because the encoder has already been fine-tuned with a variety of natural-language, it serves as a base model which can start reasoning about a new input format with relatively few gradient updates if it shares features with one of the heavily pre-trained tasks. \n",
    "- Because the decoder is only there to generate a novel sequence (and not necessarily reason about context that comes from a disjoint distribution like a question or premise), it experiences relatively little autoregressive drift and is less likely to derail.\n",
    "\n",
    "Having said that, there are some much-desired features which are less natural with this formulation:\n",
    "- Few-shot prompting being handled from the encoder is more cumbersome and requires the encoder to handle both input-like and output-like data distributions.\n",
    "- At the same time, the decoder overall has less training data to work with due to bifurcation between the two pathways.\n",
    "\n",
    "#### Swapping to a Decoder-Only Model\n",
    "\n",
    "We previously tried out the GPT-2 model to illustrate how decoder-only models function. It wasn't very good, which makes sense since it was an early model primarily suited for fine-tuning workflows. Decoder models that don't derail tend to require larger architectures, but a class of smaller decoder-only models have emerged for fine-tuning purposes. This class of models is often referred to as SLMs (Small Language Models) and retroactively includes most of the T5 models above. \n",
    "\n",
    "For our exercise, we are going to pull one of Microsoft's smallest Phi SLMs, [**Phi-1.5**](https://huggingface.co/microsoft/phi-1_5). Much like the Flan-T5 above, we will once-again bypass several of the required customizations necessary to make this system work for actual use-cases, mainly:\n",
    "- We will not be fine-tuning it for our purposes.\n",
    "- We will subscribe to the recommended format, but will not perform early stopping/extensive prompt engineering.\n",
    "- We are once again using an out-of-the-box model from over a year ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pipe = pipeline(\"text-generation\", model=\"microsoft/phi-1_5\", stop_token=\"\\n\", device=\"cuda\")\n",
    "# decoder_pipe = pipeline(\"text-generation\", model=\"gpt2\", stop_token=\"\\n\")\n",
    "streamer = get_token_generator(decoder_pipe)\n",
    "\n",
    "for entry in dataset:\n",
    "    P, Q = entry['premise'], entry['question']\n",
    "    FSP, FSQ = entry['few_shot']['question'], entry['few_shot']['answer']\n",
    "    inputs = {\n",
    "        \"decoder_input\": f\"{P}: {Q}\\n\\nAnswer: \",\n",
    "    }\n",
    "    print(\"*\" * 64)\n",
    "    for token in streamer(**inputs):\n",
    "        print(token, end=\"\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "## EXERCISE: Incorporate Few-Shot (in this case just one-shot) conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As you can see, this model has an entirely different set of pros and cons. **Specifically, it's actually being flexible and facilitating all of these new premises all through a single input pathway, while having more issues attributed to run-on generation in contrast to complete inability to generalize!** \n",
    "\n",
    "This emergent behavior of **in-context learning** and is the main enabler of the [**prompt engineering**](https://en.wikipedia.org/wiki/Prompt_engineering) paradigm which will be further discussed in Notebooks 6 and 7!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-EfMhrrqI6i"
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Wrapping Up**</font>\n",
    "\n",
    "At this point, we've seen how language models are able to generate completely new text by taking a language encoding as context. This opens up a lot of new possibilities and leaves open a lot of open questions, but at least we're now at the cutting edge and have the capacity to do some pretty powerful stuff with limited compute budget! In the next section, we're going to see a use case for which encoder-decoders really shine; **multi-modal generation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please Run When You're Done!\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Mc3Vnp9Uhn3-J_yK8QsQRtnQEf6xgTmT",
     "timestamp": 1694359379674
    },
    {
     "file_id": "1u9gcXs58VPBKEE0ItHeRLP0uP-wmTO8I",
     "timestamp": 1694318698209
    },
    {
     "file_id": "16SWgOvVC9HUbtGzlmYWABbO2WXQaWN_w",
     "timestamp": 1694313364012
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07c61f5fa45842e19156067d4055c0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0afbfc6656cc4805a588fcc77b29ac45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c7af8fb4cb04ffd8b67be6bff90f53a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dc8b4f1b3f846a2870bacf7e475c875": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f033d878a154a3e9595e2b98748c6a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1043eede14a046aa9c08fc5310a309ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87211ecf012f465ca61576405716f4ae",
      "placeholder": "​",
      "style": "IPY_MODEL_0f033d878a154a3e9595e2b98748c6a4",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "1e779860fad44264b25b1d0ccfcdb5c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a85d82c33e04d8b8ab62996afd45a78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "337a2cb3aaff41ee87246e4f1ba23826": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a26403e3b0d4b19ba96291f985fb41c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41269f96f6e147c9a62b30e050230ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df520fb7f428449dbcc76a7950656ebe",
      "placeholder": "​",
      "style": "IPY_MODEL_9f4130da5490446a9db325bf624360ec",
      "value": " 892M/892M [00:04&lt;00:00, 161MB/s]"
     }
    },
    "412ae74c61eb49bea9c3658858b50ea9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a85d82c33e04d8b8ab62996afd45a78",
      "max": 1208,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ccfb9137b09a4087b169e461c5dc8e0c",
      "value": 1208
     }
    },
    "48f31a3511be4aeab3a3568503898ff2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b699788e93e4b5281f550ee78ed0ddd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a67f39d8379d4024957c8cd5dacbe5cb",
      "placeholder": "​",
      "style": "IPY_MODEL_b6f917a0813343aea573d1488ebc7b14",
      "value": "Downloading (…)/main/tokenizer.json: 100%"
     }
    },
    "5ca5fe70f60542bab3fcb8cd63a32596": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e822f482c82e45888cdea1043f0b946f",
       "IPY_MODEL_cac4e777e034464abbc7601f6c3e05bc",
       "IPY_MODEL_e933e87300a3461c86338eb6e7e4b152"
      ],
      "layout": "IPY_MODEL_dac3d16efe2a464db37e98f5d091193c"
     }
    },
    "625fbcb95f1241f2a528025115cccb23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "688beaaf66a34116825763ba491faf22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8605b133811c409d9046bc666b142ce8",
      "placeholder": "​",
      "style": "IPY_MODEL_3a26403e3b0d4b19ba96291f985fb41c",
      "value": " 1.21k/1.21k [00:00&lt;00:00, 37.8kB/s]"
     }
    },
    "6ab7f00200014e96a5793d996d918dc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b80c76e65e241bc9335afb6cb6f76e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8605b133811c409d9046bc666b142ce8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87211ecf012f465ca61576405716f4ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88e90d2702c94304955179e36e189d6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f15ecb854e84457b4581199f4286b31",
      "placeholder": "​",
      "style": "IPY_MODEL_c44bae85439242da81b09de25e3dd6a9",
      "value": " 147/147 [00:00&lt;00:00, 6.84kB/s]"
     }
    },
    "90e353a41cf342bab3f56d754fa6917d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93a6593a6f5d48fa82492cc964574063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9cb6bc84ce3548c88fdfee89ebd16027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1043eede14a046aa9c08fc5310a309ee",
       "IPY_MODEL_c4c9b0a00763411f84251d2bd5ea4be7",
       "IPY_MODEL_41269f96f6e147c9a62b30e050230ae9"
      ],
      "layout": "IPY_MODEL_acbc3116f8e64e379e8325e1eb23408c"
     }
    },
    "9e9b6b8c170041bda5231cade5e11d48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4b699788e93e4b5281f550ee78ed0ddd",
       "IPY_MODEL_de23c16f531c4d17bf5d8a1b82880183",
       "IPY_MODEL_fe545047949445b291f79d0c8a0cab4b"
      ],
      "layout": "IPY_MODEL_625fbcb95f1241f2a528025115cccb23"
     }
    },
    "9f15ecb854e84457b4581199f4286b31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f4130da5490446a9db325bf624360ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5b3b7a8120a44bdbe6144eb6d1ef3a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd0109e8926d4e8f8f7264af1e174c1c",
       "IPY_MODEL_412ae74c61eb49bea9c3658858b50ea9",
       "IPY_MODEL_688beaaf66a34116825763ba491faf22"
      ],
      "layout": "IPY_MODEL_48f31a3511be4aeab3a3568503898ff2"
     }
    },
    "a67f39d8379d4024957c8cd5dacbe5cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acbc3116f8e64e379e8325e1eb23408c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad71c7b089574e9f97b341131cdd7735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c492bf8ffe9e4823878f836fe63c081e",
       "IPY_MODEL_f108e13198ce4b979503bfde278a6040",
       "IPY_MODEL_88e90d2702c94304955179e36e189d6a"
      ],
      "layout": "IPY_MODEL_6ab7f00200014e96a5793d996d918dc5"
     }
    },
    "b05fb3775acf47d894871a3f95fe2fd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2c008e054eb4c3abadf3e7751e195c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6f917a0813343aea573d1488ebc7b14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0e882c8aa444dac8965fed6b124366b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c44bae85439242da81b09de25e3dd6a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c492bf8ffe9e4823878f836fe63c081e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_337a2cb3aaff41ee87246e4f1ba23826",
      "placeholder": "​",
      "style": "IPY_MODEL_0dc8b4f1b3f846a2870bacf7e475c875",
      "value": "Downloading (…)neration_config.json: 100%"
     }
    },
    "c4c9b0a00763411f84251d2bd5ea4be7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0e882c8aa444dac8965fed6b124366b",
      "max": 891646390,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e63ab680776747a9a37b1ae573da522d",
      "value": 891646390
     }
    },
    "cac4e777e034464abbc7601f6c3e05bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d08ac1efa02443ac84151ce2ba3d54c0",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93a6593a6f5d48fa82492cc964574063",
      "value": 791656
     }
    },
    "ccfb9137b09a4087b169e461c5dc8e0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cd0109e8926d4e8f8f7264af1e174c1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2c008e054eb4c3abadf3e7751e195c2",
      "placeholder": "​",
      "style": "IPY_MODEL_7b80c76e65e241bc9335afb6cb6f76e9",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "d08ac1efa02443ac84151ce2ba3d54c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d88d2802236846359d6d3f9b1fa68fa9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dac3d16efe2a464db37e98f5d091193c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de23c16f531c4d17bf5d8a1b82880183": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ece679963bed4fffa680647163d1f37d",
      "max": 1389353,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0afbfc6656cc4805a588fcc77b29ac45",
      "value": 1389353
     }
    },
    "df520fb7f428449dbcc76a7950656ebe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e63ab680776747a9a37b1ae573da522d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e822f482c82e45888cdea1043f0b946f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdd4ca3271354bfe86245737ca650e22",
      "placeholder": "​",
      "style": "IPY_MODEL_1e779860fad44264b25b1d0ccfcdb5c3",
      "value": "Downloading (…)ve/main/spiece.model: 100%"
     }
    },
    "e933e87300a3461c86338eb6e7e4b152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d88d2802236846359d6d3f9b1fa68fa9",
      "placeholder": "​",
      "style": "IPY_MODEL_b05fb3775acf47d894871a3f95fe2fd6",
      "value": " 792k/792k [00:00&lt;00:00, 1.02MB/s]"
     }
    },
    "ece679963bed4fffa680647163d1f37d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f108e13198ce4b979503bfde278a6040": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90e353a41cf342bab3f56d754fa6917d",
      "max": 147,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f526829ec1d345c38cc55c47b0a687d5",
      "value": 147
     }
    },
    "f526829ec1d345c38cc55c47b0a687d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fdd4ca3271354bfe86245737ca650e22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe545047949445b291f79d0c8a0cab4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c7af8fb4cb04ffd8b67be6bff90f53a",
      "placeholder": "​",
      "style": "IPY_MODEL_07c61f5fa45842e19156067d4055c0bb",
      "value": " 1.39M/1.39M [00:00&lt;00:00, 1.43MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
