{
    "course": "NVIDIA Deep Learning Institute's Instructor-Led Course titled \"Rapid Application Development with LLMs\"",
    "filenames": [
        "00_jupyterlab.ipynb",
        "01_llm_intro.ipynb",
        "02_llm_intake.ipynb",
        "03_encoder_task.ipynb",
        "04_seq2seq.ipynb",
        "05_multimodal.ipynb",
        "06_llm_server.ipynb",
        "07_intro_agentics.ipynb",
        "08_assessment.ipynb",
        "75_langgraph.ipynb",
        "98_VLM_Launch.ipynb",
        "Table_of_Contents.ipynb"
    ],
    "slides": " <img src='/file=slides/Slide01.PNG'>: Introduction slide featuring the NVIDIA logo and the topic 'Rapid Application Development using Large Language Models'.\n\n<img src='/file=slides/Slide02.PNG'>: Illustration of Large Language Models as backbones for language understanding, including classification and generation tasks.\n\n<img src='/file=slides/Slide03.PNG'>: Conceptual framework for Language-Reasoning Systems, divided into GenAI Model and GenAI Orchestration.\n\n<img src='/file=slides/Slide04.PNG'>: Diagram of core components of Generative AI, including text and animation data types.\n\n<img src='/file=slides/Slide05.PNG'>: Course objectives slide focusing on understanding LLMs, using them for language reasoning, text generation, chat assistants, and LLM server deployment.\n\n<img src='/file=slides/Slide06.PNG'>: Prerequisites for deep learning, emphasizing intermediate Python skills and tools like NumPy, Keras, and PyTorch.\n\n<img src='/file=slides/Slide07.PNG'>: Diagram illustrating the process of machine learning and deep learning, from raw data to trained models.\n\n<img src='/file=slides/Slide08.PNG'>: Conceptual framework for Deep Learning, focusing on Machine Learning with Deep Models, including differentiable feature extractors and task heads.\n\n<img src='/file=slides/Slide09.PNG'>: Diagram showing the transformation of an input image through a neural network to a probability distribution over classes.\n\n<img src='/file=slides/Slide10.PNG'>: Illustration of the process of training a neural network to classify images, including loss function and gradient updates.\n\n<img src='/file=slides/Slide11.PNG'>: Flow diagram illustrating the progression from deep learning to language models, showing how deep learning principles are applied to language reasoning.\n\n<img src='/file=slides/Slide12.PNG'>: Slide discussing tokenization and ordered sequences of classes in language processing.\n\n<img src='/file=slides/Slide13.PNG'>: Comparison of GPT-2 and BERT models, highlighting their sequence prediction and forecasting capabilities.\n\n<img src='/file=slides/Slide14.PNG'>: Promotional graphic for HuggingFace, emphasizing its open-source large model community.\n\n<img src='/file=slides/Slide15.PNG'>: Benefits of using HuggingFace, including a large repository of models and ease of use.\n\n<img src='/file=slides/Slide16.PNG'>: Webpage demonstrating how to pull in a simple model, specifically a lightweight BERT model for text classification.\n\n<img src='/file=slides/Slide17.PNG'>: Instructions on using the Transformers library to import a pipeline and load a model.\n\n<img src='/file=slides/Slide18.PNG'>: Terminal window showing the installation of a lightweight BERT model using the 'transformers' library.\n\n<img src='/file=slides/Slide19.PNG'>: Slide titled 'Running Your Simple Model' with a list of pipes and their numerical scores.\n\n<img src='/file=slides/Slide20.PNG'>: PowerShell script with three pipelined commands, demonstrating text input and output processing.\n\n<img src='/file=slides/Slide21.PNG'>: Diagram illustrating the components of a pipeline process, including preprocess, forward, and postprocess stages.\n\n<img src='/file=slides/Slide22.PNG'>: Screenshot of a code editor with a Python script for a machine learning model training process.\n\n<img src='/file=slides/Slide23.PNG'>: Diagram illustrating the components of a pipeline in data processing, specifically for a TensorFlow model.\n\n<img src='/file=slides/Slide24.PNG'>: Slide focusing on 'HF Pipeline Options' related to automatic end-to-end pipelines.\n\n<img src='/file=slides/Slide25.PNG'>: Diagram titled 'HF Pipeline Options' outlining various automatic end-to-end pipelines in NLP and Computer Vision.\n\n<img src='/file=slides/Slide26.PNG'>: Diagram illustrating the 'Ordered Sequence of Classes' in a network or graph structure.\n\n<img src='/file=slides/Slide27.PNG'>: Diagram illustrating the concept of dealing with classes in machine learning, focusing on embedding layers.\n\n<img src='/file=slides/Slide28.PNG'>: Slide discussing two approaches to handling classes in neural networks: 'All At Once' and 'One Sequence Entry At A Time'.\n\n<img src='/file=slides/Slide29.PNG'>: Diagram illustrating the process of dealing with ordered classes, showing a flow from left to right.\n\n<img src='/file=slides/Slide30.PNG'>: Diagram illustrating a process flow for 'Transformers for Sequence Reasonings' with an 'Ordered Sequence of Classes'.\n\n<img src='/file=slides/Slide31.PNG'>: Diagram illustrating a transformer model for sequence reasoning, specifically focusing on the ordered sequence of classes.\n\n<img src='/file=slides/Slide32.PNG'>: Slide presenting a diagram illustrating the concept of 'Transformers Intuition' with a focus on the importance of attention in processing information.\n\n<img src='/file=slides/Slide33.PNG'>: Diagram illustrating the 'Transformer Value Add' concept, including a residual connection with a sigma function.\n\n<img src='/file=slides/Slide34.PNG'>: Slide introducing the BERT Model, described as a powerful encoder model, divided into Pretraining and Fine-tuning sections.\n\n<img src='/file=slides/Slide35.PNG'>: Diagram illustrating the transition from Transformer to BERT models in natural language processing.\n\n<img src='/file=slides/Slide36.PNG'>: Diagram illustrating the components of a common encoder backbone used in n-sequence encodings.\n\n<img src='/file=slides/Slide37.PNG'>: Slide titled 'Encoder Model Options' with a focus on 'Common Derivatives/Flavors'.\n\n<img src='/file=slides/Slide38.PNG'>: Slide focusing on Zero-Shot Classification for any class, featuring a diagram illustrating the process.\n\n<img src='/file=slides/Slide39.PNG'>: Slide presenting the benefits of BERT-Style Encoders, including a simple input-output schema and strong supervision.\n\n<img src='/file=slides/Slide40.PNG'>: Slide discussing 'Variable-Length Outputs' with a focus on Range Selection Sufficient, But Limited.\n\n<img src='/file=slides/Slide41.PNG'>: Slide introducing an encoder-only insufficient model for general m->n translation.\n\n<img src='/file=slides/Slide42.PNG'>: Conceptual framework for a machine learning model that transitions from encoding to decoding.\n\n<img src='/file=slides/Slide43.PNG'>: Diagram illustrating the concept of 'From Encoding to Decoding' in the context of predicting words in a sequence.\n\n<img src='/file=slides/Slide44.PNG'>: Slide presenting a diagram of an Encoder-Decoder model, described as the 'Best of Both Worlds (Possibly)'.\n\n<img src='/file=slides/Slide45.PNG'>: Slide discussing 'Context Limits' and why autoregressing indefinitely is not great.\n\n<img src='/file=slides/Slide46.PNG'>: Slide illustrating the concept of unidirectional reasoning in decoder training for language models.\n\n<img src='/file=slides/Slide47.PNG'>: Slide presenting a comparison between 'Encoders' and 'Decoders' in the context of machine learning models.\n\n<img src='/file=slides/Slide48.PNG'>: Diagram of an Encoder-Decoder model, which is a type of neural network architecture.\n\n<img src='/file=slides/Slide49.PNG'>: Slide presenting an 'Encoder-Decoder' model, described as the 'Best of Both Worlds (Possibly)'.\n\n<img src='/file=slides/Slide50.PNG'>: Diagram illustrating the concept of 'Progressing Towards Zero-Shot Understanding Tasks Without Explicitly Training To Them.'\n\n<img src='/file=slides/Slide51.PNG'>: Slide displaying a diagram illustrating the concept of 'Arbitrary Translation' in the context of Seq2Seq models.\n\n<img src='/file=slides/Slide52.PNG'>: Slide introducing an Instruction-Following Seq2Seq model with Arbitrary Translation.\n\n<img src='/file=slides/Slide53.PNG'>: Slide discussing the benefits of transformers in the context of language processing.\n\n<img src='/file=slides/Slide54.PNG'>: Slide presenting a 'Table Question Answering Model' and a 'Document Question Answering Model'.\n\n<img src='/file=slides/Slide55.PNG'>: Slide featuring a waveform graph and a Mel-frequency spectrogram, focusing on Speech-Guided Encoders in Audio-In Systems.\n\n<img src='/file=slides/Slide56.PNG'>: Diagram illustrating the process of speech-guided text generation.\n\n<img src='/file=slides/Slide57.PNG'>: Diagram illustrating the process of Image-Guided Encoders within Image-In Systems.\n\n<img src='/file=slides/Slide58.PNG'>: Diagram illustrating the process of Image-Guided Text Generation.\n\n<img src='/file=slides/Slide59.PNG'>: Grid of heatmaps showing the attention patterns of a model trained on images of dogs, cats, elephants, and zebras.\n\n<img src='/file=slides/Slide60.PNG'>: Diagram illustrating a process for generating text based on images, specifically for a game like Jeopardy.\n\n<img src='/file=slides/Slide61.PNG'>: Flowchart detailing a process for synergizing encoders to resolve modality gaps via similarity.\n\n<img src='/file=slides/Slide62.PNG'>: Diagram explaining the concept of Multimodal Retrievers, which are used to resolve modality gaps via similarity.\n\n<img src='/file=slides/Slide63.PNG'>: Slide questioning if transformers can generate images, featuring a diagram on the left side.\n\n<img src='/file=slides/Slide64.PNG'>: Sequence of three photographs illustrating the concept of a policy network in robotics.\n\n<img src='/file=slides/Slide65.PNG'>: Diagram illustrating a process for generating images using transformers.\n\n<img src='/file=slides/Slide66.PNG'>: Slide presenting 'Core Transformer Benefits' with a subtitle 'When Does It Shine'.\n\n<img src='/file=slides/Slide67.PNG'>: Slide presenting a conceptual framework for a system that transitions from a small model to a big model, from a single-user to many-user, and from text generation to web browsing.\n\n<img src='/file=slides/Slide68.PNG'>: Slide presenting a conceptual framework for a system that transitions from a small model to a big model, from a single-user to many-user, and from text generation to web browsing.\n\n<img src='/file=slides/Slide69.PNG'>: Flowchart related to 'Model Deployment' with a focus on 'Construction Perspective'.\n\n<img src='/file=slides/Slide70.PNG'>: Diagram illustrating the Model Deployment process from a Query Perspective.\n\n<img src='/file=slides/Slide71.PNG'>: Diagram illustrating the architecture of Large Model Hosting Platforms in an OpenAI-Style.\n\n<img src='/file=slides/Slide72.PNG'>: Conceptual diagram related to 'Query Router Access' in a technical context.\n\n<img src='/file=slides/Slide73.PNG'>: Diagram illustrating the NVIDIA NIM Optimized Inference Microservices.\n\n<img src='/file=slides/Slide74.PNG'>: Slide titled \"NIM In Your Course\" with a subtitle \"Your Inference API Options\".\n\n<img src='/file=slides/Slide75.PNG'>: Conceptual diagram related to 'Endpoint Orchestration' and 'Endpoint Deployment' in the context of GenAI Servers.\n\n<img src='/file=slides/Slide76.PNG'>: Diagram illustrating a General Chat Application process flow.\n\n<img src='/file=slides/Slide77.PNG'>: Diagram illustrating the interaction between a user and an agent within a system that includes a software component and a language learning machine (LLM).\n\n<img src='/file=slides/Slide78.PNG'>: Slide titled 'Semantic Tools' focusing on 'Similarity Queries'.\n\n<img src='/file=slides/Slide79.PNG'>: Diagram illustrating the process of semantic tools in the context of LLM orchestration.\n\n<img src='/file=slides/Slide80.PNG'>: Flowchart related to Semantic Tools in Routing Systems.\n\n<img src='/file=slides/Slide81.PNG'>: Diagram illustrating a Hybrid Semantic/Structured Agentic System.\n\n<img src='/file=slides/Slide82.PNG'>: Diagram illustrating the process of Structured Retrieval in a system that interacts like a human.\n\n<img src='/file=slides/Slide83.PNG'>: Diagram illustrating the process of Structured Retrieval in the context of interacting like a human.\n\n<img src='/file=slides/Slide84.PNG'>: Slide presenting a technique for predicting valid schema using prompt engineering, LLM priming, tool training, and constrained decoding.\n\n<img src='/file=slides/Slide85.PNG'>: Flowchart related to the LLM Orchestration Technique, specifically focusing on the process of tool-calling.\n\n<img src='/file=slides/Slide86.PNG'>: Diagram illustrating the concept of a 'Tooled ReAct Loop' in the context of a 'Reason And Act Loop'.\n\n<img src='/file=slides/Slide87.PNG'>: Conceptual diagram of a Multi-Agent System with a generic definition.\n\n<img src='/file=slides/Slide88.PNG'>: Diagram illustrating a data pipeline for per-agent behavior with a focus on Dive Deep Learning.\n\n<img src='/file=slides/Slide89.PNG'>: Diagram illustrating the concept of Local State Accumulation in Per-Agent Behavior.\n\n<img src='/file=slides/Slide90.PNG'>: Diagram illustrating the concept of 'Communication Protocols' in the context of 'Agentic Behavior'.\n\n<img src='/file=slides/Slide91.PNG'>: Slide presenting the case for agents, focusing on compartmentalization, context bottlenecks, modularity, and intuitive design.\n\n<img src='/file=slides/Slide92.PNG'>: Slide focusing on LangGraph, emphasizing its flexibility in supporting various control flows and ensuring reliability through easy-to-add moderation and quality loops.",
    "summary": "00_jupyterlab.ipynb\n - JupyterLab Interface: Introduces the JupyterLab interface, a dashboard that provides access to interactive iPython notebooks, a folder structure of the environment, and a terminal window into the Ubuntu operating system.\n - Clearing GPU Memory: Explains three primary ways to clear the GPU memory, including soft reset, hard stop, and a code cell to reset the kernel.\n - Important Code: `IPython`, `app = IPython.Application.instance()`, `app.kernel.do_shutdown(True)`, `Shift+Enter` to execute code, `Run` button in the menu bar.\n - Main Ideas and Relevance To Course: JupyterLab interface, GPU memory management, kernel reset, iPython notebooks, Ubuntu terminal.\n - Connections to previous notebooks: None, this is the first notebook in the course.\n - Relevant Images: JupyterLab Interface screenshot (`<img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\">`), JupyterLab Launcher screenshot (`<img src=\"imgs/jl_launcher.png\">`).\n**01_llm_intro.ipynb**\n - **Recalling Deep Learning**: Reviews the basics of deep learning and how they extend to language modeling, including linear and logistic regression, stacking linear layers, convolution, and pre-trained components.\n - **Pulling In Our First LLM**: Introduces the HuggingFace library and its transformers package for loading and using large language models, including the BERT model and its architecture.\n - **Dissecting The Pipeline**: Examines the inner workings of the HuggingFace pipeline, including the tokenizer and model components, and how they support the pipeline's workflow.\n - **Your Course Environment**: Discusses the limitations of the course environment, including system memory and GPU resources, and provides a breakdown of the allocated resources.\n - Main Ideas and Relevance To Course: Deep learning, language modeling, HuggingFace, transformers, BERT, pipeline, tokenizer, model, GPU, system memory, compute environment.\n - Important Code: `transformers` package, `pipeline` function, `AutoTokenizer`, `BertTokenizer`, `BertModel`, `FillMaskPipeline`, `MyMlmPipeline` class.\n - Connections to previous notebooks: None.\n - Relevant Images: `machine-learning-process.jpg`, `CUDA-cores.png`\n**02_llm_intake.ipynb**\n - **Part 2.1: Getting The Model Inputs**: This notebook introduces the tokenizer and embedding process, explaining how input data is converted to a sequence of tokens and embedded into a vector space. It covers tokenization, embedding, and the role of attention masks.\n - **Part 2.2: Capturing Token Semantics**: This section delves into the embedding process, exploring how word embeddings, position embeddings, and token type embeddings are used to capture semantic information. It covers embedding layers, word embeddings, position embeddings, and token type embeddings.\n - **Part 2.3: From Token-Level to Sequence-Level Reasoning**: This section introduces the transformer architecture and self-attention mechanism, explaining how it enables sequence-level reasoning. It covers transformer attention, multi-headed attention, masked attention, and residual connections.\n - Main Ideas and Relevance To Course: Large Language Models, Tokenization, Embeddings, Attention Mechanisms, Transformer Architecture, Self-Attention, Multi-Headed Attention, Masked Attention, Residual Connections, Sequence-Level Reasoning.\n - Important Code: `transformers`, `pipeline`, `tokenizer`, `embeddings`, `attention_mask`, `self_attention`, `multi_headed_attention`, `masked_attention`, `residual_connections`.\n - Connections to previous notebooks: **Notebook 1: Getting Started With Large Language Models** (introduces HuggingFace library and transformers package), **Notebook 3: LLM Encoder Tasks** (pushes BERT encoder to other tasks and introduces task-specific modifications).\n - Relevant Images: `attention-logic.png`, `bert-construction.png`.\n**03_encoder_task.ipynb**\n - **Part 3.1: The Token Prediction Task Head**: This notebook explores the token-level prediction task, including the token classification and mask filling tasks, using the HuggingFace library and the BERT model. It delves into the architecture of the classification head and the postprocessing phase.\n - **Part 3.2: Token-Level Prediction For Range Outputs**: This section introduces the range prediction task, where the model predicts the start and end tokens of the input sequence to generate a subset as the model response. It uses the RoBERTa model and the question-answering pipeline.\n - **Part 3.3: The Sequence-Level Classification Head**: This part explores the sequence-level classification head, which takes a specific output entry from the base model and runs it through a series of dense layers to form the output shape of choice. It uses the RoBERTa model and the sentiment-analysis pipeline.\n - **Part 3.4: Zero Shot Classification**: This section introduces zero-shot classification, where the model predicts probabilities for classification, one class at a time, until it has a prediction for each of the desired classes. It uses the HuggingFace library and the BART model.\n - Main Ideas and Relevance To Course: This notebook introduces task-specific modifications to the BERT encoder, explores token-level, passage-level, and range-subsetting tasks, and delves into zero-shot classification. It uses the HuggingFace library, BERT, RoBERTa, and BART models.\n - Important Code: `FillMaskPipeline`, `AutoModelForMaskedLM`, `BertTokenizer`, `BertModel`, `RobertaClassificationHead`, `RobertaForQuestionAnswering`, `AutoModelForSequenceClassification`, `pipeline()`, `transformers`, `torch`.\n - Connections to previous notebooks: This notebook builds on the concepts introduced in **02_architecture_intuitions.ipynb**, where the mechanisms behind the HuggingFace pipelines were explored. It also sets the stage for **04_encoders_and_decoders.ipynb**, where encoder-decoder and decoder-only models are introduced.\n - Relevant Images: `task-token-classification.png`, `task-qa.png`, `task-text-classification.png`, `task-zero-shot.png`.\n**04_seq2seq.ipynb**\n - **Part 4.1: The Machine Translation Task**: Introduces the machine translation task, explaining its complexity and the need for encoder-decoder models to generate ordered sequences based on input contexts. Covers the basics of encoder-decoder models and their application to machine translation.\n - **Part 4.2: Pulling In A GPT-style model**: Explores decoder-only models like GPT-2, which excel at tasks that require generating novel sequences, such as open-ended text generation and dialogue systems. Introduces the concept of autoregressive text generation and its application to GPT-2.\n - **Part 4.3: Encoders, Decoders, and Encoder-Decoders**: Defines the terms encoder, decoder, and encoder-decoder, explaining their roles in machine learning formulations. Discusses the differences between encoder-only and decoder-only models, and the use of cross-attention in encoder-decoder models.\n - **Part 4.4: Machine Translation with T5-style Encoder-Decoders**: Introduces the T5 model, a type of encoder-decoder model that excels at machine translation and other tasks that require reasoning from two sequences with potentially-differing lengths. Covers the use of T5-style encoder-decoders in machine translation and their advantages over other models.\n - **Part 4.5: Creating More General-Purpose Models**: Discusses the use of encoder-decoders in more general-purpose models, such as the Flan-T5 model, which is designed to be more flexible and adaptable to different tasks. Introduces the concept of in-context learning and the use of few-shot prompting in encoder-decoders.\n\nMain Ideas and Relevance To Course: Encoder-decoder models, machine translation, decoder-only models, autoregressive text generation, cross-attention, T5-style encoder-decoders, in-context learning, few-shot prompting.\n\nImportant Code: `transformers`, `pipeline`, `model`, `tokenizer`, `encoder`, `decoder`, `cross-attention`, `T5-style encoder-decoders`.\n\nConnections to previous notebooks: This notebook builds on the concepts introduced in Notebook 3, where BERT was used for encoder-only tasks. It also introduces new concepts, such as decoder-only models and T5-style encoder-decoders, which are not covered in previous notebooks.\n\nRelevant Images: `bert-vs-gpt.png`, `t5-architecture.png`, `t5-pic.jpg`, `t5-flan2-spec.jpg`.\n**05_multimodal.ipynb**\n - **Part 5.1: Defining A Modality**: Introduces the concept of modality, discussing different types of data (text, images, audio, video, sensor data) and how they are processed by various architectures. Explores the idea of multimodal fusion and the challenges of handling different data types.\n - **Part 5.2: Encoding Different Modalities**: Demonstrates how transformers can be used to encode different modalities, including text, audio, and images. Introduces the concept of embedding and how it is used to convert data into a compact representation.\n - **Part 5.3: Joint Projections**: Discusses the limitations of independently optimized embeddings and introduces the concept of joint optimization using CLIP (Contrastive Language-Image Pre-training). Explores how CLIP aligns embeddings from different modalities into a shared space.\n - **Part 5.4: Combining Multimodal Encoders with Decoders**: Introduces the concept of bi-encoder and early fusion approaches, including cross-attention and cross-encoding. Discusses the use of decoders to generate complex outputs, such as text from images or audio.\n - **Part 5.5: Intro to Diffusion Decoders**: Introduces the concept of diffusion models and their use in generating complex outputs. Discusses the use of diffusion models in image generation and text-guided image generation.\n - **Part 5.6: Text-Guided Image Diffusion**: Demonstrates the use of diffusion models in text-guided image generation, using the Stable Diffusion XL model.\n\nMain Ideas and Relevance To Course: Multimodal fusion, transformer architectures, embedding, joint optimization, bi-encoder and early fusion approaches, cross-attention, cross-encoding, diffusion models, text-guided image generation.\n\nImportant Code: Transformers, PyTorch, HuggingFace library, CLIP, ViT, Wav2Vec2, BERT, GPT-2, Whisper, DiffusionPipeline.\n\nConnections to previous notebooks: Notebook 2 (LLM Architecture Intuitions) - discusses the mechanisms behind HuggingFace pipelines, which is relevant to the multimodal fusion approaches discussed in this notebook. Notebook 4 (Encoders and Decoders for Sequence Generation) - introduces encoder-decoder and decoder-only models, which is relevant to the use of decoders in this notebook.\n\nRelevant Images: <img src='/file=multimodal.png'>, <img src='/file=wav2vec2.png'>, <img src='/file=clip-arch.png'>, <img src='/file=whisper-arch.png'>, <img src='/file=diffusion_policy.png'>, <img src='/file=latent-diffusion.png'>.\n06_llm_server.ipynb\n - Part 6.1: Scaling Models To Real-World Use-Cases: Introduces the concept of LLM servers, discusses the limitations of basic generative models, and explores the advantages of using LLM servers for production environments.\n - Part 6.2: Accessing Your First LLM Server: Introduces the vLLM HuggingFace Model Server, demonstrates how to deploy and interact with it, and explores methods for efficient and concurrent inference with server deployments.\n - Part 6.3: Enabling Fast Concurrent Processes: Discusses the benefits of using LLM servers for concurrent processes, demonstrates how to enable fast concurrent processes with vLLM, and explores the use of LangChain for LLM orchestration.\n - Part 6.4: Diving Deeper into Text Generation: Introduces the concept of LLM orchestration using LangChain, demonstrates how to use LangChain to create a conversation loop, and explores the use of LLMs for text generation.\n - Main Ideas and Relevance To Course: LLM servers, vLLM HuggingFace Model Server, concurrent processes, LangChain, LLM orchestration, text generation, conversation loop.\n - Important Code: `ChatNVIDIA`, `ChatPromptTemplate`, `StrOutputParser`, `requests`, `langchain_nvidia_ai_endpoints`, `langchain_core.prompts`, `langchain_core.output_parsers`.\n - Connections to previous notebooks: Notebook 4 (Encoders and Decoders for Sequence Generation) and Notebook 5 (Multimodal Architectures and Fusion Techniques) lay the groundwork for the concepts introduced in this notebook, while Notebook 7 (Towards Orchestration and Agentics) builds upon the LLM orchestration concepts introduced in this notebook.\n - Relevant Images: <img src=\"/file=img/llm-router.png\">, <img src=\"/file=img/api-options.png\">, <img src=\"/file=img/basic-chat.png\">.\nHere is the summary of the currently-provided notebook, making sure to note all sections and points:\n\n07_intro_agentics.ipynb\n - **Part 7.1: Structuring LLM Workflows**: This notebook introduces the concept of prompt engineering, which is the art and science of crafting inputs that guide LLMs to produce desired outputs. It explains how LLMs are inherently stochastic parrots and are best at short-form generation and long-form context ingestion. The notebook also discusses the importance of iterative generation, parallel generation, and tooling in LLM workflows.\n - **Part 7.2: Intro to Tooling**: This section introduces the concept of tooling, which takes LLM techniques a step further by integrating external data sources, computational tools, and dynamic systems into the generation process. It explains how LLMs can interact with external environments through observing or interacting systems.\n - **Part 7.3: Multi-Tool Agentic Systems**: This section introduces the concept of agentic systems, which are systems that can observe, think about, react to, and act upon an environment with a personal directive. It explains how agentic systems can be implemented using a routing mechanism, prediction of arguments, buffer to accumulate memory, and an overarching or selectively-applied directive.\n - **Part 7.4: Enabling The Agentic Loop**: This section introduces the ReAct (Reason+Act) loop, which is a simple assumption that keeps calling for and observing the results of tool calls in an interleaving manner until a final answer is obtained. It provides a code example of how to implement the ReAct loop using a conversational tool-calling endpoint.\n\nMain Ideas and Relevance To Course: This notebook is a crucial part of the course, as it introduces the concept of tooling and agentic systems, which are essential for building sophisticated applications with LLMs. It also provides a deeper understanding of LLM workflows and how to integrate external data sources and computational tools into the generation process.\n\nImportant Code: The notebook uses the LangChain library, which provides a set of tools for building and deploying LLM-based applications. It also uses the ChatNVIDIA class, which is a wrapper around the NVIDIA LLM API.\n\nConnections to previous notebooks: This notebook builds on the concepts introduced in previous notebooks, particularly Notebook 6, which introduced the concept of LLM servers and deployment. It also builds on the concepts introduced in Notebook 5, which explored multimodal architectures and fusion techniques.\n\nRelevant Images: None.\n**08_assessment.ipynb**\n - **Part 8.1: Assessment**: This notebook is the course assessment, where you will implement a common feature that usually sits behind the API of an image-generating endpoint, synthetic prompts, and create an integrated system that straddles several domains, using LangChain and Diffusion models.\n - Main Ideas and Relevance To Course: Synthetic prompts, image generation, LangChain, Diffusion models, multimodal models, vision-language models, text-to-text interfaces, image retrieval, pipelining, and orchestration.\n - Important Code: `ChatNVIDIA`, `DiffusionPipeline`, `langchain_core.prompts`, `langchain_core.output_parsers`, `requests`, `base64`, `torch`.\n - Connections to previous notebooks: This notebook builds on concepts from **Notebook 6: Introduction To GenAI Servers** (LLM servers and deployments), **Notebook 5: Multimodal Architectures and Fusion Techniques** (multimodal models), and **Notebook 4: Encoders and Decoders for Sequence Generation** (encoder-decoder and decoder-only models).\n - Relevant Images: None.\n**Notebook 7.5: LangGraph.ipynb**\n - **Part 8.1: Agentic Notebook Retrieval**: Introduces a conversational agent that interacts with the user, accesses and retrieves information from Jupyter notebooks, and provides concise and helpful responses based on the retrieved information. The agent is built using the LangChain library and utilizes a state graph to manage the conversation flow.\n - **Part 7.5.1: Introducing LangGraph**: Introduces the LangGraph framework, a multi-agent orchestration framework that makes some useful design decisions and is a fantastic starting point for those looking to go further into this area. LangGraph enhances the workflow by providing state management, conditional transitions, and modularity.\n - **Part 7.5.2: Recreating Our ReAct Loop**: Recreates the ReAct loop using LangGraph, a state graph approach to modeling the agentic traversal process. This section provides a more modular and scalable approach to the ReAct loop.\n - **Part 7.5.3: Equipping Our Agent**: Equips the agent with a simple but powerful tool, `read_notebook`, which allows the agent to enrich its context with the full content of a notebook on command.\n - **Part 7.5.4: Continuing With LangGraph?**: Encourages the user to continue exploring LangGraph and its applications, providing resources and tutorials for further learning.\n\nMain Ideas and Relevance To Course: LangGraph, multi-agent orchestration, state graph, conversational agent, agentic notebook retrieval, ReAct loop, modularity, scalability, tooling, agentics.\n\nImportant Code: `langgraph`, `langchain`, `ChatNVIDIA`, `ChatOpenAI`, `ConversationalToolCaller`, `StateGraph`, `MemorySaver`, `RunnableConfig`, `ToolNode`, `ToolMessage`.\n\nConnections to previous notebooks: This notebook builds upon the concepts introduced in **Notebook 7: Towards Orchestration and Agentics**, which delves deeper into orchestrating LLMs to build sophisticated applications. The LangGraph framework is a key component of this notebook, and its introduction is a natural progression from the concepts explored in **Notebook 6: Introduction To GenAI Servers**, which introduced the concept of LLM servers and deployments.\n\nRelevant Images: None.\n**Notebook 98: Launching Your vLLM Servers**\n - **Section 1 Name (As Seen In Notebook):** Launching vLLM Servers\n   - This notebook introduces the process of launching a vLLM OpenAI-style server running a local Visual Language Model (in this case, Microsoft's Phi-3.5-vision-instruct model) using the `vllm serve` command.\n   - Important topics: vLLM, OpenAI-style server, Visual Language Model, Microsoft's Phi-3.5-vision-instruct model, `vllm serve` command.\n - **Main Ideas and Relevance To Course:** This notebook is a practical application of the concepts learned in the previous notebooks, specifically in Notebook 6: Introduction To GenAI Servers, where the concept of LLM servers was introduced. It also builds upon the knowledge of HuggingFace models and pipelines from Notebook 1: Getting Started With Large Language Models.\n   - Key features: vLLM, OpenAI-style server, Visual Language Model, `vllm serve` command, HuggingFace models and pipelines.\n - **Important Code:** `vllm serve` command, `--trust-remote-code`, `--max_model_len`, `--gpu-memory-utilization`, `--enforce-eager`, `--port 9000`.\n - **Connections to previous notebooks:** Notebook 6: Introduction To GenAI Servers (LLM servers), Notebook 1: Getting Started With Large Language Models (HuggingFace models and pipelines).\n - **Relevant Images:** None.\nTable_of_Contents.ipynb\n - **Welcome to the course!**: Introduction to the course, overview of the notebooks, and a brief description of the topics to be covered.\n - **Microservices**: Overview of the microservices used in the course, including chatbot, composer, docker-router, and llm_client.\n - **Caches**: Overview of the caches used in the course, including nim-cache, temp-dir, imgs, img-files, and audio-files.\n - Main Ideas and Relevance To Course: This notebook serves as an introduction to the course and its structure, providing an overview of the microservices and caches used throughout the course.\n - Important Code: Python code for accessing the chatbot and docker-router microservices, as well as HTML code for displaying links to the chatbot.\n - Connections to previous notebooks: This notebook does not build directly on previous notebooks, but rather serves as an introduction to the course and its structure.\n - Relevant Images: None.\n\nNote: This notebook is a table of contents and does not contain any code or images that are specific to this notebook. It is a general introduction to the course and its structure."
}