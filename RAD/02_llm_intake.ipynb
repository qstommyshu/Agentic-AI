{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2Dhs90vKb9d"
   },
   "source": [
    "# <font color=\"#76b900\">**Notebook 2:** LLM Architecture Intuitions</font>\n",
    "\n",
    "In the previous notebook, we explored the surface-level interface of HuggingFace &#x1F917; pipelines and peeked beneath the abstraction. You should now be familiar with the `preprocess -> forward -> postprocess` structure which simplifies the natural language interface for typical use.\n",
    "\n",
    "In this notebook, we’ll dig deeper into the mechanisms behind these abstractions, focusing on how the models handle data and reason about sequences at a fundamental level.\n",
    "\n",
    "#### **Learning Objectives:**\n",
    "\n",
    "- Understand tokenization and embedding: how input data is represented and what properties the network leverages.\n",
    "- Explore transformer encoder architectures for sequence-level reasoning in an $n$-sequence-in, $n$-sequence-out formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIAiFiYPAFyg"
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 2.1:** Getting The Model Inputs\n",
    "\n",
    "We previously discussed that the pipeline converts data to and from tensor representations via the `preprocess` and `postprocess` stages. In `preprocess`, the tokenizer converts the input string into a series of ***tokens*** — symbolic representations of words, letters, or subwords — that function as basic building blocks of a sentence. This tokenization process is fundamental to all large language models since it gives us something to latch onto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n",
    "## Testing out the tokenizer\n",
    "msg = \"Hello world!\"\n",
    "print(\"Tokenize:\", unmasker.tokenizer.tokenize(msg))      ## See token boundaries\n",
    "print(\"Encoding:\", x := unmasker.tokenizer.encode(msg))   ## See special tokens at end\n",
    "print(\"Decoding:\", x := unmasker.tokenizer.decode(x))     ## See decoding\n",
    "\n",
    "## Specifying multiple sentences\n",
    "unmasker.tokenizer(\"Hello world!\", \"Have a great day!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When given our string, the tokenizer responds with several components:**\n",
    "- `input_ids`: These are just the IDs of the tokens that make up our sentence. Said tokens can be words, punctuation, letters, whatever. Just individual entries out of a set vocabulary, exactly like classes.\n",
    "- `token_type_ids`: A flag indicating whether a token is part of the first or second sentence, which is useful for BERT’s training and select tasks but will not affect us.\n",
    "- `attention_mask`: Regulates which sequence entry each other sequence entry can attend to during processing. We’ll explore this later.\n",
    "\n",
    "For our purposes, `input_ids` are the most crucial. These token ID sequences allow LLMs to reason about natural language by treating samples as an ordered sequence of tokens. This should feel somewhat familiar if you’ve worked with classification tasks in deep learning, although reasoning over sequences introduces additional complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNGHSdojBUaa"
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 2.2:** Capturing Token Semantics\n",
    "\n",
    "Now that we understand natural language reasoning as working with an ordered sequence of tokens, how do we approach this? Let’s break it down using some familiar concepts from deep learning:\n",
    "\n",
    "- **On the output side**, we can predict a probability distribution over possible classes. For example, if we’re classifying among the tokens `cat`, `dog`, and `bird`, we output a vector like `<is_cat, is_dog, is_bird>`. The ground truth is represented as a one-hot vector, where the correct class is 1 and the rest are 0.\n",
    "\n",
    "- **On the input side**, while one-hot encoding could work, it’s more efficient to use an **Embedding Layer**. This is essentially a matrix where the class index points to a row of semantic information. Whether we use a one-hot vector or an embedding, the model maintains a record of class semantics (either in the weights of the first layer or in a lookup matrix).\n",
    "\n",
    "We can look over our unmasker's embedding strategy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1694317935957,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "iU9a00WcGRdm",
    "outputId": "ce49d963-cb97-41ec-996a-38edd5f3bb97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = unmasker.model\n",
    "# dir(model)\n",
    "# dir(model.bert)\n",
    "model.bert.embeddings\n",
    "# model.bert.embeddings.word_embeddings\n",
    "# model.bert.embeddings.position_embeddings\n",
    "# model.bert.embeddings.token_type_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgWJ8vhbGSJE"
   },
   "source": [
    "From this, we can identify the 3 components discussed in the presentation:\n",
    "- **Word Embeddings:** Semantic vectors representing the input tokens.\n",
    "- **Position Embeddings**: Semantic vectors representing the position of the words.\n",
    "- **Token Type Embedding**: Semantic vectors representing whether the token belongs to the first or second sentence.\n",
    "\n",
    "Notice how the `Embedding` component is constructed with the format:\n",
    "\n",
    "```\n",
    "Embedding(in_channel, out_channel)\n",
    "```\n",
    "\n",
    "We can see from this that BERT uses 768-dimensional embeddings, and can speculate on how they are obtained. The word embeddings seem to be coming from a 30,522-dimensional vector (the number of unique tokens in the vocabulary), the positional ones from 512, and the token types from just a few. Let's explore these a bit further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiB9VTXsH8WB"
   },
   "source": [
    "#### **Investigating the Word Embeddings**\n",
    "\n",
    "Let’s start by examining the word embeddings of a sample sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1694317935957,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "GA3gjraKH9Kk",
    "outputId": "55858ed8-d0cf-40b5-a36b-16300f2ff008",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = unmasker.tokenizer\n",
    "\n",
    "string = \"Hello World From Me, my cat and my dog!\"\n",
    "tokens = [tokenizer.convert_ids_to_tokens(x) for x in tokenizer.encode(string)]\n",
    "token_ids = torch.tensor(tokenizer(string)['input_ids'])\n",
    "embeddings = model.bert.embeddings.word_embeddings(token_ids)\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8LjH6fYIJzO"
   },
   "source": [
    "The output confirms that each token in our sentence is mapped to a 768-dimensional vector. These embeddings capture the meaning of each token. To explore this further, let’s define some helper functions to compute and visualize similarity between embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1694317936218,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "AIzRFOk_ILr0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(x1, x2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    x1_normalized = x1 / torch.norm(x1, dim=1, keepdim=True)\n",
    "    x2_normalized = x2 / torch.norm(x2, dim=1, keepdim=True)\n",
    "    return x1_normalized @ x2_normalized.T\n",
    "\n",
    "def scaled_dp_similarity(x1, x2):\n",
    "    \"\"\"Compute dot-product similarity between two vectors.\"\"\"\n",
    "    dot_product = x1 @ x2.T\n",
    "    d = torch.sqrt(torch.tensor(x1.shape[-1]))\n",
    "    return dot_product / d\n",
    "\n",
    "def softmax_similarity(x1, x2):\n",
    "    \"\"\"Compute softmaxed dp similarity between two vectors.\"\"\"\n",
    "    out = scaled_dp_similarity(x1, x2)\n",
    "    return torch.softmax(out, dim=1)\n",
    "\n",
    "def plot_mtx(matrix, name='', tokens=[]):\n",
    "    \"\"\"Compute similarity matrix for embeddings.\"\"\"\n",
    "    # Plot similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    label_dict = {} if tokens is None else {'xticklabels' : tokens, 'yticklabels': tokens}\n",
    "    sns.heatmap(\n",
    "        np.round(matrix.detach().numpy(), 3),\n",
    "        annot=True, cmap='coolwarm',\n",
    "        # vmin=-1, vmax=1,\n",
    "        **label_dict\n",
    "    )\n",
    "    plt.title(f\"Embedding {name} Matrix\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uARirRUVIN-o"
   },
   "source": [
    "With these visualization and metric functions defined, we can view the similarities of the embeddings in different measurement spaces:\n",
    "\n",
    "#### **Basic Cosine Similarity**\n",
    "\n",
    "The following will compute the cosine similarity:\n",
    "\n",
    "```python\n",
    "plot_mtx(cosine_similarity(embeddings, embeddings), 'Cosine Sim', tokens)\n",
    "```\n",
    "\n",
    "This produces a normalized similarity matrix that shows the cosine similarity between each pair of token embeddings. While this captures angular relationships, it discards magnitude information.\n",
    "\n",
    "#### **Softmax Attention**\n",
    "\n",
    "As we'll soon see this idea being incorporated into the architecture, it's worth investigating what happens when we decide to transition to softmax-based similarity:\n",
    "\n",
    "```python\n",
    "plot_mtx(softmax_similarity(embeddings, embeddings), 'Softmax(x1) Sim', tokens)\n",
    "```\n",
    "You'll see that the matrix is no longer symmetric since we're applying softmax on a per-row basis, but it does have a nice intuitive analog when you format it as a matrix product: **Relative to the others, how much does a token contribute to every other token?** This formulation will come up later as \"attention.\"\n",
    "\n",
    "You'll also notice that the magnitudes are pretty small, but we can increase the magnitude of the embeddings and observe a much more polarizing similarity matrix.\n",
    "\n",
    "```python\n",
    "plot_mtx(softmax_similarity(embeddings*10, embeddings*10), 'Softmax(x10) Sim', tokens)\n",
    "```\n",
    "  \n",
    "Because the metric now factors magnitude into the decision process but keeps the output bounded and under control, this is a great choice when you want to inject similarity into optimization (again, foreshadowing).\n",
    "\n",
    "Regardless, the takehome message for word embeddings is roughly **\"learned vector representation for each token based on its meaning and usage in sentences.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mtx(cosine_similarity(embeddings, embeddings), 'Cosine Sim', tokens)\n",
    "\n",
    "# plot_mtx(scaled_dp_similarity(embeddings, embeddings), 'Scaled DP Sim', tokens)\n",
    "# plot_mtx(scaled_dp_similarity(embeddings*10, embeddings), 'Scaled DP (x10, x1) Sim', tokens)\n",
    "# plot_mtx(scaled_dp_similarity(embeddings*10, embeddings*10), 'Scaled DP (x10, x10) Sim', tokens)\n",
    "\n",
    "# plot_mtx(softmax_similarity(embeddings, embeddings), 'Softmax(x1) Sim', tokens)\n",
    "# plot_mtx(softmax_similarity(embeddings*10, embeddings*10), 'Softmax(x10) Sim', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sReNjJpXIlSP"
   },
   "source": [
    "#### **Investigating the Positional Embeddings**\n",
    "\n",
    "Now that we've seen the word embeddings, we can take a look at the positional embeddings:\n",
    "\n",
    "```python\n",
    "model.bert.embeddings.position_embeddings ## -> Embedding(512, 768)\n",
    "```\n",
    "\n",
    "In contrast to the word embeddings, these are 512-dimensional and capture the semantics of the different positions in the input sequence. That way, our aggregate embedding can reason about both the words themselves and where they are in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ0n60xhIrro"
   },
   "source": [
    "You'll notice that the positional embedding has a more predictable and uniform cosine similarity plots compared to the word embeddings, which are all actually pretty consistent with a few key exceptions.\n",
    "\n",
    "**You're free to visualize a subset of the positional embeddings matrix below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1694317936219,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "aW1lxTkoIvVx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_options = torch.arange(len(tokens)) ## 0, 1, ..., n\n",
    "pos_embeddings = model.bert.embeddings.position_embeddings(pos_options)\n",
    "plot_mtx(cosine_similarity(pos_embeddings, pos_embeddings), 'Cosine Sim', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXwGMOOhI-WK"
   },
   "source": [
    "#### **Wrapping Up The Embeddings**\n",
    "\n",
    "To wrap up our embedding discussions, we do still have our **token_type_embedding** embeddings, but they follow roughly the same logic; take in some extra semantic information about the sentence structure, encode it to some compatable representation, and incorporate it into the input. The authors saw this extra information as useful for supervising the model's training routine, so the overall embedding definition for BERT is:\n",
    "\n",
    "`embed = WordEmbed[token] + PosEmbed[pos] + TypeEmbed[pos]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1694317936219,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "_R3BjEOkJBhs",
    "outputId": "7b89ff1a-81a6-41ab-81bb-e54bbb8fbcfe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.bert.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qKG1domJDdU"
   },
   "source": [
    "Then at the end, the LayerNorm section and Dropout section are also included, and these will permeate your architectures going forward. A light discussion is sufficient to motivate them:\n",
    "\n",
    "- The [**LayerNorm layer**](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) normalizes the data flowing through it so that each minibatch subscribes to a similar distribution. You've probably seen [**BatchNorm**](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) from computer vision; this has a similar logic, but now the normalization covers the layer outputs instead of the batch. For some more discussion, we found the [**PowerNorm paper**](https://arxiv.org/abs/2003.07845) helpful.\n",
    "- The [**Dropout layer**](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) just masks out some of the values during training. You've probably seen this before, and the logic is the same as usual; prevent over-reliance on a selection of features and distribute the logic throughout the network.\n",
    "\n",
    "HuggingFace, being open-source, provides insight into these components. You can find the BERT implementation in [`transformers/models/bert/modeling_bert.py`](https://github.com/huggingface/transformers/blob/0a365c3e6a0e174302debff4023182838607acf1/src/transformers/models/bert/modeling_bert.py#L180C11-L180C11), which may shed light on some lingering questions like \"is this addition or concatenation\" (it's addition) or \"do we need more considerations to make this scheme work in practice\" (yes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj7nG585JOnt"
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 2.3:** From Token-Level to Sequence-Level Reasoning\n",
    "\n",
    "**To summarize the key points of the LLM intake strategy:**\n",
    "\n",
    "- A passage is **tokenized** into an ordered sequence of tokens (unique class instances).\n",
    "- The tokens are **embedded** into a vector that captures useful semantic features (meaning, position, etc).\n",
    " \n",
    "\n",
    "**With this, we have some obvious options for how to reason about our data:**\n",
    "\n",
    "1. **Token-by-token reasoning**: Treat each token in isolation (like classification tasks).\n",
    "    - **Problem**: Tokens need to understand the context of other tokens in the sequence.\n",
    "2. **Dense layer approach**: Reason about the sequence as a whole by combining all tokens at once.\n",
    "    - **Problem**: Creates an intractable neural network.\n",
    "\n",
    "The LLM solution is to do something between those two options: \n",
    "> Allow reasoning to be done **on each token *(Token-Level Reasoning)***, but also allow for small opportunities in which the network can **consider the sequence as a whole *(Sequence-Level Reasoning)***!\n",
    "\n",
    "That's where the **transformer** components come in!\n",
    "\n",
    "#### **Transformer Attention**\n",
    "\n",
    "Transformers, introduced in the 2017 paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762), have become central to state-of-the-art language models. This architecture uses an ***attention mechanism*** to create an interface where the other entries of the sequence can communicate semantic information to other tokens in the series.\n",
    "\n",
    "The formulation goes as follows: If we have semantic and positional information present in our embeddings, we can train a mapping from our embeddings into three semantic spaces $K$, $Q$, and $V$:\n",
    "\n",
    "- **$K$ (Key)** and **$Q$ (Query)** are arguments to a similarity function (recall scaled softmax attention) to guage how much weight should be given between any pair of sequence entries in the input.\n",
    "\n",
    "- **$V$ (Value)** is the information that should pass through to the next component, and is weighted by `SoftmaxAttention(Key, Query)` to produce an output that is positionally and semantically motivated.\n",
    "\n",
    "\n",
    "**In other words:** Given a semantic/position-rich sequence of $d_k$-element embeddings ($S$) and three dense layers ($K$, $Q$, and $V$) that operate per-sequence-entry, we can train a neural network to make semantic/position-driven predictions via the forward pass equation:\n",
    "\n",
    "$$\\text{Self-Attention} = \\text{SoftmaxAttention}(K_s, Q_s) \\cdot V_s$$$$= \\frac{K_s Q_s ^T}{\\sqrt{d_k}}V_s$$\n",
    "\n",
    "**This effectively lets us produce:**\n",
    "- **$K(E),V(E),Q(E)$:** Three different embeddings derived from $E$.\n",
    "- **$\\sigma(A)$:** Gives intuition of how each embedding relates to all the other embeddings.\n",
    "- **$\\sigma(A)V$:** Embedding which considers both (pre-interface) $V$ and (post-interface) attention.\n",
    "\n",
    "<div><img src=\"imgs/attention-logic.png\" width=\"1000\"/></div>\n",
    "\n",
    "This mechanism is known as **self-attention** because the `Key`, `Query`, and `Value` vectors are derived from the same sequence. Other types of attention will be introduced later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruqnhclbNli5"
   },
   "source": [
    "#### **Seeing Attention in the BERT Encoder**\n",
    "\n",
    "Now that we understand self-attention, let's look at how BERT’s encoder handles our embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1694317936220,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "17XfGL09NqwD",
    "outputId": "163de708-172f-4ae6-a334-29ccc447ac90"
   },
   "outputs": [],
   "source": [
    "unmasker.model.bert.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1MBqwscNlCy"
   },
   "source": [
    "Those interested can dig much deeper into the implementation details, a high-level overview should be sufficient to see the big picture:\n",
    "- `BertSdpa(ScaledDotProductAttention)SelfAttention`: This component takes a sequence of vectors (let's call it `x`) as input and gets the `Q`, `K`, and `V` components via `query(x)`, `key(x)`, and `value(x)`, respectively. As these are all $768$-dimensional vectors - and are thereby multiplicatively compatible under transpose - the layer performs the attention computation with a few key modifications:\n",
    "    - **Multi-Headed Attention:** $K$, $Q$, and $V$ are all slices up along the embedding dimension such that we get 12 trios with dimension $768/12 = 64$. This will give us 12 different attention results, allowing the network to distribute attention in a variety of ways. After all 12 results are obtained, just concatenate embedding-wise and you'll be back up to 768-feature vectors.\n",
    "    - **Masked Attention:** This is less useful for BERT but explains what the `attention_mask` input is doing. Essentially, it's a true-or-false \"should-I-add-negative-infinity-to-the-attention\" mask to keep the model from attending to things it shouldn't. This will be more useful in later architectures.\n",
    "    - **Residual Connections:** To help the network keep the token-level information propagating through the network and improve the overall gradient flow, most architectures add residual connections around the transformer components.\n",
    "\n",
    "- `BertSelfOutput -> BertIntermediate -> BertOutput`: These are all just token-level dense layers with non-linear activations and some `LayerNorm`/`Dropout` layers mixed in for regularization. Each element in the sequence is thereby fed through an MLP *(multi-later perceptron, or multi-layer dense network)* with dimensions progressing through $768 \\to 768 \\to 3072 \\to 768$ to create a new data representation.\n",
    "\n",
    "BERT stacks 12 such layers in sequence. Each layer performs attention, followed by token-wise transformations, resulting in deep reasoning about the input tokens. Not too bad, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"imgs/bert-construction.png\" width=\"800\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00X5bjwcNueo"
   },
   "source": [
    "#### **Visualizing The Attention Mechanism In Action**\n",
    "\n",
    "We can request the realized attention values computed at each `SelfAttention` layer by specifying `output_attentions=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1694317936460,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "nSYojpqSN3Qk",
    "outputId": "fd57f19f-41d1-404c-9659-d750ed24c02b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "## Tokenization Efforts\n",
    "string = \"Hello Mr. Bert! How is it [MASK]?\"\n",
    "input_tensors = unmasker.preprocess(string)\n",
    "input_ids = unmasker.tokenizer.encode(string)\n",
    "input_tokens = unmasker.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "## Encoding Efforts\n",
    "embeddings = unmasker.model.bert.embeddings(input_tensors['input_ids'])\n",
    "x = unmasker.model.bert.encoder(embeddings, input_tensors['attention_mask'], output_attentions=True)\n",
    "\n",
    "print(\"From encoder.forward():\")\n",
    "for k,v in x.items():\n",
    "    v = v if hasattr(v, \"shape\") else torch.stack(v)\n",
    "    print(f\" > '{k}' : {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrQ5n7S5N0rt"
   },
   "source": [
    "To visualize this, we can use the [**`BertViz` package**](https://github.com/jessevig/bertviz) to display the attention associations from our last forward pass in an interactive grid! Please feel free to test this out with other input strings to see what changes.\n",
    "- See what happens to the dimensionality when you increase the number of tokens.\n",
    "- See what happens to the connections, and see if you see any patterns worth noting.\n",
    "- Why do you think the CLS and SEP tokens get so much attention in many of the attention heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1694317956922,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 300
    },
    "id": "z7FjdryUN6FY",
    "outputId": "8b479087-52a2-42f8-ced7-ee768fd18d1f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bertviz import model_view\n",
    "\n",
    "model_view(x['attentions'], input_tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qihft1FJO7a2"
   },
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Wrapping Up**</font>\n",
    "\n",
    "At this point, we've shown the core intuitions of how these models reason about text:\n",
    "\n",
    "- Embed the semantics and positions of the tokens into per-entry embeddings.\n",
    "- Reason about the entries, mostly in isolation and with small and tight interfaces to consider the other entries in the sequence.\n",
    "\n",
    "Though these processes may seem intuitive in hindsight, they are extremely effective and have formed the backbone of sequence-reasoning solutions for more than half a decade! You will continue to see their impacts throughout the rest of the course, so make sure to remember what they are and how they work.\n",
    "\n",
    "**In the next section, we'll find out more about how they are used for practical task-specific applications with the help of some additional lightweight components!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please Run When You're Done!\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNrHvUEGbmmFHp3hETBA4dk",
   "provenance": [
    {
     "file_id": "16SWgOvVC9HUbtGzlmYWABbO2WXQaWN_w",
     "timestamp": 1694313364012
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
