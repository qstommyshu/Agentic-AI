{
    "00_jupyterlab.ipynb": {
        "outline": "00_jupyterlab.ipynb\n - JupyterLab Interface: Introduction to JupyterLab, a dashboard for managing environment, interactive iPython notebooks, folder structure, and terminal window into Ubuntu operating system.\n - Clearing GPU Memory: Three methods to clear GPU memory: Soft Reset, Hard Stop, and Soft Reset via code cell, with code examples and descriptions of each method.\n - Important Code: `print()` function, `Shift+Enter` to execute code, `IPython` module, `app = IPython.Application.instance()`, `app.kernel.do_shutdown(True)`.\n - Connections to previous notebooks: None, this is the first notebook.\n - Relevant Images: JupyterLab Interface screenshot (`jl_launcher.png`).",
        "sections": [
            "**JupyterLab Interface**\n\nThe JupyterLab interface is a dashboard that provides a comprehensive environment for managing projects and interacting with code. It consists of a menu bar at the top, a file browser in the left sidebar, and a main work area that is initially open to the \"Launcher\" page. The file browser can be navigated like a typical file explorer, and double-clicking on an item opens a new tab with its content. The main work area displays tabbed views of open files, which can be closed, moved, and edited as needed.\n\nThe JupyterLab interface is the primary workspace for this notebook, and understanding its layout and functionality is essential for navigating and completing the exercises. The interface is designed to provide a seamless experience for working with code, data, and other project files. By familiarizing oneself with the JupyterLab interface, users can efficiently manage their projects and make the most of the notebook's features.",
            "**Clearing GPU Memory**\n\nAt times, it is necessary to clear the GPU memory to reset the GPU state or start fresh for a new set of exercises. There are three primary ways to clear the GPU memory:\n\n* **Soft Reset**: This can be done using the \u27f3 button on the menu above the notebook to reset the kernel of the current tab. This is a gentle reset that stops the kernel without terminating it.\n* **Hard Stop**: The square-in-circle icon below the file browser in the left-hand kernel menu houses the open kernels. Closing them will put a hard stop to their compute resources, effectively terminating the kernel.\n* **Soft Reset via Code Cell**: A code cell is provided that can be run to clear the GPU memory. The code uses the `IPython` module to shut down the kernel. The code is: `import IPython; app = IPython.Application.instance(); app.kernel.do_shutdown(True)`. This method is a soft reset that stops the kernel without terminating it.\n\nThese methods are useful for managing the GPU memory and ensuring a clean slate for new exercises.",
            "**Clearing GPU Memory**\n\nThere are three primary ways to clear the GPU memory. This is necessary to reset the GPU state when an experiment goes wrong or to start fresh for a new set of exercises. The methods are:\n\n* **Soft Reset**: This can be done using the \u27f3 button on the menu above the notebook to reset the kernel of the current tab. This is a gentle reset that stops the kernel without closing it.\n* **Hard Stop**: The square-in-circle icon below the file browser in the left-hand kernel menu houses the open kernels. Closing them will put a hard stop to their compute resources, effectively shutting them down.\n* **Soft Reset via Code Cell**: A code cell is provided that can be run to clear the GPU memory. The code uses the `IPython` module to shut down the kernel. The code is: `import IPython; app = IPython.Application.instance(); app.kernel.do_shutdown(True)`. This method is similar to the soft reset button but is done programmatically.",
            "**Important Code**\n\nThe notebook introduces two important code snippets related to executing and managing code in JupyterLab. The first example is a simple `print()` function, which is executed by highlighting the cell and pressing `Shift+Enter` or clicking the \"Run\" button in the menu bar. This is a basic way to execute code in a cell.\n\nThe second code snippet is a more advanced example that demonstrates how to clear the GPU memory using the `IPython` module. The code creates an instance of the `IPython.Application` class and calls the `do_shutdown(True)` method on the kernel. This is a way to reset the kernel and free up GPU resources. This code is provided as a habit-forming exercise to help users manage their environment effectively.",
            "**Important Code**\n\nThe notebook introduces a simple `print()` function in a code cell, which is executed by highlighting the cell and pressing `Shift+Enter` or clicking the \"Run\" button in the menu bar. This is a basic example of executing code in JupyterLab. Additionally, the notebook provides a code cell that demonstrates how to clear the GPU memory using the `IPython` module. The code snippet `app = IPython.Application.instance()` and `app.kernel.do_shutdown(True)` is used to shut down the kernel, which is a way to reset the GPU state. This code is intended to be run to get into the habit of clearing GPU memory, which is a crucial step in managing the environment.",
            "**Important Code**\n\nThe notebook introduces a simple `print()` function in a code cell, which can be executed by highlighting the cell and pressing `Shift+Enter` or clicking the \"Run\" button in the menu bar. This will output the string \"This is just a simple print statement\" to the console. The code cell is provided as an example to demonstrate how to execute code in JupyterLab.\n\nAdditionally, the notebook introduces the `IPython` module and provides an example code cell that uses it to clear the GPU memory. The code cell imports the `IPython` module and creates an instance of the `Application` class, which is then used to shut down the kernel with `app.kernel.do_shutdown(True)`. This code is intended to be run to clear the GPU memory, and is one of the three methods for doing so, along with Soft Reset and Hard Stop."
        ]
    },
    "01_llm_intro.ipynb": {
        "outline": "01_llm_intro.ipynb\n - **Part 1.1: Recalling Deep Learning**: Review of deep learning basics, including linear and logistic regression, stacking linear layers, convolution, and pre-trained components like VGG-16/ResNet, with a focus on language modeling as a complex and high-dimensional task.\n - **Part 1.2: Pulling In Our First LLM**: Introduction to HuggingFace, a community for accessing, developing, and hosting deep learning models, and the HuggingFace Models catalog, with a focus on large language models (LLMs) and the `bert-base-uncased` model.\n - **Part 1.3: Dissecting The Pipeline**: Examination of the inner workings of the pipeline, including the tokenizer and model components, and the flow of input to output.\n - **Part 1.4: Your Course Environment**: Overview of the course environment, including system memory, GPU, and CUDA cores, and a breakdown of the allocated resources.\n \nMain Ideas and Relevance To Course: Deep learning basics, language modeling, HuggingFace, LLMs, pipeline architecture, tokenizer, model, and course environment setup.\n\nImportant Code: `transformers` package, `pipeline` function, `AutoTokenizer`, `BertTokenizer`, `BertModel`, `FillMaskPipeline`, `AutoModelForMaskedLM`, `BertForMaskedLM`, `MyMlmPipeline` class, `nvidia-smi` and `free -h` commands.\n\nConnections to previous notebooks: None, this is the first notebook in the course.\n\nRelevant Images: None.",
        "sections": [
            "**Part 1.1: Recalling Deep Learning**\n\nThis section reviews the foundational concepts of deep learning, highlighting the progression from simple models to more complex ones. The author starts by describing the typical learning path, which involves:\n\n* Beginning with linear and logistic regression to model simple linear relationships between inputs and outputs\n* Moving on to stacking linear layers with non-linear activations to increase predictive power\n* Using informed, sparsely-connected techniques like convolution to add more control to the model's reasoning\n* Employing pre-trained components, such as VGG-16/ResNet, to leverage existing knowledge and reduce the need for extensive training data\n\nThe author emphasizes that this progression is not unique to vision tasks, but also applies to language modeling, which is the primary focus of the course. The section concludes by highlighting the complexity and high dimensionality of language, making it a challenging task that can be simplified using creative techniques and large pre-trained models. This sets the stage for the course, which aims to explore language modeling, the tools available, and the types of problems that can be solved using these models.",
            "**Part 1.2: Pulling In Our First LLM**\n\nThis section introduces the HuggingFace community, a platform for accessing, developing, and hosting deep learning models. The HuggingFace Models catalog is presented as a valuable resource for finding and utilizing large language models (LLMs). The `bert-base-uncased` model is highlighted as a representative example, showcasing its architecture and capabilities. The section emphasizes the importance of the `transformers` package, which is HuggingFace's primary library for language models, and the Pipeline API, which simplifies complex deep learning tasks.\n\nThe section demonstrates how to load the `bert-base-uncased` model using the `transformers` package and the `pipeline` function, and how to use it for mask-filling. The code snippet provided shows how to create a pipeline instance, specify the model and tokenizer, and use it to fill in a mask in a given sentence. This example serves as a starting point for the course, illustrating the ease of use and potential of LLMs.\n\nThe section also touches on the importance of understanding the inner workings of the pipeline, which will be explored in more detail later in the course. The `bert-base-uncased` model is presented as a powerful tool for language modeling, and the section sets the stage for further exploration of LLMs and their applications.",
            "**Part 1.3: Dissecting The Pipeline**\n\nThe notebook delves into the inner workings of the pipeline, a crucial component of the HuggingFace library. The pipeline is a high-level interface that simplifies the process of using a model for a specific task, such as masked language modeling. To gain a deeper understanding of the pipeline, the notebook creates a custom pipeline class, `MyMlmPipeline`, which inherits from the `FillMaskPipeline` class. This custom pipeline allows for manual inspection of the data flow through the pipeline.\n\nThe notebook then examines the two core components of the pipeline: the tokenizer and the model. The tokenizer is responsible for converting input strings into a format that the model can process, while the model itself performs the actual computation. The notebook demonstrates how to create a tokenizer and a model instance, and how to use them to perform masked language modeling. By creating a custom pipeline, the notebook provides a clear and concise example of how the pipeline works, making it easier to understand the underlying mechanics.\n\nThe custom pipeline class, `MyMlmPipeline`, is defined with three methods: `__init__`, `__call__`, `preprocess`, `forward`, and `postprocess`. The `__init__` method initializes the pipeline with a tokenizer and a model, while the `__call__` method is a placeholder for the actual pipeline logic. The `preprocess` method is responsible for converting the input string into a format that the model can process, the `forward` method performs the actual computation, and the `postprocess` method converts the output back into a human-readable format. By examining these methods, the notebook provides a clear understanding of the data flow through the pipeline.",
            "**Part 1.4: Your Course Environment**\n\nThe notebook discusses the course environment, highlighting the importance of being mindful of the limitations of the compute setup. The environment is equipped with a high-performance compute setup, which is sufficient for the exercises, but has limitations in terms of system memory and GPU resources. The notebook emphasizes the need to manage model sizes to avoid running out of GPU RAM, which can be a bottleneck for using accelerated LLMs.\n\nThe allocated resources are broken down, with a display of the GPU specification using the `nvidia-smi` command and the memory specification using the `free -h` command. This information is provided to help students understand the constraints of the environment and plan their model usage accordingly.\n\nFinally, the notebook includes a code cell that restarts the Jupyter kernel to clear memory and avoid issues with future notebooks. This is a practical step to ensure a clean slate for the next notebook.",
            "**Part 1.4: Your Course Environment**\n\nThe notebook discusses the course environment, highlighting the importance of being mindful of the limitations of the compute setup. The environment is equipped with a high-performance compute setup, sufficient for the exercises, with the following key resources:\n\n* **System Memory**: Large language models can require massive memory, with some needing tens or even hundreds of GB. The environment is equipped to handle significant workloads, but it's essential to keep model sizes reasonable unless otherwise instructed.\n* **GPU**: GPU acceleration is crucial for fast deep learning, especially given the parallel processing demands of deep models. The thousands of CUDA cores in modern NVIDIA GPUs significantly speed up training and inference by performing parallelizable tasks quickly.\n* **GPU RAM**: Large models need to reside in GPU memory for fast inference. Limited GPU RAM can be a bottleneck for using accelerated LLMs, so model sizes should be managed accordingly.\n\nThe notebook provides a breakdown of the allocated resources using the `nvidia-smi` and `free -h` commands. It's essential to restart the Jupyter kernel using the provided code to clear memory and avoid issues with future notebooks. This section serves as a reminder of the course environment's capabilities and limitations, setting the stage for the exploration of language models and their applications in the subsequent notebooks.",
            "**Part 1.4: Your Course Environment**\n\nThe notebook discusses the course environment, highlighting the importance of being mindful of the limitations of the compute setup. The environment is equipped with a high-performance compute setup, sufficient for the exercises, with the following key resources:\n\n* **System Memory**: Large language models can require massive memory, with some needing tens or even hundreds of GB. The environment is equipped to handle significant workloads, but it's essential to keep model sizes reasonable unless otherwise instructed.\n* **GPU**: GPU acceleration is crucial for fast deep learning, especially given the parallel processing demands of deep models. The thousands of CUDA cores in modern NVIDIA GPUs significantly speed up training and inference by performing parallelizable tasks quickly.\n* **GPU RAM**: Large models need to reside in GPU memory for fast inference. Limited GPU RAM can be a bottleneck for using accelerated LLMs, so model sizes should be managed accordingly.\n\nThe notebook provides a breakdown of the allocated resources using the `nvidia-smi` and `free -h` commands. It's essential to restart the Jupyter kernel using the provided code to clear memory and avoid issues with future notebooks. This is a crucial step to ensure a smooth learning experience throughout the course."
        ]
    },
    "02_llm_intake.ipynb": {
        "outline": "02_llm_intake.ipynb\n - **Part 2.1: Getting The Model Inputs**: Tokenization and embedding of input data, including the `preprocess` and `postprocess` stages, and the role of the tokenizer in converting input strings to token sequences.\n - **Part 2.2: Capturing Token Semantics**: Investigation of token embeddings, including word embeddings, position embeddings, and token type embeddings, and the use of embedding layers to capture semantic information.\n - **Part 2.3: From Token-Level to Sequence-Level Reasoning**: Introduction to transformer architectures and self-attention mechanisms, including multi-headed attention and residual connections, and the use of these mechanisms to reason about sequences.\n\nMain Ideas and Relevance To Course: Tokenization, embedding, transformer architectures, self-attention, multi-headed attention, residual connections, sequence-level reasoning, and the role of these concepts in large language models.\n\nImportant Code: `pipeline` function, `transformers` package, `BertTokenizer`, `BertModel`, `BertSdpa`, `ScaledDotProductAttention`, `SelfAttention`, `LayerNorm`, `Dropout`, `bertviz` package.\n\nConnections to previous notebooks: This notebook builds on the introduction to HuggingFace and large language models in 01_llm_intro.ipynb, and provides a deeper dive into the inner workings of the pipeline and the transformer architecture.\n\nRelevant Images: `attention-logic.png`, `bert-construction.png`.",
        "sections": [
            "**Part 2.1: Getting The Model Inputs**\n\nThe notebook begins by discussing the `preprocess` and `postprocess` stages of the HuggingFace pipeline, which convert input data to and from tensor representations. The `preprocess` stage is where the tokenizer comes into play, converting input strings into a series of tokens, which are symbolic representations of words, letters, or subwords. This tokenization process is fundamental to large language models, as it provides a way to represent input data in a format that can be processed by the model.\n\nThe tokenizer is used to tokenize a sample string, and the resulting token sequence is then used to demonstrate the output of the `preprocess` stage. The output includes `input_ids`, `token_type_ids`, and `attention_mask`, which are used to represent the input data in a format that can be processed by the model. The `input_ids` are the most crucial component, as they represent the token sequence as a series of IDs that can be used to reason about the input data.\n\nThe notebook then uses the `unmasker` pipeline to test the tokenizer, demonstrating how it can be used to tokenize a string and extract the `input_ids`, `token_type_ids`, and `attention_mask`. This provides a clear understanding of how the `preprocess` stage works and how it prepares the input data for processing by the model. This section lays the foundation for the rest of the notebook, which explores how the model processes the input data and reasons about the token sequence.",
            "**Capturing Token Semantics**\n\nThe notebook delves into the token embeddings, which are the core of the transformer architecture. Token embeddings are learned vector representations of each token in the input sequence, capturing its semantic meaning and context. The notebook explores three types of embeddings: word embeddings, position embeddings, and token type embeddings.\n\nWord embeddings are learned vectors that represent the meaning of each token, such as words, punctuation, or subwords. The notebook uses the BERT model's embedding layer to extract the word embeddings, which are 768-dimensional vectors. The embeddings are obtained by looking up the token IDs in a vocabulary matrix, which is a matrix where each row corresponds to a unique token and each column corresponds to a dimension in the embedding space.\n\nThe notebook also investigates the position embeddings, which capture the semantic information about the position of each token in the input sequence. These embeddings are 512-dimensional and are obtained by looking up the position IDs in a position matrix. The position embeddings are used to inform the model about the relative position of each token in the sequence.\n\nThe notebook also touches on token type embeddings, which capture the semantic information about the type of each token, such as whether it belongs to the first or second sentence. These embeddings are used to inform the model about the sentence structure and are obtained by looking up the token type IDs in a token type matrix.\n\nThe notebook uses the `cosine_similarity` function to compute the similarity between the word embeddings, and the `softmax_similarity` function to compute the similarity between the word embeddings with a softmax activation function. The notebook also uses the `scaled_dp_similarity` function to compute the dot product similarity between the word embeddings.\n\nThe notebook uses the `plot_mtx` function to visualize the similarity matrices, which are used to understand the relationships between the word embeddings. The notebook also uses the `bertviz` package to visualize the attention associations between the word embeddings.\n\nThe notebook concludes by highlighting the importance of token embeddings in capturing the semantic meaning and context of each token in the input sequence. The token embeddings are used as input to the transformer architecture, which uses self-attention mechanisms to reason about the input sequence.",
            "**From Token-Level to Sequence-Level Reasoning**\n\nThe notebook introduces the concept of sequence-level reasoning, which is a key aspect of large language models. The authors argue that token-by-token reasoning is insufficient, as it fails to capture the context of other tokens in the sequence. On the other hand, dense layer approaches are intractable due to the large number of tokens. To address this, the authors propose a middle ground: allowing reasoning to be done on each token (token-level reasoning) while also allowing for small opportunities to consider the sequence as a whole (sequence-level reasoning).\n\nThe transformer architecture is introduced as a solution to this problem. It uses an attention mechanism to create an interface where the other entries of the sequence can communicate semantic information to other tokens in the series. The attention mechanism is based on a similarity function that takes the query, key, and value vectors as input. The query and key vectors are used to compute the similarity between the tokens, while the value vector is used to compute the output. The output is then weighted by the similarity to produce an output that is positionally and semantically motivated.\n\nThe transformer architecture is composed of multiple self-attention layers, each of which takes the output of the previous layer as input. The self-attention mechanism is used to compute the attention weights, which are then used to compute the output of the layer. The output of each layer is then passed through a feed-forward network (FFN) to produce the final output. The FFN is composed of two linear layers with a ReLU activation function in between. The output of the FFN is then passed through a layer normalization layer to produce the final output of the layer.\n\nThe transformer architecture is stacked multiple times to produce the final output of the model. The output of each layer is passed through a residual connection to the input of the previous layer, which helps to preserve the information from the previous layer. The final output of the model is the output of the last layer. The transformer architecture is a key component of the BERT model, and is used to perform sequence-level reasoning.",
            "**From Token-Level to Sequence-Level Reasoning**\n\nThe notebook introduces the concept of sequence-level reasoning, which is a key aspect of large language models. The authors argue that token-by-token reasoning is insufficient, as it fails to capture the context of other tokens in the sequence. On the other hand, dense layer approaches are intractable due to the large number of tokens. To address this, the authors propose a middle ground: allowing reasoning to be done on each token (token-level reasoning) while also allowing for small opportunities to consider the sequence as a whole (sequence-level reasoning).\n\nThe transformer architecture is introduced as a solution to this problem. It uses an attention mechanism to create an interface where the other entries of the sequence can communicate semantic information to other tokens in the series. The attention mechanism is based on a similarity function that takes the query, key, and value vectors as input. The query and key vectors are used to compute the similarity between the tokens, and the value vector is used to compute the output. The output is then weighted by the similarity to produce an output that is positionally and semantically motivated.\n\nThe transformer architecture is composed of multiple self-attention layers, each of which takes the output of the previous layer as input. The self-attention mechanism is used to compute the attention weights, and the output is then passed through a feed-forward network (FFN) to produce the final output. The FFN is composed of two linear layers with a ReLU activation function in between. The output of the FFN is then passed through a layer normalization layer to produce the final output.\n\nThe transformer architecture is stacked 12 times in BERT, with each layer performing self-attention followed by an FFN. The output of each layer is also passed through a residual connection to the input of the previous layer. This allows the network to learn long-range dependencies in the input sequence. The attention weights are also visualized using the `bertviz` package, which shows the attention associations between the tokens in the input sequence.",
            "**Part 2.3: From Token-Level to Sequence-Level Reasoning**\n\nThis section introduces the transformer architecture and self-attention mechanisms, which enable large language models to reason about sequences. The transformer architecture is a key component of BERT, and it allows the model to attend to different parts of the input sequence and weigh their importance. The self-attention mechanism is a crucial part of this architecture, and it enables the model to consider the relationships between different tokens in the input sequence.\n\nThe self-attention mechanism works by first computing three sets of vectors: the key (K), query (Q), and value (V) vectors. These vectors are derived from the input embeddings, and they are used to compute the attention weights. The attention weights are then used to compute the output of the self-attention mechanism, which is a weighted sum of the value vectors. This output is then added to the input embeddings, and the resulting vector is passed through a series of dense layers to produce the final output.\n\nThe transformer architecture uses a multi-headed attention mechanism, which allows the model to attend to different parts of the input sequence in different ways. This is achieved by splitting the input embeddings into multiple attention heads, each of which computes the attention weights independently. The outputs of the different attention heads are then concatenated and passed through a final dense layer to produce the final output. The transformer architecture also uses residual connections to enable the model to learn long-range dependencies in the input sequence.\n\nThe self-attention mechanism and the transformer architecture are key components of large language models, and they enable the models to reason about sequences in a way that is not possible with traditional recurrent neural networks. The transformer architecture is particularly well-suited to natural language processing tasks, where the input sequence is often long and complex. The self-attention mechanism allows the model to focus on the most relevant parts of the input sequence, and to ignore irrelevant information. This makes the transformer architecture a powerful tool for a wide range of natural language processing tasks.",
            "**Part 2.3: From Token-Level to Sequence-Level Reasoning**\n\nThis section introduces the transformer architecture and self-attention mechanisms, which enable large language models to reason about sequences. The transformer architecture is a key component of BERT, and it allows the model to consider the relationships between different tokens in the input sequence. The self-attention mechanism is a crucial part of the transformer architecture, and it enables the model to weigh the importance of different tokens in the input sequence.\n\nThe self-attention mechanism works by computing a weighted sum of the input tokens, where the weights are computed based on the similarity between the input tokens. This is done by computing three vectors: the key (K), query (Q), and value (V) vectors. The key and query vectors are used to compute the similarity between the input tokens, and the value vector is used to compute the weighted sum of the input tokens. The self-attention mechanism is applied multiple times, with different weights and biases, to allow the model to consider different aspects of the input sequence.\n\nThe transformer architecture also includes multi-headed attention, which allows the model to consider multiple aspects of the input sequence simultaneously. This is done by applying the self-attention mechanism multiple times, with different weights and biases, and then concatenating the results. The transformer architecture also includes residual connections, which allow the model to learn long-range dependencies in the input sequence. The self-attention mechanism and multi-headed attention are key components of the transformer architecture, and they enable the model to reason about sequences in a way that is not possible with traditional recurrent neural networks.\n\nThe transformer architecture is a key component of BERT, and it allows the model to consider the relationships between different tokens in the input sequence. The self-attention mechanism and multi-headed attention enable the model to weigh the importance of different tokens in the input sequence, and to consider different aspects of the input sequence simultaneously. This allows the model to reason about sequences in a way that is not possible with traditional recurrent neural networks, and it is a key component of the success of BERT and other transformer-based models."
        ]
    },
    "03_encoder_task.ipynb": {
        "outline": "03_encoder_task.ipynb\n - **Part 3.1: The Token Prediction Task Head**: Investigation of the token-level prediction task, including the use of a classification head to predict tokens, and the role of the `FillMaskPipeline` in token-level prediction, with a focus on the `BertOnlyMLMHead` and its components, including `BertLMPredictionHead` and `GELUActivation`.\n - **Part 3.2: Token-Level Prediction For Range Outputs**: Exploration of the range prediction task, including the use of a classification head to predict start and end tokens, and the role of the `AutoModelForQuestionAnswering` and `AutoTokenizer` in range prediction, with a focus on the `RobertaClassificationHead` and its components.\n - **Part 3.3: The Sequence-Level Classification Head**: Investigation of the sequence-level classification task, including the use of a classification head to predict passage-level information, and the role of the `AutoModelForSequenceClassification` and `AutoTokenizer` in sequence-level classification, with a focus on the `RobertaClassificationHead` and its components.\n - **Part 3.4: Zero Shot Classification**: Introduction to zero-shot classification, including the use of a classification head to predict multiple classes, and the role of the `facebook/bart-large-mnli` model in zero-shot classification.\n\nMain Ideas and Relevance To Course: Token-level prediction, range prediction, sequence-level classification, zero-shot classification, classification heads, transformer architectures, self-attention, multi-headed attention, residual connections, and the role of these concepts in large language models.\n\nImportant Code: `FillMaskPipeline`, `BertOnlyMLMHead`, `RobertaClassificationHead`, `AutoModelForMaskedLM`, `AutoModelForQuestionAnswering`, `AutoModelForSequenceClassification`, `pipeline` function, `transformers` package.\n\nConnections to previous notebooks: This notebook builds on the introduction to HuggingFace and large language models in 01_llm_intro.ipynb, and provides a deeper dive into the inner workings of the pipeline and the transformer architecture, including tokenization, embedding, and sequence-level reasoning.\n\nRelevant Images: `task-token-classification.png`, `task-qa.png`, `task-zero-shot.png`, `task-text-classification.png`.",
        "sections": [
            "**Part 3.1: The Token Prediction Task Head**\n\nThe token prediction task head is a key component of the transformer architecture, responsible for predicting tokens in a sequence. This task is central to many BERT-like models, which are trained on the masked language modeling (MLM) objective. In this section, the notebook delves into the details of the token prediction task head, specifically the `BertOnlyMLMHead` component.\n\nThe `BertOnlyMLMHead` is a classification head that takes the output of the transformer encoder and predicts the probability distribution over the vocabulary. The head consists of a dense layer with GELU activation and layer normalization, followed by another dense layer that outputs the final probabilities. The notebook provides a detailed breakdown of the `BertOnlyMLMHead` architecture, including the use of GELU activation and layer normalization.\n\nThe token prediction task head is used in the `FillMaskPipeline`, which is a pre-trained pipeline that can be used for token-level prediction tasks. The pipeline takes a string input and returns a dictionary with the predicted token probabilities. The notebook provides an example of how to use the `FillMaskPipeline` to predict the most likely token to fill a masked position in a sentence. The `FillMaskPipeline` is a key component of the notebook, and its use is demonstrated throughout the section.\n\nThe token prediction task head is a fundamental component of the transformer architecture, and its use is critical for many NLP tasks. The notebook provides a detailed understanding of the `BertOnlyMLMHead` and its role in the `FillMaskPipeline`, which is essential for understanding the rest of the notebook. The section also provides a foundation for understanding the use of classification heads in transformer architectures, which is a key concept in the rest of the notebook.",
            "**Part 3.2: Token-Level Prediction For Range Outputs**\n\nIn this section, the notebook explores the range prediction task, which is a variation of the token-level prediction task. The main difference is that the classification head is trained to predict the start and end tokens of the model input, rather than individual tokens. This is achieved by using a classification head that outputs two logits per token, one for the start probability and one for the end probability. The post-processing step then selects the start and end tokens by maximizing the sum of the two predicted logits.\n\nThe notebook uses the `AutoModelForQuestionAnswering` and `AutoTokenizer` to demonstrate this task, specifically the `RobertaClassificationHead` component. This head is similar to the `BertOnlyMLMHead` used in the token-level prediction task, but with the addition of two output layers, one for the start probability and one for the end probability. The notebook also provides an example of how to use this model to predict the answer to a question, given a passage of text.\n\nThe range prediction task is an example of a token-level prediction task that is limited to predicting a subset of the input tokens. This is in contrast to the token-level prediction task, which predicts individual tokens. The notebook highlights the benefits of this approach, including the ability to limit the reasoning space of smaller models and the potential for more stable and risk-averse applications. However, it also notes that this approach can be viewed as a drawback, as it restricts the model's ability to generate conversational outputs.",
            "**Part 3.3: The Sequence-Level Classification Head**\n\nIn this section, the notebook explores the sequence-level classification task, where the model is trained to predict passage-level information. The `AutoModelForSequenceClassification` and `AutoTokenizer` are used to perform this task. The `RobertaClassificationHead` is a key component of this task, and it is used to take the output of the transformer backbone and produce a probability distribution over the possible classes. The head consists of a dense layer with a dropout layer and an output projection layer.\n\nThe notebook uses the `AutoModelForSequenceClassification` and `AutoTokenizer` to perform sentiment analysis on a given text. The `emo_model` is created using the `pipeline` function, and it is used to classify the sentiment of a given text as positive, negative, or neutral. The `emo_model` is trained on the `SamLowe/roberta-base-go_emotions` model, which is a pre-trained model that has been fine-tuned for sentiment analysis.\n\nThe `RobertaClassificationHead` is a key component of the `emo_model`, and it is used to take the output of the transformer backbone and produce a probability distribution over the possible classes. The head consists of a dense layer with a dropout layer and an output projection layer. The notebook shows how the `RobertaClassificationHead` is used to perform sentiment analysis, and how it can be used to classify text as positive, negative, or neutral. This section provides a clear understanding of how the sequence-level classification task is performed using the `AutoModelForSequenceClassification` and `AutoTokenizer`, and how the `RobertaClassificationHead` is used to produce a probability distribution over the possible classes.",
            "**Part 3.4: Zero Shot Classification**\n\nIn this section, the notebook introduces the concept of zero-shot classification, a type of inference where a model is asked to predict things it was not specifically trained to predict. This is in contrast to few-shot inference, where the model is asked to predict things it was trained on, but with limited data. The notebook explains that zero-shot classification is useful for generating an unbounded number of realizations, one generation at a time, and that this type of inference is common in the course.\n\nThe notebook then guides the user to test out a zero-shot pipeline using the `facebook/bart-large-mnli` model, which is trained on the MultiNLI dataset. The user is asked to skim through the task specification, import the model, and test it out with some examples. The notebook provides a hint to consider the task that the model is trained on and to check out the model card for more information.\n\nThe zero-shot classification task is a key concept in the notebook, as it allows the model to generate an unbounded number of realizations, one generation at a time. This is in contrast to the previous sections, which focused on token-level prediction, range prediction, and sequence-level classification, which are all specific types of inference that the model can perform. The zero-shot classification task is a more general type of inference that can be used to generate a wide range of outputs, making it a powerful tool for generating text and other types of data.",
            "**Part 3.4: Zero Shot Classification**\n\nIn this section, the notebook introduces the concept of zero-shot classification, a type of inference where a model is asked to predict things it was never specifically trained to predict. This is in contrast to few-shot inference, where a model is asked to predict things it was trained on, but with limited data. The notebook explains that zero-shot classification is useful for generating an unbounded number of realizations, one generation at a time, and that this type of inference is common in the course.\n\nThe notebook then guides the user to test out a zero-shot pipeline using the `facebook/bart-large-mnli` model, which is trained on the MultiNLI dataset. The user is asked to skim through the task specification, import the model, and test it out with some examples. The notebook provides a hint to consider the task that the model is trained on and to check out the model card for more information.\n\nThe zero-shot classification task is a key concept in the notebook, as it allows the model to generate an unbounded number of realizations, one generation at a time. This is in contrast to the previous sections, which focused on token-level prediction, range prediction, and sequence-level classification. The zero-shot classification task is a key application of the transformer architecture, and the notebook provides a clear and concise introduction to this concept.",
            "**Part 3.4: Zero Shot Classification**\n\nIn this section, the notebook introduces the concept of zero-shot classification, a type of inference where a model is asked to predict things it was never specifically trained to predict. This is in contrast to few-shot inference, where a model is asked to predict things it was trained on, but with limited data. The notebook explains that zero-shot classification is useful for generating an unbounded number of realizations, one generation at a time, and that this type of inference is common in the course.\n\nThe notebook then guides the user to test out a zero-shot pipeline using the `facebook/bart-large-mnli` model, which is trained on the MultiNLI dataset. The user is asked to skim through the task specification, import the model, and test it out with some examples. The notebook provides a hint to consider the task that the model is trained on and to check out the model card for more information.\n\nThe zero-shot classification task is a key concept in the notebook, as it allows the model to generate an unbounded number of realizations, one generation at a time. This is in contrast to the previous sections, which focused on token-level prediction, range prediction, and sequence-level classification. The zero-shot classification task is a key application of the transformer architecture, and the notebook provides a clear and concise introduction to this concept."
        ]
    },
    "04_seq2seq.ipynb": {
        "outline": "04_seq2seq.ipynb\n - **Part 4.1: The Machine Translation Task**: Exploration of the machine translation task, including the use of encoder-decoder models for sequence generation, with a focus on the Flan-T5 model and its components, including the encoder and decoder, and the role of the `transformers` package in sequence generation.\n - **Part 4.2: Pulling In A GPT-style model**: Introduction to decoder-only models, including the GPT-2 model and its components, and the role of autoregressive generation in sequence generation.\n - **Part 4.3: Encoders, Decoders, and Encoder-Decoders**: General discussion of encoders, decoders, and encoder-decoders, including the differences between them and their applications, with a focus on the `transformers` package and its components.\n - **Part 4.4: Machine Translation with T5-style Encoder-Decoders**: Exploration of the use of encoder-decoders for machine translation, including the T5 model and its components, and the role of cross-attention in sequence generation.\n - **Part 4.5: Creating More General-Purpose Models**: Discussion of creating more general-purpose models, including the use of encoder-decoders and decoder-only models, with a focus on the Flan-T5 model and its components.\n\nMain Ideas and Relevance To Course: Sequence generation, encoder-decoder models, decoder-only models, autoregressive generation, cross-attention, sequence-to-sequence tasks, machine translation, transformer architectures, self-attention, multi-headed attention, residual connections, and the role of these concepts in large language models.\n\nImportant Code: `pipeline` function, `transformers` package, `get_token_generator` function, `AutoModelForSeq2SeqLM`, `AutoTokenizer`, `FlanT5ForConditionalGeneration`.\n\nConnections to previous notebooks: This notebook builds on the introduction to HuggingFace and large language models in 01_llm_intro.ipynb, and provides a deeper dive into the inner workings of sequence generation and the transformer architecture, including tokenization, embedding, and sequence-level reasoning.\n\nRelevant Images: `bert-vs-gpt.png`, `t5-architecture.png`, `t5-pic.jpg`, `t5-flan2-spec.jpg`.\n\nThis notebook is a natural progression from the previous notebook, as it explores the use of sequence generation and encoder-decoder models for tasks such as machine translation. The notebook builds on the concepts of tokenization, embedding, and sequence-level reasoning introduced in the previous notebook, and provides a deeper dive into the inner workings of the transformer architecture. The notebook also introduces new concepts such as autoregressive generation, cross-attention, and sequence-to-sequence tasks, and provides a discussion of the differences between encoders, decoders, and encoder-decoders.",
        "sections": [
            "**Part 4.1: The Machine Translation Task**\n\nThe machine translation task is a complex problem that requires models to understand and generate languages with vastly different grammar, syntax, word order, and cultural nuances. The task is to translate text from one language to another, such as from Japanese to English. Encoder-decoder models, like Flan-T5, are well-suited for this task because they can first encode the source language sentence into a fixed representation and then decode it into a new, ordered sequence in the target language.\n\nThe encoder-decoder architecture is particularly effective for machine translation because it allows the model to understand the meaning of the source sentence before generating the translation. This is in contrast to encoder-only models, like BERT, which are better suited for tasks that don't require dynamic output generation. The decoder in the encoder-decoder model generates the translation one token at a time, using the encoded representation of the source sentence as context.\n\nThe notebook explores the machine translation task using the Flan-T5 model, which is a type of encoder-decoder model. The model is trained on a variety of tasks, including machine translation, and is able to generate translations by conditioning on the input sequence and generating one token at a time. The notebook also discusses the importance of the encoder-decoder architecture for machine translation and how it allows the model to generate high-quality translations.",
            "**Part 4.2: Pulling In A GPT-style model**\n\nIn this section, the notebook introduces decoder-only models, specifically the GPT-2 model, which is designed for autoregressive text generation. Unlike encoder-decoder models, decoder-only models predict tokens based on previous context, making them well-suited for tasks like open-ended text generation and dialogue systems. The GPT-2 model is trained to predict the next token in a sequence based on the previous tokens, using a process called autoregressive generation.\n\nThe notebook explains how the GPT-2 model works by generating text one token at a time, starting from a given prompt. The model uses a transformer architecture, which is similar to the encoder-decoder models discussed in the previous section, but with a key difference: the decoder-only model generates text without the need for an encoder. The notebook provides a code example that demonstrates how to use the GPT-2 model to generate text, and also shows how to investigate the forward pass of the model to understand how it generates text.\n\nThe GPT-2 model is an example of a decoder-only model that excels in generative tasks, and its architecture is well-suited for tasks where the input and output distributions blur together. The notebook highlights the key differences between encoder-decoder models and decoder-only models, and how they are used in different applications. This section provides a deeper understanding of the transformer architecture and its variants, and how they can be used for sequence generation tasks.",
            "**Part 4.3: Encoders, Decoders, and Encoder-Decoders**\n\nIn this section, the notebook delves into the general concepts of encoders, decoders, and encoder-decoders, providing a deeper understanding of the underlying architecture of sequence generation models. Encoders are models that transform input data into a fixed representation, while decoders transform input data directly into an explicit representation. Encoder-decoders, on the other hand, combine the strengths of both encoders and decoders to generate ordered sequences.\n\nThe notebook explains that BERT is considered an encoder architecture because it is sufficient for simple n-to-n or n-to-1 mappings, but falls short for generating new m-sequences progressively. In contrast, decoder-only models like GPT-2 are designed for autoregressive text generation, predicting tokens based on previous context. The notebook highlights the importance of understanding the differences between encoders, decoders, and encoder-decoders, and how they are applied in various tasks.\n\nThe section also discusses the concept of cross-attention, which allows for the use of an m-element sequence as context for an n-element sequence. This is achieved by using an attention interface to incorporate the m-element sequence into the n-element sequence, enabling the model to reason about both sequences simultaneously. The notebook provides a mathematical explanation of how cross-attention works, demonstrating that it can be used to incorporate an m-element sequence as context for an n-element sequence. This concept is crucial for understanding the inner workings of encoder-decoders and their applications in sequence generation tasks.",
            "**Part 4.4: Machine Translation with T5-style Encoder-Decoders**\n\nThis section explores the use of encoder-decoders for machine translation, specifically the T5 model and its components. The T5 model is a type of encoder-decoder model that uses a transformer architecture to generate text. The encoder takes in a source sentence and generates a fixed representation, which is then passed to the decoder to generate the translated sentence. The decoder uses cross-attention to attend to both the source sentence and the target sentence, allowing it to generate the translated sentence one token at a time.\n\nThe T5 model is trained on a large corpus of text data, including multiple tasks such as translation, summarization, and question-answering. This allows the model to learn a general-purpose language understanding and generation capability. The model is then fine-tuned on a specific translation task, such as translating from English to French. The fine-tuning process involves adjusting the model's weights to optimize its performance on the specific task.\n\nThe section also discusses the use of the `transformers` package to implement the T5 model and its components. The `transformers` package provides a simple and efficient way to implement transformer-based models, including the T5 model. The section also provides an example code snippet that demonstrates how to use the `transformers` package to implement the T5 model and perform machine translation.\n\nThe T5 model is a powerful tool for machine translation, and its use of cross-attention and encoder-decoder architecture allows it to generate high-quality translations. The model's ability to learn a general-purpose language understanding and generation capability also makes it a useful tool for other natural language processing tasks. The section provides a clear and concise overview of the T5 model and its components, and demonstrates how to use the `transformers` package to implement the model and perform machine translation.",
            "**Part 4.5: Creating More General-Purpose Models**\n\nThis section discusses the creation of more general-purpose models, specifically encoder-decoder models and decoder-only models. The focus is on the Flan-T5 model, a type of encoder-decoder model that has been pre-trained on a wide range of natural language tasks. The model is designed to be fine-tuned for specific tasks, and its encoder is pre-trained on a variety of natural language tasks, making it a good starting point for new tasks.\n\nThe section highlights the benefits of using encoder-decoder models like Flan-T5, including their ability to reason about new input formats with relatively few gradient updates. However, it also notes that these models can be cumbersome to use for few-shot prompting, as the encoder must handle both input-like and output-like data distributions. In contrast, decoder-only models like the Phi-1.5 model are more flexible and can facilitate new premises through a single input pathway, but may experience run-on generation.\n\nThe section concludes by noting that the Phi-1.5 model is an example of a Small Language Model (SLM), a class of smaller decoder-only models that are well-suited for fine-tuning. The use of SLMs and encoder-decoders like Flan-T5 is a key aspect of the prompt engineering paradigm, which is further explored in Notebooks 6 and 7.",
            "**Part 4.5: Creating More General-Purpose Models**\n\nThis section discusses the creation of more general-purpose models, specifically encoder-decoders and decoder-only models. The Flan-T5 model is used as an example of an encoder-decoder model, which is a type of model that uses a combination of an encoder and a decoder to generate text. The encoder is responsible for encoding the input text into a fixed representation, while the decoder generates the output text one token at a time.\n\nThe Flan-T5 model is a type of encoder-decoder model that is specifically designed for text-to-text tasks, such as machine translation and text summarization. It uses a combination of a transformer encoder and a transformer decoder to generate text. The encoder takes in the input text and generates a fixed representation, which is then passed to the decoder. The decoder generates the output text one token at a time, using the fixed representation generated by the encoder as context.\n\nThe section also discusses the use of decoder-only models, such as the Phi-1.5 model, which is a type of model that uses only a decoder to generate text. Decoder-only models are useful for tasks such as text generation and language modeling, where the input and output distributions are similar. The Phi-1.5 model is a small language model that is designed for fine-tuning and can be used for a variety of tasks, including text generation and language modeling.\n\nThe section also touches on the concept of in-context learning, which is the ability of a model to learn from a few examples and generalize to new tasks. The Flan-T5 model and the Phi-1.5 model are both examples of models that can be used for in-context learning. The section provides an example of how to use the Phi-1.5 model to generate text for a variety of tasks, including machine translation and text summarization."
        ]
    },
    "05_multimodal.ipynb": {
        "outline": "05_multimodal.ipynb\n - **Part 5.1: Defining A Modality**: Exploration of the concept of modality, including the definition of modality, common modalities (text, images, audio, video, sensor data), and the importance of understanding the structure and patterns of different data types.\n - **Part 5.2: Encoding Different Modalities**: Discussion of how transformers can be used to encode and process different modalities, including text, audio, and images, with a focus on the use of pre-trained models and the importance of understanding the structure and patterns of each modality.\n - **Part 5.3: Joint Projections**: Exploration of the concept of joint projections, including the use of CLIP (Contrastive Language-Image Pre-training) to project different modalities into a shared embedding space, and the importance of joint optimization for multimodal tasks.\n - **Part 5.4: Combining Multimodal Encoders with Decoders**: Discussion of how to combine multimodal encoders with decoders, including the use of cross-attention and the importance of using domain-specific decoders for generating complex outputs.\n - **Part 5.5: Intro to Diffusion Decoders**: Introduction to diffusion decoders, including the use of progressive denoising refinement and the importance of using diffusion models for generating high-complexity outputs.\n - **Part 5.6: Text-Guided Image Diffusion**: Exploration of text-guided image diffusion, including the use of diffusion models for generating images from text prompts and the importance of using attention mechanisms for conditioning the denoising process.\n\nMain Ideas and Relevance To Course: Multimodal architectures, multimodal fusion, joint projections, cross-attention, diffusion models, progressive denoising refinement, text-guided image generation, attention mechanisms, multimodal reasoning, and the importance of understanding the structure and patterns of different data types.\n\nImportant Code: `BertTokenizer`, `BertModel`, `Wav2Vec2Tokenizer`, `Wav2Vec2Model`, `ViTImageProcessor`, `ViTModel`, `CLIPProcessor`, `CLIPModel`, `DiffusionPipeline`, `DiffusionModel`.\n\nConnections to previous notebooks: This notebook builds on the introduction to large language models in 01_llm_intro.ipynb and provides a deeper dive into the inner workings of multimodal architectures and multimodal fusion. The notebook also introduces new concepts such as diffusion models and text-guided image generation, and provides a discussion of the importance of understanding the structure and patterns of different data types.\n\nRelevant Images: `multimodal.png`, `wav2vec2.png`, `vit-model.png`, `clip-arch.png`, `whisper-arch.png`, `diffusion_img.png`, `latent-diffusion.png`.",
        "sections": [
            "**Part 5.1: Defining A Modality**\n\nA modality is a particular form or type of data characterized by its structure and the way it conveys information. Common modalities include text, images, audio, video, and sensor data. Each modality has inherent relationships that govern how its pieces of information interact, and different architectures are needed to handle them effectively. The notebook emphasizes the importance of understanding the structure and patterns of different data types to effectively process and reason about them.\n\nThe notebook highlights the versatility of transformers, which can be applied to a wide range of modalities, including text, images, and audio. Transformers excel at capturing relationships between tokens in a sequence, making them highly effective for tasks such as text classification, generation, and translation. The notebook also discusses how transformers can be used to encode and process different modalities, including text, audio, and images, using pre-trained models.\n\nThe concept of modality is crucial in multimodal architectures, where different modalities need to be processed and fused together to achieve a common goal. Understanding the structure and patterns of different modalities is essential to design effective multimodal architectures that can reason about and generate complex outputs. The notebook sets the stage for the rest of the notebook, which explores how to encode and process different modalities, fuse them together, and generate complex outputs using multimodal architectures.",
            "**Part 5.2: Encoding Different Modalities**\n\nIn this section, the notebook explores how transformers can be used to encode and process different modalities, including text, audio, and images. The authors discuss how transformers excel at capturing relationships between tokens in a sequence, making them highly effective for tasks such as text classification, generation, and translation. They then demonstrate how transformers can be applied to other data types, such as audio and images, by using pre-trained models and understanding the structure and patterns of each modality.\n\nThe notebook provides code examples for encoding text, audio, and images using transformers. For text, it uses the BERT model and tokenizer to encode a list of text captions and dialogue. For audio, it uses the Wav2Vec2 model and tokenizer to encode an audio file. For images, it uses the Vision Transformer (ViT) model and processor to encode an image. The code snippets demonstrate how to preprocess the input data, load the pre-trained models, and extract the encoded representations.\n\nThe authors highlight the importance of understanding the structure and patterns of each modality when using transformers for encoding. For example, text is an ordered sequence of tokens, while audio is a time-series dataset with each point corresponding to a particular sensor reading or soundwave intensity. Images are 2D arrays of pixel values, which can be treated as a sequence of patches using the ViT formulation. By understanding these differences, the authors show how transformers can be adapted to handle various modalities and extract meaningful representations from each.",
            "**Joint Projections**\n\nJoint projections refer to the process of projecting different modalities into a shared embedding space, where they can be easily compared and contrasted. This is particularly useful for multimodal tasks, where we want to combine information from multiple sources to make predictions or generate outputs. One popular approach to joint projections is the use of CLIP (Contrastive Language-Image Pre-training), which trains a model to project images and text into a shared embedding space.\n\nCLIP uses a contrastive learning objective to align the embeddings of paired images and their captions, minimizing the distance between them in the shared space while maximizing the distance between unrelated pairs. This joint optimization process encourages the model to learn useful modular representations that can facilitate multimodal tasks like image-text retrieval. By projecting different modalities into a shared space, CLIP enables us to compare and contrast them in a more meaningful way, which is essential for tasks like image captioning, where we want to generate a descriptive sentence that corresponds to a given image.\n\nThe notebook demonstrates the power of joint optimization by exploring how CLIP embeddings perform on a task it's designed for - aligning images with their corresponding text descriptions. We see that the embeddings are aligned across modalities, and the similarity matrices show meaningful relationships for the advertised task (aligning images with their captions). This highlights the importance of joint optimization in multimodal learning and demonstrates how CLIP can be used to align different modalities in a shared space.",
            "**Combining Multimodal Encoders with Decoders**\n\nIn this section, the notebook explores how to combine multimodal encoders with decoders to generate complex outputs. The authors discuss the importance of using domain-specific decoders for tasks such as image captioning and speech recognition, where the output is a sequence of tokens that requires a specific structure. They introduce the concept of cross-attention, which allows decoders to dynamically reference encoded inputs, enabling them to generate outputs that are conditioned on the input modalities.\n\nThe notebook provides an example of using a ViT (Vision Transformer) encoder to extract features from an image, and a GPT-2 decoder to generate a text caption. The cross-attention mechanism is used to allow the decoder to attend to the image features while generating the caption. The authors also discuss the use of a pipeline-based approach, where the encoder and decoder are combined into a single model, and the output is generated through a series of transformations.\n\nThe section also touches on the use of early fusion, where the input modalities are combined early in the processing pipeline, and the output is generated through a single decoder. The authors note that this approach can be useful for tasks where the input modalities are highly correlated, and the output requires a high degree of multimodal reasoning. Overall, this section provides a detailed explanation of how to combine multimodal encoders with decoders to generate complex outputs, and how to use cross-attention and early fusion to enable multimodal reasoning.",
            "**Part 5.5: Intro to Diffusion Decoders**\n\nDiffusion decoders are a type of generative model that use a progressive denoising refinement approach to generate high-complexity outputs. Unlike autoregressive models, which generate one token at a time, diffusion models generate outputs by iteratively refining a noisy input through a series of denoising steps. This approach is particularly well-suited for tasks such as image generation, where the goal is to produce high-quality, high-resolution images.\n\nThe notebook introduces the concept of diffusion decoders and their application to image generation. It discusses the use of progressive denoising refinement, where the model iteratively refines a noisy input through a series of denoising steps, and the importance of using diffusion models for generating high-complexity outputs. The notebook also touches on the use of attention mechanisms for conditioning the denoising process, which allows the model to focus on specific regions of the image and generate more detailed and realistic outputs.\n\nThe notebook provides a code example of using a diffusion model to generate images from a text prompt, using the `DiffusionPipeline` class from the `diffusers` library. The code demonstrates how to use the pipeline to generate an image from a text prompt, and how to visualize the denoising process by saving the intermediate images at each step. The notebook also discusses the importance of using diffusion models for generating high-complexity outputs, and how they can be used in conjunction with other multimodal models to generate more realistic and detailed outputs.\n\nThe introduction of diffusion decoders in this section ties in with the rest of the notebook by providing a new approach to generating high-complexity outputs, which can be used in conjunction with the multimodal fusion techniques discussed earlier in the notebook. The use of diffusion models for image generation also highlights the importance of understanding the structure and patterns of different data types, and how to use this understanding to develop effective models for generating high-quality outputs.",
            "**Text-Guided Image Diffusion**\n\nText-guided image diffusion is a technique that leverages diffusion models to generate high-quality images from text prompts. This approach involves conditioning a denoising process on a text embedding, allowing the model to refine the image generation process based on the input text. The process begins with a text encoder that converts the input text into a semantically-dense representation, which is then used to condition the denoising process.\n\nThe denoising process itself is a key component of text-guided image diffusion. In this context, the denoising process involves progressively refining an initial noise signal to produce a high-quality image. This is achieved through a series of iterative transformations, where the model refines the noise signal at each step to produce a more coherent and detailed image. The text embedding is used to condition the denoising process, allowing the model to incorporate the input text into the image generation process.\n\nThe use of attention mechanisms is also crucial in text-guided image diffusion. By incorporating attention into the denoising process, the model can selectively focus on specific regions of the image and refine them based on the input text. This allows the model to generate high-quality images that are closely tied to the input text. The notebook demonstrates the use of text-guided image diffusion using the `diffusers` library and the `StabilityAI` model, showcasing the potential of this technique for generating high-quality images from text prompts."
        ]
    },
    "06_llm_server.ipynb": {
        "outline": "06_llm_server.ipynb\n - **Part 6.1: Scaling Models To Real-World Use-Cases**: Exploration of the limitations of basic generative models in production environments, including underpowered models, inefficient inferences, and single-user single-instance deployments, with a focus on the need for more powerful and scalable models.\n - **Part 6.2: Accessing Your First LLM Server**: Introduction to LLM servers, including the vLLM HuggingFace Model Server, which supports a variety of HuggingFace models, and the concept of inference servers for deploying and accessing more general decoder-style models.\n - **Part 6.3: Enabling Fast Concurrent Processes**: Discussion of the benefits of using inference servers, including concurrency and non-blocking connections, with a focus on the vLLM server's ability to handle multiple requests and users simultaneously.\n - **Part 6.4: Diving Deeper into Text Generation**: Exploration of the capabilities of LLM servers for text generation, including the use of LangChain for LLM orchestration and the creation of a conversation loop with a user interface.\n\nMain Ideas and Relevance To Course: LLM servers, inference servers, concurrency, non-blocking connections, text generation, LangChain, LLM orchestration, conversation loop, user interface.\n\nImportant Code: `requests`, `OpenAI`, `langchain_nvidia_ai_endpoints`, `ChatNVIDIA`, `StrOutputParser`, `ChatPromptTemplate`.\n\nConnections to previous notebooks: This notebook builds on the introduction to large language models in 01_llm_intro.ipynb and provides a deeper dive into the use of LLM servers for text generation and concurrency. The notebook also introduces new concepts such as LangChain and LLM orchestration, and provides a discussion of the importance of scalability and concurrency in production environments.\n\nRelevant Images: `llm-router.png`, `api-options.png`, `basic-chat.png`.",
        "sections": [
            "**Part 6.1: Scaling Models To Real-World Use-Cases**\n\nThe first part of the notebook highlights the limitations of basic generative models in production environments. These limitations include:\n\n* **Underpowered models**: The current models are not sufficient for non-trivial text generation and instruction following tasks.\n* **Inefficient inferences**: The models are not optimized for speed and efficiency, making them unsuitable for real-world applications.\n* **Single-user single-instance deployments**: The current deployments are limited to a single user and instance, making them unscaleable and impractical for large user bases.\n\nThese limitations are addressed by introducing the concept of **LLM servers**, which provide a more powerful and scalable solution for text generation and instruction following tasks. LLM servers can handle multiple requests and users simultaneously, making them more suitable for production environments. The notebook also introduces the **vLLM HuggingFace Model Server**, which supports a variety of HuggingFace models and provides a standardized API for interacting with the models. This sets the stage for the rest of the notebook, which explores the use of LLM servers for text generation and concurrency.",
            "**Part 6.2: Accessing Your First LLM Server**\n\nThis section introduces the concept of LLM servers, specifically the vLLM HuggingFace Model Server, which supports a variety of HuggingFace models. The vLLM server is a popular open-source LLM serving project that can download or cache-load a model and its configurations, convert the model cache into a more optimized form, load the model into a differentiable pipeline, and create and expose connection routes to access the model in a standard, independent, and scalable fashion.\n\nThe vLLM server is deployed to follow the OpenAI inference API schema, which is a standard in the ecosystem. This allows users to swap between different models and create connectors that can operate reliably. The server is also designed to be stateless, independent, and scalable, assuming multiple processes may be using its endpoints at a time. To get started with the vLLM server, the user is directed to check out the `98_VLM_Launch.ipynb` notebook and execute the kickstart cell, which loads a Visual Language Model (VLM) from HuggingFace and deploys it to a server.\n\nThe section also discusses the importance of using a separate notebook for the server, as managing multiple event loops inside a Jupyter notebook's Python kernel can be cumbersome. The user is then guided through the process of accessing the LLM server, including sending a GET request to the server to list the available models, and using the OpenAI client to call the LLM server for inference. The section concludes with a discussion of the vLLM server's ability to handle multiple requests and users simultaneously, making it a suitable choice for production environments.",
            "**Part 6.3: Enabling Fast Concurrent Processes**\n\nThis section of the notebook focuses on the benefits of using inference servers, specifically the vLLM server, to enable fast and concurrent processes. The author highlights that the vLLM server allows for multiple requests and users to query a particular model simultaneously, making it an ideal solution for real-world applications. The notebook demonstrates this capability by creating a simple stream that yields response chunks as soon as they are generated, and then shows how to merge multiple streams together to process tasks concurrently.\n\nThe author also discusses the concept of in-flight batching, which allows for pre-fill or autoregressive calls to distribute among a set of active threads with smoothed-out priority. This enables the vLLM server to handle a large number of concurrent requests without significant performance degradation. The notebook provides code examples to illustrate these concepts, including a simple stream and a more complex example that merges multiple streams together to process tasks concurrently.\n\nThe section also touches on the idea of optimizing deployments for speed, concurrency, and flexibility within a given budget. The author mentions that the vLLM server can be optimized for deployment on various platforms, and that the Llama-3.1-8B model is a decently-sized LLM model that is already optimized for deployment on the system. The notebook provides code examples to demonstrate the use of the Llama-3.1-8B model and its optimized deployment.",
            "**Part 6.4: Diving Deeper into Text Generation**\n\nThis section of the notebook delves into the capabilities of LLM servers for text generation, building on the foundation established in the previous sections. The author introduces the concept of LangChain, an LLM orchestration framework that streamlines the model usage and state management workflow. LangChain is used to create a pipeline that combines a `ChatPromptTemplate` to structure the input messages, a `ChatNVIDIA` connector to interact with the LLM, and a `StrOutputParser` to extract the output from the LLM's response.\n\nThe author then demonstrates how to use this pipeline to generate text based on a given context, which is loaded from a notebook file. The context is used to create a state that is passed to the pipeline, which generates a response. The author also provides an exercise to try different scenarios, such as short-input short-output, long-input short-output, short-input long-output, and long-input long-output, as well as code output.\n\nThe section also touches on the idea of creating a conversation loop, where the user can interact with the LLM in a multi-turn conversation. The author provides a code snippet to create a simple user interface that allows the user to input messages, which are then passed to the LLM pipeline, and the response is displayed to the user. The conversation loop is a key aspect of the notebook, as it demonstrates how the LLM can be used in a more interactive and dynamic way.",
            "**Part 6.4: Diving Deeper into Text Generation**\n\nThis section of the notebook delves into the capabilities of LLM servers for text generation, with a focus on using the LangChain framework for LLM orchestration. The author introduces the concept of a \"Runnable\" in LangChain, which is a lambda function that can chain together with others to create a pipeline for text generation. The author then defines a `ChatPromptTemplate` to structure messages in a way that guides the LLM towards understanding the context and generating responses accordingly.\n\nThe author creates a pipeline using the `ChatPromptTemplate`, the `ChatNVIDIA` connector, and the `StrOutputParser` component to automatically pull the content from the resulting `AIMessage`. This pipeline is then used to generate a summary of the notebook, engage in a dialogue about its contents, and even generate creative outputs that expand upon the themes and ideas explored.\n\nThe author then challenges the LLM to produce different types of outputs, including short-input short-output, long-input short-output, short-input long-output, and long-input long-output, as well as code output (Python, SQL, etc.). The author notes that the ramifications of these contexts will be discussed in more detail in the next notebook.\n\nFinally, the author provides an exercise to create a conversation loop using the pipeline, where the user can interact with the LLM in a multi-turn conversation. The author provides a basic implementation of the conversation loop, where the user can input messages and the LLM responds accordingly. The author notes that this is a starting point and can be expanded upon in future notebooks.",
            "**Part 6.4: Diving Deeper into Text Generation**\n\nThis section of the notebook delves into the capabilities of LLM servers for text generation, with a focus on using the LangChain framework for LLM orchestration. The author introduces the concept of a `ChatPromptTemplate` to structure messages in a way that guides the LLM towards understanding the context and generating responses accordingly. This template is combined with the `ChatNVIDIA` connector, which accepts a string or messages list and returns an `AIMessage`, and the `StrOutputParser` component, which automatically pulls the content from the resulting `AIMessage`.\n\nThe author then demonstrates how to create a conversation loop with a user interface, using the `ChatPromptTemplate` and `ChatNVIDIA` connector to stream the LLM's response directly to the output and accumulate it. The conversation loop is implemented using a while loop that continues to prompt the user for input and stream the LLM's response until the user interrupts the process.\n\nThe section also includes an exercise that encourages the reader to try forcing the LLM pipeline into different situations, such as short-input short-output, long-input short-output, short-input long-output, and long-input long-output, as well as code output (e.g. Python, SQL). This exercise is intended to help the reader understand the capabilities and limitations of the LLM pipeline and to explore the potential applications of LLMs in different contexts."
        ]
    },
    "07_intro_agentics.ipynb": {
        "outline": "Here is the summary of the currently-provided notebook, `07_intro_agentics.ipynb`:\n\n07_intro_agentics.ipynb\n - **Part 7.1: Structuring LLM Workflows**: Introduction to prompt engineering, the art and science of crafting inputs that guide LLMs to produce desired outputs, with a focus on the limitations of LLMs in generating long-form text and the importance of retrieval techniques to enhance context and improve LLM outputs.\n - **Part 7.2: Intro to Tooling**: Discussion of tooling, which takes LLM techniques a step further by integrating external data sources, computational tools, and dynamic systems into the generation process, with a focus on the importance of grammar enforcement and the use of schema inputs to specify a required format for the output.\n - **Part 7.3: Multi-Tool Agentic Systems**: Introduction to agentic systems, which can observe, think about, react to, and act upon an environment with a personal directive, with a focus on the importance of routing mechanisms, prediction of arguments, and buffer accumulation.\n - **Part 7.4: Enabling The Agentic Loop**: Implementation of the ReAct (Reason+Act) loop, which makes a simple assumption: keep calling for and observing the results of tool calls in an interleaving manner until you have a final answer.\n\nMain Ideas and Relevance To Course: LLM workflows, prompt engineering, tooling, grammar enforcement, schema inputs, agentic systems, routing mechanisms, prediction of arguments, buffer accumulation, ReAct loop, tool-calling, conversational tool-calling, LangGraph.\n\nImportant Code: `langchain_nvidia_ai_endpoints`, `ChatNVIDIA`, `RunnablePassthrough`, `RunnableLambda`, `ToolNode`, `ConversationalToolCaller`.\n\nConnections to previous notebooks: This notebook builds on the introduction to LLMs in 01_llm_intro.ipynb and provides a deeper dive into the use of LLMs for text generation and concurrency, as well as the introduction of new concepts such as tooling and agentic systems.\n\nRelevant Images: `data-pipelines.png`, `simple-agent.png`, `basic-react.png`.",
        "sections": [
            "**Part 7.1: Structuring LLM Workflows**\n\nPrompt engineering is the art and science of crafting inputs that guide LLMs to produce desired outputs. LLMs are inherently stochastic parrots, meaning they output probabilistic responses based on their input and training data. This makes certain tasks easier for even the most powerful models, such as summarization or synthesis-style tasks. However, long-form text generation is harder to keep on track, and LLMs are likely to derail due to accumulating error of autoregressive sampling.\n\nLLMs are perfect for tasks like summarization and long-form question-answering, which boil down to the problem of knowledge synthesis or distillation. This is because most models tend to be trained for short-form generation and long-form context ingestion. To overcome the limitations of LLMs, techniques like iterative generation, parallel generation, and retrieval can be used. Iterative generation involves generating a document one piece at a time, while parallel generation involves generating multiple pieces concurrently. Retrieval techniques can be used to enhance context and improve LLM outputs.\n\nThe notebook introduces the concept of prompt engineering and the limitations of LLMs in generating long-form text. It also discusses the importance of retrieval techniques and the use of iterative and parallel generation to overcome these limitations. The notebook provides code examples to demonstrate these concepts, including a pipeline that uses a chatbot to generate a summary of a notebook and a function that generates a structured summary of a course.",
            "**Tooling**\n\nTooling takes LLM techniques a step further by integrating external data sources, computational tools, and dynamic systems into the generation process. This is achieved through the use of **grammar enforcement**, which allows LLMs to adhere to a predefined output schema. Grammar enforcement is made possible by the use of **schema inputs**, which specify the required format for the output. This enables LLMs to interact with external systems and tools, such as databases, APIs, and other software components, in a more structured and controlled manner.\n\nThe notebook introduces the concept of **tooling** and demonstrates how it can be used to integrate external tools into the LLM workflow. The example code shows how to create a tool that can be used to perform arithmetic operations, such as addition and multiplication. The tool is then used in a conversational tool-calling endpoint, which allows the LLM to interact with the tool and generate output in a structured format.\n\nThe notebook also discusses the importance of **schema inputs** and how they can be used to specify the required format for the output. This is achieved through the use of **grammar enforcement**, which ensures that the LLM output conforms to the specified schema. The example code demonstrates how to use schema inputs to specify the format of the output and how to use grammar enforcement to ensure that the LLM output conforms to the specified schema.",
            "**Part 7.3: Multi-Tool Agentic Systems**\n\nIn this section, we explore the concept of agentic systems, which can observe, think about, react to, and act upon an environment with a personal directive. An agentic system is essentially a system that can make decisions based on its current state and take actions to achieve a goal. The key components of an agentic system include a routing mechanism to pick a pathway, a prediction of the arguments for the chosen pathway, a buffer to accumulate memory, and an overarching or selectively-applied directive.\n\nThe notebook introduces a simple agentic loop that minimally-satisfies as an agent, which uses a routing mechanism to decide whether to continue the conversation or not. However, this is just a starting point, and the notebook goes on to discuss more advanced agentic systems, including multi-tool agentic systems. A multi-tool agentic system is a system that can use multiple tools to achieve a goal, and the notebook provides an example of a calculator agent that uses multiple tools to perform mathematical operations.\n\nThe notebook also introduces the concept of conversational tool-calling, which allows the LLM to call tools and receive feedback in a conversational manner. This is achieved through the use of a `ConversationalToolCaller` component, which is a custom component that enables the LLM to call tools and receive feedback in a conversational manner. The notebook provides an example of how to use this component to create a conversational tool-calling endpoint.\n\nOverall, this section provides a foundation for understanding agentic systems and how they can be used to create more complex and controllable LLM systems. It also introduces the concept of conversational tool-calling, which is a key component of agentic systems.",
            "**Part 7.4: Enabling The Agentic Loop**\n\nIn this section, we implement the ReAct (Reason+Act) loop, a simple yet powerful agentic framework that makes a key assumption: keep calling for and observing the results of tool calls in an interleaving manner until you have a final answer. This approach is particularly useful when working with conversational tool-calling endpoints, which can handle both conversation and tool-calling within a single request.\n\nThe ReAct loop is implemented using a while loop that continues to call the `agent_chain` (a conversational tool-calling endpoint) and the `tool_caller` (a tool that strips tool calls from the message and pairs them with an appropriate executor) until a final answer is obtained. The loop accumulates the agent's responses and tool calls, and uses the `tool_caller` to execute the tool calls and obtain the final answer.\n\nThe ReAct loop is a key component of the notebook, as it enables the agentic system to reason and act iteratively, culminating in the ReAct loop. This approach is particularly useful for complex tasks that require multiple tool calls and conversations with the user. By implementing the ReAct loop, we can create a more sophisticated and flexible agentic system that can handle a wide range of tasks and scenarios.",
            "**Part 7.4: Enabling The Agentic Loop**\n\nIn this section, the notebook introduces the ReAct (Reason+Act) loop, a simple yet powerful agentic framework that enables the system to call multiple tools before getting back to the user with a final answer. The ReAct loop assumes that the system should keep calling for and observing the results of tool calls in an interleaving manner until it has a final answer. This is achieved by implementing a multi-step agentic loop that accumulates a chat history and tool calls, and uses a conversational tool-calling endpoint to execute the tool calls and update the chat history.\n\nThe notebook provides a code example that demonstrates how to implement the ReAct loop using the `ConversationalToolCaller` component, which is a custom tool-calling formulation that allows the system to call tools within a single request. The code uses a `while` loop to repeatedly call the `ConversationalToolCaller` component, accumulating the chat history and tool calls until a final answer is obtained. The notebook also provides a challenge question that asks the user to compute the Fibonacci sequence up to 25, use tools to compute the digits 26-30, and then compare the size of the 30th number to the 25th number.\n\nThe ReAct loop is a key component of the notebook, as it enables the system to perform complex tasks by breaking them down into smaller, manageable steps and executing them in an interleaving manner. This approach allows the system to reason and act iteratively, making it a powerful tool for building sophisticated applications.",
            "**Part 7.4: Enabling The Agentic Loop**\n\nIn this section, the notebook introduces the ReAct (Reason+Act) loop, a simple yet powerful agentic framework that enables the system to call multiple tools before getting back to the user with a final answer. The ReAct loop assumes that the system should keep calling for and observing the results of tool calls in an interleaving manner until it has a final answer. This is achieved by implementing a multi-step agentic loop that accumulates a chat history and tool calls, and uses a conversational tool-calling endpoint to execute the tool calls and update the chat history.\n\nThe notebook provides a code example that demonstrates how to implement the ReAct loop using the `ConversationalToolCaller` component, which is a custom tool-calling formulation that allows the system to call tools within a single request. The example code shows how to use the `ConversationalToolCaller` to execute a series of tool calls and accumulate the results, and how to use the `ToolNode` component to strip the message of tool calls and pair each of them with an appropriate executor to produce the right output.\n\nThe ReAct loop is a key concept in the notebook, as it enables the system to reason and act iteratively, culminating in the agentic loop. This is a crucial step in building sophisticated applications that can interact with users and perform complex tasks. The ReAct loop is also a key component of the LangGraph framework, which is a more advanced agentic framework that will be introduced in the next notebook."
        ]
    },
    "08_assessment.ipynb": {
        "outline": "08_assessment.ipynb\n - **Part 8.1: Assessment**: Introduction to the course assessment, which involves implementing a feature that usually sits behind the API of an image-generating endpoint, specifically synthetic prompts, and combining them with vision to create images inspired by another image.\n - **Part 8.2: Running The Assessment**: Instructions on how to run the assessment, including saving results and querying the assessment runner.\n - **Part 8.3: Wrapping Up**: Conclusion of the course, including downloading course material and checking the \"Next Steps\" and \"Feedback\" sections.\n\nMain Ideas and Relevance To Course: Synthetic prompts, image generation, vision-language models, diffusion models, text-to-text interfaces, image retrieval, image generation pipelines, LangChain, ChatNVIDIA, DiffusionPipeline.\n\nImportant Code: `ChatNVIDIA`, `DiffusionPipeline`, `langchain_core.prompts`, `langchain_core.output_parsers`, `requests`, `base64`.\n\nConnections to previous notebooks: This notebook builds on the introduction to vision-language models in 06_vlm_spinup.ipynb and the use of LLMs for text generation in 07_intro_agentics.ipynb, and provides a deeper dive into the use of LLMs for image generation and agentic systems.\n\nRelevant Images: `rad-assessment.png`.",
        "sections": [
            "**Part 8.1: Assessment**\n\nThe first part of the notebook introduces the course assessment, which focuses on implementing a feature that typically sits behind the API of an image-generating endpoint: synthetic prompts. Synthetic prompts are used to create text-conditioned diffusion models that learn strong customization priors. The goal is to generate high-quality images that perfectly fit any natural-language prompt, as long as the prompt is expressive enough.\n\nIn practice, image generators are often prompted with loose directives, leading to \"image retrieval,\" where the model produces images with minor modifications to the training data. To address this, many providers use text-to-text interfaces that map from \"regular human prompt\" to \"diffusion input prompt\" space. The assessment aims to implement a potential schema that combines synthetic prompts with vision to \"create images inspired by another image.\"\n\nThe assessment is divided into four tasks: Image Ingestion, Image Creation, Prompt Synthesis, and Pipelining and Iterating. The first three tasks are building blocks for the final solution in Task 4, which will be graded. The tasks involve implementing methods to reason about an image, generate an image based on a description, and synthesize prompts to map from the VLM output domain to the Diffusion input domain.",
            "**Part 8.2: Running The Assessment**\n\nTo complete the assessment, the user must run the following cells to save their results and then query the assessment runner. The `save_images_and_metadata` function is defined to collect all image paths and metadata, and save them in a JSON file. This function takes in a list of results, which is expected to be a list of tuples containing the generated images, prompts, and original description. The function iterates over the results, saves each image, and stores its path, as well as the original description and prompts. The metadata is then saved in a JSON file.\n\nThe `save_images_and_metadata` function is then called with the `results` variable, which is a list of results from the `generate_images_from_image` function. The `results` variable is a list of tuples containing the generated images, prompts, and original description for each image. The function is called with the `save_dir` parameter set to `\"generated_images\"`, which is the directory where the images and metadata will be saved.\n\nAfter saving the results, the user must send the submission to the assessment runner by making a POST request to the `http://docker_router:8070/run_assessment` endpoint with the submission as JSON data. The response from the assessment runner is then printed to the console, including any messages or exceptions that may have occurred. If the submission is successful, the response will contain a result, which is printed to the console.",
            "**Part 8.3: Wrapping Up**\n\nThis section serves as a conclusion to the course, providing a final set of instructions and recommendations for the learner. The main objective is to ensure that the learner has completed the course and is prepared to move forward. The section is divided into three main points:\n\n* **Downloading Course Material**: The learner is encouraged to download the course material for later reference, which is likely to be useful for future projects or as a resource for reviewing the concepts covered in the course.\n* **Next Steps**: The learner is advised to check the \"Next Steps\" section of the course, which is likely to provide information on what to do next, such as taking additional courses or exploring related topics.\n* **Feedback**: The learner is invited to provide feedback on the course, which is an essential step in improving the course content and ensuring that it meets the needs of its users.\n\nOverall, this section serves as a final checkpoint to ensure that the learner has completed the course and is prepared to move forward. It also provides an opportunity for the learner to provide feedback and suggestions for improvement.",
            "**Part 8.3: Wrapping Up**\n\nThis section serves as a conclusion to the course, providing a final set of instructions and recommendations for the learner. The main points of this section are:\n\n* The learner is encouraged to download the course material for later reference, as it will be useful for future studies and applications.\n* The learner is advised to check the \"Next Steps\" and \"Feedback\" sections of the course, which provide additional information and guidance on how to proceed.\n* The course concludes with a message of appreciation for the learner's time and effort, and a statement of enthusiasm for the learner's continued learning and growth.\n\nIn terms of its connection to the rest of the notebook, this section serves as a final wrap-up of the course material and provides a sense of closure for the learner. It is a natural conclusion to the assessment and the various tasks and exercises that were presented throughout the notebook.",
            "**Part 8.2: Running The Assessment**\n\nThis section provides instructions on how to run the assessment, which involves generating a submission based on the results of the previous tasks. The submission is expected to be a collection of images and metadata, which will be sent to the assessment runner for evaluation.\n\nThe submission is generated by calling the `save_images_and_metadata` function, which takes the results of the previous tasks as input and saves the images and metadata to a directory. The function uses the `os` and `json` modules to create a directory for the images and save the metadata in a JSON file. The metadata includes the original description of the image, the prompts used to generate the images, and the paths to the generated images.\n\nOnce the submission is generated, it is sent to the assessment runner using a POST request to the `http://docker_router:8070/run_assessment` endpoint. The response from the assessment runner is then printed to the console, including any messages or exceptions that may have occurred during the assessment. If the assessment is successful, the response will include a result, which is also printed to the console.",
            "**Part 8.2: Running The Assessment**\n\nThis section provides instructions on how to run the assessment, which involves generating a submission based on the results of the previous tasks. The submission is expected to be a collection of images and metadata, which will be sent to the assessment runner for evaluation.\n\nThe submission is generated by calling the `save_images_and_metadata` function, which takes the results of the previous tasks as input and saves the images and metadata to a directory. The function uses the `os` and `json` modules to create a directory for the images and save the metadata in a JSON file. The metadata includes the original description of the image, the prompts used to generate the images, and the paths to the generated images.\n\nOnce the submission is generated, it is sent to the assessment runner using a POST request to the `http://docker_router:8070/run_assessment` endpoint. The response from the assessment runner is then printed to the console, including any messages or exceptions that may have occurred during the assessment. If the assessment is successful, the response will include a result, which is also printed to the console."
        ]
    },
    "75_langgraph.ipynb": {
        "outline": "75_langgraph.ipynb\n - **Part 7.5.1: Introducing LangGraph**: Introduction to LangGraph, a multi-agent orchestration framework that makes design decisions and is a starting point for further exploration in this area, including state management, conditional transitions, and modularity.\n - **Part 7.5.2: Recreating Our ReAct Loop**: Recreating the ReAct loop using LangGraph, including a while-loop approach and a more modular agent + tools option, with a focus on flexibility and scalability.\n - **Part 7.5.3: Equipping Our Agent**: Equipping the agent with a simple but powerful tool, `read_notebook`, which allows the agent to enrich its context with the full content of a notebook on command, and creating a tooled LLM agent.\n - **Part 7.5.4: Continuing With LangGraph?**: Encouragement to continue exploring LangGraph and its applications, including tutorials and agentic paradigms.\n\nMain Ideas and Relevance To Course: LangGraph, multi-agent orchestration, state management, conditional transitions, modularity, ReAct loop, agentics, LLMs, tooling, notebook retrieval, conversational AI.\n\nImportant Code: `langgraph`, `StateGraph`, `MemorySaver`, `ConversationalToolCaller`, `ChatPromptTemplate`, `RunnableConfig`, `ToolNode`, `ToolMessage`.\n\nConnections to previous notebooks: This notebook builds on the introduction to agentics in 07_intro_agentics.ipynb and provides a deeper dive into the use of LLMs for agentic systems and tooling, and is a stepping stone for further exploration of LangGraph and its applications.\n\nRelevant Images: None.",
        "sections": [
            "**Part 7.5.1: Introducing LangGraph**\n\nLangGraph is a multi-agent orchestration framework that streamlines the design and development of complex conversational systems. It provides a structured approach to managing conversation flow, allowing developers to define states, transitions, and actions in a clear and maintainable way. This framework is particularly well-suited for multi-agent systems and intricate workflows, where traditional event loops can become cumbersome.\n\nLangGraph enhances the conversation flow by introducing state management, conditional transitions, and modularity. State management enables clear delineation of different states within the conversation, making it easier to track and manage the agent's progress and decisions. Conditional transitions allow for the definition of edges that dictate how the conversation flows based on specific triggers or conditions. Modularity promotes flexibility by allowing different nodes (functions) to handle specific tasks, facilitating easier updates and expansions.\n\nLangGraph is designed to be more scalable and maintainable than custom implementations, which can become overly complex and difficult to manage. It also provides a range of integrations, including LangServe, LangSmith, and LangGraph-Studio, which can simplify the development and deployment process. However, LangGraph may be overkill for simpler applications, and its learning curve can be steep due to its strong assumptions and complex features. Nevertheless, it is a powerful tool for building complex conversational systems, and its use is encouraged for those looking to scale their applications.",
            "**Recreating Our ReAct Loop**\n\nThe notebook introduces the concept of recreating the ReAct loop using LangGraph, a multi-agent orchestration framework. The ReAct loop is a basic agentic workflow that involves an agent interacting with a user, accessing and retrieving information from notebooks, and providing responses based on the retrieved information. The notebook presents two approaches to recreating the ReAct loop: a while-loop approach and a more modular agent + tools option.\n\nThe while-loop approach is a straightforward implementation of the ReAct loop, where the agent continuously prompts the LLM with the user's input and the LLM's response, until a tool is called. The more modular agent + tools option, on the other hand, separates the agent's functionality from the tooling, allowing for greater flexibility and scalability. This approach uses a `ToolNode` to handle tool invocations and a `tools_fn` function to process the results of the tool calls.\n\nThe notebook also introduces a `loop_or_end` function, which determines whether the agent should continue to the next state or end the loop based on whether a tool was called. This function is used to create a conditional edge in the LangGraph, which allows the agent to transition between states based on the outcome of the tool calls. The notebook provides a basic implementation of the ReAct loop using the modular agent + tools option, and provides a starting point for further customization and extension.",
            "**Equipping Our Agent (Part 7.5.3)**\n\nIn this section, the notebook equips the agent with a simple but powerful tool, `read_notebook`, which allows the agent to enrich its context with the full content of a notebook on command. This tool is created using the `@tool` decorator and is designed to display a file to the user and the end-user. The notebook also modifies the schema of the `read_notebook` tool to specify the finite options for the files, which can be used to enforce grammar.\n\nThe notebook then creates a tooled LLM agent by binding the `conv_llm` to the `toolset` containing the `read_notebook` tool. This allows the agent to use the `read_notebook` tool to retrieve information from notebooks. The notebook also creates a new graph with the tooled agent and tools function, which uses the `ToolNode` to invoke the `read_notebook` tool when a tool is called.\n\nThe notebook then uses the `stream_response` function to stream the response of the tooled agent, which includes the output of the `read_notebook` tool. The `stream_response` function is used to print the messages from the buffer, and it takes several parameters, including the new message, the app, the config, and whether to print the stream. The notebook uses this function to print the response of the tooled agent, which includes the output of the `read_notebook` tool.",
            "**Part 7.5.4: Continuing With LangGraph?**\n\nThis section serves as a conclusion and a call to action for the reader to continue exploring LangGraph and its applications. The author encourages the reader to try out the tutorials and keep an eye out for new agentic paradigms as they emerge. This section is a gentle nudge to help the reader transition from the notebook to further learning and experimentation with LangGraph.\n\nThe author provides a link to the LangGraph tutorials, which is a valuable resource for readers who want to dive deeper into the framework. By mentioning the tutorials, the author is providing a clear next step for readers who are interested in continuing their learning journey with LangGraph.\n\nThe section is brief and to the point, serving as a natural conclusion to the notebook. It does not introduce any new concepts or code, but rather provides a sense of closure and a sense of direction for the reader. Overall, this section is a gentle reminder that LangGraph is a powerful tool that can be used to build complex agentic systems, and that there is more to explore and learn.",
            "**Equipping Our Agent**\n\nIn this section, the notebook equips the agent with a simple but powerful tool, `read_notebook`, which allows the agent to enrich its context with the full content of a notebook on command. This tool is created using the `@tool` decorator and is designed to display a file to the user and the end-user. The notebook also defines a `toolset` containing this `read_notebook` tool, and creates a `tooled_agent_fn` and `tooled_tools_fn` by partially applying the `agent_fn` and `tools_fn` functions with the `conv_llm` and `ToolNode` objects, respectively.\n\nThe notebook then creates a new graph with the `create_graph` function, using the `tooled_agent_fn` and `tooled_tools_fn` as the nodes, and defines a conditional edge that routes the conversation to the `tools` node if the agent's response contains a tool call, and to the `end` node otherwise. This setup allows the agent to use the `read_notebook` tool to retrieve information from a notebook and continue the conversation based on the retrieved information.\n\nThe notebook also defines a `stream_response` function that can be used to stream the conversation from the compiled graph, and uses this function to stream the conversation with the agent, starting with a user input of \"Give me an interesting code snippet from Notebook 5.\" The `stream_response` function prints the conversation in a structured format, showing the message content, metadata, and any tool results.",
            "**Equipping Our Agent (Part 7.5.3)**\n\nIn this section, the notebook introduces a simple but powerful tool called `read_notebook` that allows the agent to enrich its context with the full content of a notebook on command. This tool is equipped with the `FileLister` class, which is used to display a file to the user and the end-user. The `read_notebook` tool is then added to the toolset, and a new agent function, `tooled_agent_fn`, is created by binding the toolset to the `conv_llm` LLM and the `chat_prompt` prompt. This new agent function is used to create a new graph, `app`, which includes the `tooled_agent_fn` and a new tool function, `tooled_tools_fn`, which uses the `ToolNode` class to handle tool invocations.\n\nThe `tooled_tools_fn` function is used to handle tool invocations and append the results to the message buffer. The `loop_or_end` function is used to determine whether to route to the `tools` node or the `end` node based on whether a tool is called. The `stream_response` function is used to stream the response from the graph, and it is called with a question to demonstrate the agent's ability to retrieve information from a notebook. The `read_notebook` tool is used to retrieve the content of the notebook, and the agent's response is streamed to the user.\n\nThis section demonstrates how to equip the agent with a tool that allows it to retrieve information from a notebook, and how to use the `ToolNode` class to handle tool invocations. It also shows how to create a new graph with the equipped agent and tool, and how to stream the response from the graph."
        ]
    },
    "98_VLM_Launch.ipynb": {
        "outline": "98_VLM_Launch.ipynb\n - **Launching Your vLLM Servers**: Launching a vLLM OpenAI-style server running a local Visual Language Model (in this case, Microsoft's Phi-3.5-vision-instruct model), using the `vllm serve` command, with options for trust-remote-code, max-model-len, gpu-memory-utilization, enforce-eager, and port.\n - Main Ideas and Relevance To Course: vLLM, OpenAI-style server, Visual Language Model, Phi-3.5-vision-instruct, model serving, GPU memory management, LLMs, NIM, HuggingFace.\n - Important Code: `vllm serve`, `--trust-remote-code`, `--max-model-len`, `--gpu-memory-utilization`, `--enforce-eager`, `--port`.\n - Connections to previous notebooks: This notebook builds on the introduction to LLMs and agentics in previous notebooks, and is a stepping stone for further exploration of vLLM and its applications, including the use of LLMs for agentic systems and tooling.\n - Relevant Images: None.",
        "sections": [
            "**Launching Your vLLM Servers**\n\nThis section of the notebook is focused on launching a Visual Language Model (vLLM) server, specifically an OpenAI-style server, using the `vllm serve` command. The model being used is Microsoft's Phi-3.5-vision-instruct, a pre-trained model that has been selected for its ease of use, speed, and ability to read. The model is available as a NIM (NVIDIA Model) on the build.nvidia.com platform.\n\nThe `vllm serve` command is used to start the server, and several options are specified to customize the serving process. These options include:\n\n* `--trust-remote-code`, which allows the model to execute custom code modules not found in the basic HuggingFace transformers.\n* `--max-model-len`, which sets the maximum length of the model's input to 16,384, to prevent the GPU from running low on memory when working with other processes.\n* `--gpu-memory-utilization`, which caps the GPU memory usage to 80% to avoid conflicts with other local processes.\n* `--enforce-eager`, which enforces eager execution, and\n* `--port`, which specifies the port number to use for the server (in this case, port 9000).\n\nThese options are used to streamline the serving process and ensure that the model runs efficiently and effectively.",
            "**Main Ideas and Relevance To Course**\n\nThis section of the notebook is focused on launching a vLLM (Visual Large Language Model) server, specifically using the OpenAI-style server architecture. The vLLM server is a critical component of the notebook, as it enables the execution of a Visual Language Model, in this case, Microsoft's Phi-3.5-vision-instruct model. This model is chosen for its ease of use, relatively fast processing speed, and ability to read and understand visual inputs.\n\nThe vLLM server is launched using the `vllm serve` command, which is a key concept in this notebook. The command is accompanied by several options that configure the server's behavior, including `--trust-remote-code`, `--max-model-len`, `--gpu-memory-utilization`, `--enforce-eager`, and `--port`. These options are crucial in ensuring the server runs smoothly and efficiently, particularly in a GPU environment. The `--trust-remote-code` option, for instance, is necessary because the Phi-3.5-vision-instruct model incorporates custom code modules not found in basic HuggingFace transformers. The `--max-model-len` option is set to 16,384 to prevent the model from consuming excessive GPU memory.",
            "**Important Code:**\n\nThe notebook contains a single code block that launches a vLLM (Visual Language Model) server using the `vllm serve` command. The command is used to run a local instance of the Phi-3.5-vision-instruct model, a Visual Language Model developed by Microsoft. This model is selected for its ease of use, relatively fast processing speed, and ability to read and understand visual inputs.\n\nThe `vllm serve` command is accompanied by several options that configure the server's behavior. These options include:\n\n* `--trust-remote-code`: This option is enabled to allow the model to execute custom code modules that are not part of the standard HuggingFace transformers library.\n* `--max-model-len 16384`: This option sets the maximum length of the model's input to 16,384 characters, which is necessary to accommodate the model's KV-cache and prevent GPU memory issues.\n* `--gpu-memory-utilization 0.8`: This option caps the GPU memory utilization at 80%, preventing potential conflicts with other local processes.\n* `--enforce-eager`: This option is used to enforce eager execution, which is a mode of execution that allows the model to run in a more predictable and debuggable manner.\n* `--port 9000`: This option specifies the port number that the server will listen on for incoming requests.\n\nThese options are used to configure the vLLM server to run the Phi-3.5-vision-instruct model in a way that is optimized for performance and stability.",
            "**Important Code:**\n\nThe notebook contains a single code block that launches a vLLM (Visual Language Model) server using the `vllm serve` command. The command is used to run a local instance of the Phi-3.5-vision-instruct model, a Visual Language Model developed by Microsoft. The model is served on port 9000, and several options are specified to configure the server:\n\n* `--trust-remote-code` is enabled, which allows the model to execute custom code modules not included in the basic HuggingFace transformers.\n* `--max-model-len` is set to 16384, which is the maximum length of the model's input. This is done to prevent the model from consuming too much GPU memory, which can lead to conflicts with other local processes.\n* `--gpu-memory-utilization` is capped at 0.8, which means the model will not use more than 80% of the available GPU memory.\n* `--enforce-eager` is enabled, which ensures that the model executes eagerly, rather than lazily.\n* `--port 9000` specifies the port on which the model will be served.\n\nThese options are used to configure the vLLM server to run efficiently and effectively, while also preventing potential conflicts with other local processes.",
            "**Important Code:**\n\nThe notebook contains a single code block that launches a vLLM (Visual Language Model) server using the `vllm serve` command. The command is used to run a local instance of the Phi-3.5-vision-instruct model, a Visual Language Model developed by Microsoft. The model is served on port 9000, and several options are specified to configure the server:\n\n* `--trust-remote-code` is enabled, which allows the model to execute custom code modules not included in the basic HuggingFace transformers.\n* `--max-model-len` is set to 16384, which is the maximum length of the model's input. This is done to prevent the model from consuming too much GPU memory, which can lead to conflicts with other local processes.\n* `--gpu-memory-utilization` is capped at 0.8, which means the model will not use more than 80% of the available GPU memory.\n* `--enforce-eager` is enabled, which ensures that the model executes eagerly, rather than lazily.\n* `--port 9000` specifies the port on which the model will be served.\n\nThese options are used to configure the vLLM server to run efficiently and effectively, while also preventing potential conflicts with other local processes.",
            "**Important Code: `vllm serve`, `--trust-remote-code`, `--max-model-len`, `--gpu-memory-utilization`, `--enforce-eager`, `--port`**\n\nThe `vllm serve` command is used to launch a vLLM OpenAI-style server. This command is the central component of the notebook, and it is used to run a local Visual Language Model (in this case, Microsoft's Phi-3.5-vision-instruct model). The `vllm serve` command is followed by the model name, which in this case is `microsoft/phi-3.5-vision-instruct`.\n\nThe command also includes several options that are used to configure the server. The `--trust-remote-code` option is enabled, which allows the model to execute custom code modules that are not part of the basic HuggingFace transformers. The `--max-model-len` option is set to 16384, which is the maximum length of the model's input. The `--gpu-memory-utilization` option is set to 0.8, which means that the GPU memory will be utilized up to 80% to avoid conflicts with other local processes. The `--enforce-eager` option is also enabled, which ensures that the model is executed eagerly, rather than lazily. Finally, the `--port` option is set to 9000, which is the port number that the server will listen on."
        ]
    },
    "Table_of_Contents.ipynb": {
        "outline": "Table_of_Contents.ipynb\n - **Welcome to the course!**: Introduction to the course, overview of the content, and navigation instructions.\n - **Microservices**: Overview of the microservices used in the course, including chatbot, composer, docker-router, and llm_client, with descriptions of their functions and uses.\n - **Caches**: Overview of the caches used in the course, including nim-cache, temp-dir, imgs, img-files, audio-files, and generated_images, with descriptions of their functions and uses.\nMain Ideas and Relevance To Course: Course structure, microservices, caches, navigation, LLMs, NIM, Docker, Gradio, API access.\nImportant Code: `%%js`, `requests`, `http://docker_router:8070/containers`, `http://docker_router:8070/containers/{service_name}/logs`.\nConnections to previous notebooks: This notebook provides an overview of the course structure and introduces the microservices and caches used throughout the course, building on the introduction to LLMs and agentics in previous notebooks.\nRelevant Images: None.",
        "sections": [
            "**Welcome to the course!**\n\nThe first section of the notebook serves as an introduction to the course, providing an overview of the content and navigation instructions. The section is brief and to the point, setting the stage for the rest of the notebook. It is likely that this section will be read by students at the beginning of the course, and its purpose is to provide a gentle introduction to the material that will be covered.\n\nThe section is concise and does not contain any code or technical details. Instead, it focuses on providing a high-level overview of the course structure and encouraging students to explore the various components of the notebook. This section is likely intended to be a gentle introduction to the course, and its tone is informal and welcoming.\n\nIn terms of its connection to the rest of the notebook, this section provides a foundation for the technical content that follows. It sets the stage for the introduction of the microservices and caches that are used throughout the notebook, and provides a sense of what to expect from the rest of the course. By reading this section, students should gain a basic understanding of the course structure and be able to navigate the notebook with ease.",
            "**Microservices**\n\nThe notebook introduces four microservices: `./chatbot`, `./composer`, `./docker-router`, and `./llm_client`. Each microservice has a specific function:\n\n* `./chatbot` is a basic chatbot interface that allows access to multiple models. It has three modes: Basic, Context, and Agentic. The Basic mode provides direct access to the LLM, while the Context mode loads the context of the specified notebooks at the start. The Agentic mode attempts to load notebooks from the environment and reason about them.\n* `./composer` is the spinup routine used to construct the environment. It can be used to replicate the environment for advanced use cases.\n* `./docker-router` is a helper microservice used for advanced use cases, including facilitating assessment.\n* `./llm_client` enables API access to `build.nvidia.com`, which hosts NVIDIA NIM endpoints. It is used in notebooks 6 and above.\n\nThese microservices are likely used throughout the course to provide a structured and organized environment for the user to work with. The `./chatbot` microservice, in particular, is a key component that allows the user to interact with the LLM and access its capabilities. The other microservices support the functionality of the chatbot and provide additional tools for the user to work with.",
            "**Caches**\n\nThe notebook introduces several caches used throughout the course, which are directories that store data for efficient access and reuse. The caches include:\n\n* `./nim-cache`: a cache for the NIM model, used in notebooks 6+ to access a locally-hosted NIM instance (`Llama-3.1-8b-instruct`).\n* `./temp-dir`: a cache of the notebooks, used by later notebook ingestion routines and for cleaning the slate.\n* `./imgs`: a directory containing images from the course, used in the notebooks.\n* `./img-files`: a directory containing simple image examples for easy use.\n* `./audio-files`: a directory containing simple audio examples for easy use.\n* `./generated_images`: a directory created by the user for their assessment, containing synthetic images.\n\nThese caches are used to improve the efficiency and speed of the course, allowing for faster access to data and models. They also provide a way to clean up and reset the environment as needed. The `./nim-cache` cache is particularly important, as it enables access to a locally-hosted NIM instance, which is used in later notebooks.",
            "**Caches**\n\nThe notebook introduces several caches used throughout the course, each serving a specific purpose. The `./nim-cache` cache is used to store the NIM model, which is a locally-hosted instance of the Llama-3.1-8b-instruct model. This cache is used in notebooks 6 and above to access the NIM instance. The `./temp-dir` cache is used to store notebooks and is employed by later notebook ingestion routines. It also serves as a clean-slate option for users who want to start fresh. The `./imgs`, `./img-files`, and `./audio-files` directories store images and audio files used in the course, providing easy access to these resources. Finally, the `./generated_images` directory is a user-created space for storing synthetic images generated during the assessment. These caches are essential for the course's functionality, enabling efficient access to models, notebooks, and media files.",
            "**Caches**\n\nThe notebook introduces several caches used throughout the course, each serving a specific purpose. The `./nim-cache` cache is used to store the NIM model, which is a locally-hosted instance of the Llama-3.1-8b-instruct model. This cache is used in notebooks 6 and above to access the NIM instance. The `./temp-dir` cache is used to store notebooks and is employed by later notebook ingestion routines. It also serves as a clean-slate option for users who want to start fresh. The `./imgs`, `./img-files`, and `./audio-files` directories store images and audio files used in the course, respectively. Finally, the `./generated_images` directory is a user-created space for storing synthetic images generated during the assessment. These caches are essential for the course's functionality, particularly in notebooks 6 and above, where they enable access to the NIM model and facilitate the use of images and audio files.",
            "**Caches**\n\nThe notebook introduces several caches used throughout the course, each serving a specific purpose. The `./nim-cache` cache is used to store the NIM model, which is a locally-hosted instance of the Llama-3.1-8b-instruct model. This cache is used in notebooks 6 and later to access the NIM instance. The `./temp-dir` cache is used to store notebooks and is employed by later notebook ingestion routines. It also serves as a clean-slate option for users who want to start fresh. The `./imgs`, `./img-files`, and `./audio-files` directories store images and audio files used in the course, respectively. Finally, the `./generated_images` directory is a user-created space for storing synthetic images generated during the assessment. These caches are essential for the course's functionality and are used in conjunction with the microservices and LLMs introduced earlier in the notebook."
        ]
    },
    "course": "NVIDIA Deep Learning Institute's Instructor-Led Course called \"Rapid Application Development with Large Language Models\"",
    "filenames": [
        "00_jupyterlab.ipynb",
        "01_llm_intro.ipynb",
        "02_llm_intake.ipynb",
        "03_encoder_task.ipynb",
        "04_seq2seq.ipynb",
        "05_multimodal.ipynb",
        "06_llm_server.ipynb",
        "07_intro_agentics.ipynb",
        "08_assessment.ipynb",
        "75_langgraph.ipynb",
        "98_VLM_Launch.ipynb",
        "Table_of_Contents.ipynb"
    ],
    "summary": "Here's a summary of the course, covering each notebook:\n\n**Notebook 1: Getting Started With Large Language Models**\nThe first notebook introduces the concept of large language models (LLMs) and their applications. It covers the basics of LLMs, including their architecture, training, and fine-tuning. The notebook also explores the Hugging Face library and its transformers package, which provides a simple interface for loading and using pre-trained LLMs.\n\n**Notebook 2: LLM Architecture Intuitions**\nThis notebook delves deeper into the inner workings of LLMs, focusing on the transformer architecture and its components. It explains how LLMs process input data, including tokenization, embedding, and attention mechanisms. The notebook also introduces the concept of self-attention and its role in LLMs.\n\n**Notebook 3: LLM Encoder Tasks**\nThe third notebook explores the use of LLMs for various tasks, including token-level prediction, sequence-level classification, and zero-shot classification. It covers the concept of task-specific pipelines and how to use them to perform different tasks. The notebook also introduces the idea of using LLMs for text classification and sentiment analysis.\n\n**Notebook 4: Encoders and Decoders for Sequence Generation**\nThis notebook introduces the concept of encoder-decoder models, which are designed for sequence generation tasks such as machine translation and text summarization. It covers the architecture of encoder-decoder models and how they work. The notebook also explores the use of decoder-only models, such as GPT-2, for generating text.\n\n**Notebook 5: Multimodal Architectures and Fusion Techniques**\nThe fifth notebook explores the use of multimodal models, which can handle multiple types of data, including text, images, and audio. It covers the concept of joint optimization and how it can be used to align different modalities. The notebook also introduces the idea of cross-attention and its role in multimodal models.\n\n**Notebook 6: Introduction To GenAI Servers**\nThis notebook introduces the concept of GenAI servers, which are designed to deploy and access LLMs in a scalable and efficient way. It covers the use of vLLM, a popular open-source LLM serving project, and how to deploy a model using it. The notebook also explores the use of NVIDIA NIM for real-world use cases.\n\n**Notebook 7: Towards Orchestration and Agentics**\nThe seventh notebook explores the concept of LLM orchestration, which involves using multiple LLMs to perform complex tasks. It covers the use of LangChain, a framework for building LLM pipelines, and how to use it to create a conversational agent. The notebook also introduces the idea of agentic systems, which can reason and act iteratively.\n\n**Notebook 8: Course Assessment**\nThe final notebook is an assessment of the course, where students are asked to implement a feature that usually sits behind the API of an image-generating endpoint, synthetic prompts. It covers the use of text-to-text interfaces to map from \"regular human prompt\" to \"diffusion input prompt\" space. The notebook also introduces the concept of LangGraph, a multi-agent orchestration framework.\n\nBig-picture ideas to help an instructor explain the material and understand which parts of the course to refer to when addressing questions:\n\n* The course covers the basics of LLMs, including their architecture, training, and fine-tuning.\n* It explores the use of LLMs for various tasks, including token-level prediction, sequence-level classification, and zero-shot classification.\n* The course introduces the concept of encoder-decoder models and decoder-only models for sequence generation tasks.\n* It covers the use of multimodal models and joint optimization for aligning different modalities.\n* The course explores the concept of GenAI servers and how to deploy and access LLMs in a scalable and efficient way.\n* It introduces the idea of LLM orchestration and agentic systems, which can reason and act iteratively.\n* The course provides a comprehensive overview of the field of LLMs and their applications.\n\nWhen addressing questions, an instructor can refer to the following parts of the course:\n\n* For questions about the basics of LLMs, refer to Notebook 1.\n* For questions about LLM architecture and components, refer to Notebook 2.\n* For questions about task-specific pipelines and LLMs for text classification and sentiment analysis, refer to Notebook 3.\n* For questions about encoder-decoder models and decoder-only models, refer to Notebook 4.\n* For questions about multimodal models and joint optimization, refer to Notebook 5.\n* For questions about GenAI servers and LLM orchestration, refer to Notebook 6.\n* For questions about agentic systems and LangChain, refer to Notebook 7.\n* For questions about synthetic prompts and LangGraph, refer to Notebook 8."
}